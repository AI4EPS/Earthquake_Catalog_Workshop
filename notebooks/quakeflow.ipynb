{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U git+https://github.com/AI4EPS/GaMMA.git\n",
    "# !pip install -U git+https://github.com/AI4EPS/ADLoc.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "from glob import glob\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import fsspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import obspy\n",
    "import obspy.clients.fdsn\n",
    "import pandas as pd\n",
    "from obspy.clients.fdsn.mass_downloader import (\n",
    "    CircularDomain,\n",
    "    MassDownloader,\n",
    "    Restrictions,\n",
    ")\n",
    "from pyproj import Proj\n",
    "from tqdm import tqdm\n",
    "\n",
    "from adloc.eikonal2d import init_eikonal2d\n",
    "from adloc.sacloc2d import ADLoc\n",
    "from adloc.utils import invert_location\n",
    "from gamma.utils import association, estimate_eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(root_path: str = \"local\", region: str = \"demo\") -> Dict:\n",
    "\n",
    "    if not os.path.exists(f\"{root_path}/{region}\"):\n",
    "        os.makedirs(f\"{root_path}/{region}\", exist_ok=True)\n",
    "\n",
    "    regions = {\n",
    "            \"demo\": {\n",
    "                \"longitude0\": -117.504,\n",
    "                \"latitude0\": 35.705,\n",
    "                \"maxradius_degree\": 0.5,\n",
    "                \"mindepth\": 0,\n",
    "                \"maxdepth\": 20,\n",
    "                \"starttime\": \"2019-07-04T00:00:00\",\n",
    "                \"endtime\": \"2019-07-05T00:00:00\",\n",
    "                \"network\": \"CI\",\n",
    "                \"channel\": \"HH*,BH*,EH*,HN*\",\n",
    "                \"provider\": [\n",
    "                    \"SCEDC\"\n",
    "                ],\n",
    "            },\n",
    "            \"ridgecrest\": {\n",
    "                \"longitude0\": -117.504,\n",
    "                \"latitude0\": 35.705,\n",
    "                \"maxradius_degree\": 1.0,\n",
    "                \"mindepth\": 0,\n",
    "                \"maxdepth\": 20,\n",
    "                \"starttime\": \"2019-07-04T00:00:00\",\n",
    "                \"endtime\": \"2019-07-10T00:00:00\",\n",
    "                \"network\": \"CI\",\n",
    "                \"channel\": \"HH*,BH*,EH*,HN*\",\n",
    "                \"provider\": [\n",
    "                    \"SCEDC\"\n",
    "                ],\n",
    "            },\n",
    "    }\n",
    "\n",
    "    ## Set config\n",
    "    config = regions[region.lower()]\n",
    "\n",
    "    ## PhaseNet\n",
    "    config[\"phasenet\"] = {}\n",
    "    ## GaMMA\n",
    "    config[\"gamma\"] = {}\n",
    "    ## ADLoc\n",
    "    config[\"adloc\"] = {}\n",
    "    ## HypoDD\n",
    "    config[\"hypodd\"] = {}\n",
    "\n",
    "    with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:\n",
    "        json.dump(config, fp, indent=2)\n",
    "\n",
    "    print(json.dumps(config, indent=4))\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config  = set_config(region = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):\n",
    "    ## Plot earthquake locations\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        catalog['longitude'],\n",
    "        catalog['latitude'],\n",
    "        c=catalog['depth_km'],\n",
    "        cmap='viridis_r',\n",
    "        s=1,\n",
    "        alpha=0.6,\n",
    "        vmin = config[\"mindepth\"],\n",
    "        vmax = config[\"maxdepth\"]/2,\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    plt.colorbar(scatter, label='Depth (km)')\n",
    "    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])\n",
    "\n",
    "    # Add gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    plt.title(f'{method} Catalog ({len(catalog)})')\n",
    "    plt.show()\n",
    "\n",
    "def plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):\n",
    "    ## Plot earthquake locations\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        stations['longitude'],\n",
    "        stations['latitude'],\n",
    "        c=\"C0\",\n",
    "        s=40,\n",
    "        marker=\"^\",\n",
    "        alpha=0.6,\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "    if catalog is not None:\n",
    "        scatter = ax.scatter(\n",
    "            catalog['longitude'],\n",
    "            catalog['latitude'],\n",
    "            c=catalog['depth_km'],\n",
    "            cmap='viridis_r',\n",
    "            s=1,\n",
    "            alpha=0.6,\n",
    "            vmin = config[\"mindepth\"],\n",
    "            vmax = config[\"maxdepth\"]/2,\n",
    "            transform=ccrs.PlateCarree()\n",
    "        )\n",
    "        plt.colorbar(scatter, label='Depth (km)')\n",
    "    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])\n",
    "\n",
    "    # Add gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    plt.title(f'Stations ({len(stations[\"station\"].unique())})')\n",
    "    plt.show()\n",
    "\n",
    "def download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n",
    "\n",
    "    result_path = f\"{region}/obspy\"\n",
    "    if not os.path.exists(f\"{root_path}/{result_path}\"):\n",
    "        os.makedirs(f\"{root_path}/{result_path}\")\n",
    "    # print(json.dumps(config, indent=4))\n",
    "\n",
    "    ## Download catalog \n",
    "    client = obspy.clients.fdsn.Client(\"usgs\")\n",
    "    events = client.get_events(\n",
    "        starttime=config[\"starttime\"],\n",
    "        endtime=config[\"endtime\"],\n",
    "        latitude=config[\"latitude0\"],\n",
    "        longitude=config[\"longitude0\"],\n",
    "        maxradius=config[\"maxradius_degree\"],\n",
    "    )\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "\n",
    "    ## Save catalog\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        if len(event.magnitudes) > 0:\n",
    "            catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n",
    "            catalog[\"longitude\"].append(event.origins[0].longitude)\n",
    "            catalog[\"latitude\"].append(event.origins[0].latitude)\n",
    "            catalog[\"depth_km\"].append(event.origins[0].depth/1e3)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "    catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)\n",
    "\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_catalog = download_catalog(region=region, config=config)\n",
    "plot_catalog(standard_catalog, method=\"Standard\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n",
    "\n",
    "    result_dir = f\"{region}/obspy\"\n",
    "    if not os.path.exists(f\"{root_path}/{result_dir}\"):\n",
    "        os.makedirs(f\"{root_path}/{result_dir}\")\n",
    "    if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):\n",
    "        os.makedirs(f\"{root_path}/{result_dir}/inventory/\")\n",
    "\n",
    "    ## Download stations\n",
    "    stations = obspy.core.inventory.Inventory()\n",
    "    for provider in config[\"provider\"]:\n",
    "        client = obspy.clients.fdsn.Client(provider)\n",
    "        stations += client.get_stations(\n",
    "                network=config[\"network\"],\n",
    "                station=\"*\",\n",
    "                starttime=config[\"starttime\"],\n",
    "                endtime=config[\"endtime\"],\n",
    "                latitude=config[\"latitude0\"],\n",
    "                longitude=config[\"longitude0\"],\n",
    "                maxradius=config[\"maxradius_degree\"],\n",
    "                channel=config[\"channel\"],\n",
    "                level=\"response\",\n",
    "            )\n",
    "    stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")\n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "\n",
    "    ## Save stations\n",
    "    station_dict = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            inv = stations.select(network=network.code, station=station.code)\n",
    "            inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")\n",
    "            for channel in station:\n",
    "                sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"\n",
    "                station_dict[sid] = {\n",
    "                    \"network\": network.code,\n",
    "                    \"station\": station.code,\n",
    "                    \"location\": channel.location_code,\n",
    "                    \"channel\": channel.code,\n",
    "                    \"longitude\": channel.longitude,\n",
    "                    \"latitude\": channel.latitude,\n",
    "                    \"elevation_m\": channel.elevation,\n",
    "                    \"response\": round(channel.response.instrument_sensitivity.value, 2),\n",
    "                }\n",
    "\n",
    "    with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:\n",
    "        json.dump(station_dict, fp, indent=2)\n",
    "\n",
    "    with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "    \n",
    "    return pd.DataFrame.from_dict(station_dict, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = download_station(region=region, config=config)\n",
    "plot_stations(stations, catalog = standard_catalog, region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_remote_path(provider, bucket, starttime, network, station, location, channel):\n",
    "\n",
    "    starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx\n",
    "    if provider.lower() == \"scedc\":\n",
    "        year = starttime.strftime(\"%Y\")\n",
    "        dayofyear = starttime.strftime(\"%j\")\n",
    "        if location == \"\":\n",
    "            location = \"__\"\n",
    "        path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_<5}{channel}{location:_<2}_{year}{dayofyear}.ms\"\n",
    "    elif provider.lower() == \"ncedc\":\n",
    "        year = starttime.strftime(\"%Y\")\n",
    "        dayofyear = starttime.strftime(\"%j\")\n",
    "        path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "    return path\n",
    "\n",
    "def download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n",
    "\n",
    "    waveform_dir = f\"{region}/waveforms\"\n",
    "    if not os.path.exists(f\"{root_path}/{waveform_dir}\"):\n",
    "        os.makedirs(f\"{root_path}/{waveform_dir}\")\n",
    "    # print(json.dumps(config, indent=4))\n",
    "\n",
    "    ## Download from cloud\n",
    "    for provider in config[\"provider\"]:\n",
    "        if provider.lower() in [\"scedc\", \"ncedc\"]:\n",
    "            cloud = {\n",
    "                \"provider\": provider.lower(),\n",
    "                \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",\n",
    "            }\n",
    "\n",
    "        DELTATIME = \"1D\"\n",
    "        starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")\n",
    "        starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()\n",
    "        with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:\n",
    "            stations = json.load(f)\n",
    "\n",
    "        for starttime in tqdm(starttimes, desc=\"Downloading\"):\n",
    "            for _, station in stations.items():\n",
    "                network, station, location, channel = station[\"network\"], station[\"station\"], station[\"location\"], station[\"channel\"]\n",
    "                mseed_path = map_remote_path(\n",
    "                    cloud[\"provider\"],\n",
    "                    cloud[\"bucket\"],\n",
    "                    starttime,\n",
    "                    network,\n",
    "                    station,\n",
    "                    location,\n",
    "                    channel,\n",
    "                )\n",
    "                try:\n",
    "                    if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):\n",
    "                        # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")\n",
    "                        continue\n",
    "                    if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):\n",
    "                        os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")\n",
    "                    with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:\n",
    "                        data = f.read()\n",
    "                    with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:\n",
    "                        f.write(data)\n",
    "                except Exception as e:\n",
    "                    # print(f\"Failed to download {e}\")\n",
    "                    pass\n",
    "\n",
    "    # %% Download from FDSN\n",
    "    domain = CircularDomain(\n",
    "        longitude=config[\"longitude0\"],\n",
    "        latitude=config[\"latitude0\"],\n",
    "        minradius=0,\n",
    "        maxradius=config[\"maxradius_degree\"],\n",
    "    )\n",
    "\n",
    "    restrictions = Restrictions(\n",
    "        starttime=obspy.UTCDateTime(config[\"starttime\"]),\n",
    "        endtime=obspy.UTCDateTime(config[\"endtime\"]),\n",
    "        chunklength_in_sec=3600 * 24, # 1 day\n",
    "        network=config[\"network\"] if \"network\" in config else None,\n",
    "        station=config[\"station\"] if \"station\" in config else None,\n",
    "        minimum_interstation_distance_in_m=0,\n",
    "        minimum_length=0.1,\n",
    "        reject_channels_with_gaps=False,\n",
    "    )\n",
    "\n",
    "    def get_mseed_storage(network, station, location, channel, starttime, endtime):\n",
    "        mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"\n",
    "        if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):\n",
    "            # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")\n",
    "            return True\n",
    "        return f\"{root_path}/{waveform_dir}/{mseed_name}\"\n",
    "\n",
    "    mdl = MassDownloader(\n",
    "        providers=config[\"provider\"],\n",
    "        # providers=[\"IRIS\"],\n",
    "    )\n",
    "    mdl.download(\n",
    "        domain,\n",
    "        restrictions,\n",
    "        mseed_storage=get_mseed_storage,\n",
    "        stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",\n",
    "        download_chunk_size_in_mb=20,\n",
    "        threads_per_client=1,\n",
    "        print_report=False,\n",
    "    )\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_waveform(region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -> str:\n",
    "\n",
    "    result_path = f\"{region}/phasenet\"\n",
    "    if not os.path.exists(f\"{root_path}/{result_path}\"):\n",
    "        os.makedirs(f\"{root_path}/{result_path}\")\n",
    "\n",
    "    # %%\n",
    "    waveform_dir = f\"{region}/waveforms\"\n",
    "    mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))\n",
    "\n",
    "    # %% group 3C channels\n",
    "    mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))\n",
    "\n",
    "    # %%\n",
    "    with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:\n",
    "        fp.write(\"fname\\n\")\n",
    "        fp.write(\"\\n\".join(mseed_list))\n",
    "\n",
    "    # %%\n",
    "    model_path = \"QuakeFlow/PhaseNet/\"\n",
    "    cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"\n",
    "    # cmd += \" --sampling_rate 100\" \n",
    "    os.system(cmd)\n",
    "\n",
    "    return f\"{root_path}/{result_path}/phasenet_picks.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phasenet_picks = run_phasenet(region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n",
    "\n",
    "    data_path = f\"{region}/phasenet\"\n",
    "    result_path = f\"{region}/gamma\"\n",
    "    if not os.path.exists(f\"{root_path}/{result_path}\"):\n",
    "        os.makedirs(f\"{root_path}/{result_path}\")\n",
    "\n",
    "    picks_csv = f\"{data_path}/phasenet_picks.csv\"\n",
    "    gamma_events_csv = f\"{result_path}/gamma_events.csv\"\n",
    "    gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"\n",
    "    station_json = f\"{region}/obspy/stations.json\"\n",
    "\n",
    "    ## read picks\n",
    "    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n",
    "    picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")\n",
    "    picks[\"id\"] = picks[\"station_id\"]\n",
    "    picks[\"timestamp\"] = picks[\"phase_time\"]\n",
    "    picks[\"amp\"] = picks[\"phase_amplitude\"]\n",
    "    picks[\"type\"] = picks[\"phase_type\"]\n",
    "    picks[\"prob\"] = picks[\"phase_score\"]\n",
    "\n",
    "    ## read stations\n",
    "    stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")\n",
    "    stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n",
    "    stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n",
    "    proj = Proj(f\"+proj=sterea +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")\n",
    "    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n",
    "        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    "    )\n",
    "    stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n",
    "    # print(stations.to_string())\n",
    "\n",
    "    ## setting GaMMA configs\n",
    "    config[\"use_dbscan\"] = True\n",
    "    config[\"use_amplitude\"] = True\n",
    "    config[\"method\"] = \"BGMM\"\n",
    "    if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n",
    "        config[\"oversample_factor\"] = 5\n",
    "    if config[\"method\"] == \"GMM\":  ## GaussianMixture\n",
    "        config[\"oversample_factor\"] = 1\n",
    "\n",
    "    # earthquake location\n",
    "    config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\n",
    "    config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n",
    "    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n",
    "    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n",
    "    xmin, ymin = proj(minlon, minlat)\n",
    "    xmax, ymax = proj(maxlon, maxlat)\n",
    "    # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n",
    "    zmin = config[\"mindepth\"] if \"mindepth\" in config else 0\n",
    "    zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30\n",
    "    config[\"x(km)\"] = (xmin, xmax)\n",
    "    config[\"y(km)\"] = (ymin, ymax)\n",
    "    config[\"z(km)\"] = (zmin, zmax)\n",
    "    config[\"bfgs_bounds\"] = (\n",
    "        (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n",
    "        (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n",
    "        (0, config[\"z(km)\"][1] + 1),  # z\n",
    "        (None, None),  # t\n",
    "    )\n",
    "\n",
    "    # DBSCAN\n",
    "    config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s\n",
    "    config[\"dbscan_min_samples\"] = 3\n",
    "\n",
    "    ## Eikonal for 1D velocity model\n",
    "    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n",
    "    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n",
    "    vp_vs_ratio = 1.73\n",
    "    vs = [v / vp_vs_ratio for v in vp]\n",
    "    h = 0.3\n",
    "    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n",
    "    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\n",
    "\n",
    "    # filtering\n",
    "    config[\"min_picks_per_eq\"] = 5\n",
    "    config[\"min_p_picks_per_eq\"] = 0\n",
    "    config[\"min_s_picks_per_eq\"] = 0\n",
    "    config[\"max_sigma11\"] = 2.0  # s\n",
    "    config[\"max_sigma22\"] = 1.0  # log10(m/s)\n",
    "    config[\"max_sigma12\"] = 1.0  # covariance\n",
    "\n",
    "    ## filter picks without amplitude measurements\n",
    "    if config[\"use_amplitude\"]:\n",
    "        picks = picks[picks[\"amp\"] != -1]\n",
    "\n",
    "    # for k, v in config.items():\n",
    "    #     print(f\"{k}: {v}\")\n",
    "\n",
    "    print(f\"Number of picks: {len(picks)}\")\n",
    "\n",
    "    # %%\n",
    "    event_idx0 = 0  ## current earthquake index\n",
    "    assignments = []\n",
    "    events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\n",
    "\n",
    "    if len(events) == 0:\n",
    "        return \n",
    "    \n",
    "    ## create catalog\n",
    "    events = pd.DataFrame(events)\n",
    "    events[[\"longitude\", \"latitude\"]] = events.apply(\n",
    "        lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1\n",
    "    )\n",
    "    events[\"depth_km\"] = events[\"z(km)\"]\n",
    "    events.sort_values(\"time\", inplace=True)\n",
    "    with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:\n",
    "        events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    ## add assignment to picks\n",
    "    assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n",
    "    picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n",
    "    picks.sort_values([\"phase_time\"], inplace=True)\n",
    "    with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:\n",
    "        picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "\n",
    "    # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_catalog = run_gamma(region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n",
    "\n",
    "    data_path = f\"{root_path}/{region}/gamma\"\n",
    "    result_path = f\"{root_path}/{region}/adloc\"\n",
    "    figure_path = f\"{root_path}/{region}/adloc/figures\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    if not os.path.exists(figure_path):\n",
    "        os.makedirs(figure_path)\n",
    "\n",
    "    picks_file = f\"{data_path}/gamma_picks.csv\"\n",
    "    events_file = f\"{data_path}/gamma_events.csv\"\n",
    "    stations_file = f\"{root_path}/{region}/obspy/stations.json\"\n",
    "\n",
    "    proj = Proj(f\"+proj=sterea +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")\n",
    "\n",
    "    ## read picks and associated events\n",
    "    picks = pd.read_csv(picks_file)\n",
    "    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])\n",
    "     # drop unnecessary columns\n",
    "    picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "    if os.path.exists(events_file):\n",
    "        events = pd.read_csv(events_file)\n",
    "        events[\"time\"] = pd.to_datetime(events[\"time\"])\n",
    "        events[[\"x_km\", \"y_km\"]] = events.apply(\n",
    "            lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    "        )\n",
    "        events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0\n",
    "    else:\n",
    "        events = None\n",
    "\n",
    "    ## read stations\n",
    "    # stations = pd.read_csv(stations_file, sep=\"\\t\")\n",
    "    stations = pd.read_json(stations_file, orient=\"index\")\n",
    "    stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n",
    "    stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n",
    "    stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000\n",
    "    if \"station_term_time_p\" not in stations.columns:\n",
    "        stations[\"station_term_time_p\"] = 0.0\n",
    "    if \"station_term_time_s\" not in stations.columns:\n",
    "        stations[\"station_term_time_s\"] = 0.0\n",
    "    if \"station_term_amplitude\" not in stations.columns:\n",
    "        stations[\"station_term_amplitude\"] = 0.0\n",
    "    stations[[\"x_km\", \"y_km\"]] = stations.apply(\n",
    "        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n",
    "    )\n",
    "    stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n",
    "\n",
    "    ## setting ADLoc configs\n",
    "    config[\"use_amplitude\"] = True\n",
    "\n",
    "    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n",
    "    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n",
    "    xmin, ymin = proj(minlon, minlat)\n",
    "    xmax, ymax = proj(maxlon, maxlat)\n",
    "    zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n",
    "    config[\"xlim_km\"] = (xmin, xmax)\n",
    "    config[\"ylim_km\"] = (ymin, ymax)\n",
    "    config[\"zlim_km\"] = (zmin, zmax)\n",
    "\n",
    "    ## Eikonal for 1D velocity model\n",
    "    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n",
    "    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n",
    "    vp_vs_ratio = 1.73\n",
    "    vs = [v / vp_vs_ratio for v in vp]\n",
    "    # Northern California (Gil7)\n",
    "    # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]\n",
    "    # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]\n",
    "    # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]\n",
    "    h = 0.3\n",
    "    vel = {\"Z\": zz, \"P\": vp, \"S\": vs}\n",
    "    config[\"eikonal\"] = {\n",
    "        \"vel\": vel,\n",
    "        \"h\": h,\n",
    "        \"xlim_km\": config[\"xlim_km\"],\n",
    "        \"ylim_km\": config[\"ylim_km\"],\n",
    "        \"zlim_km\": config[\"zlim_km\"],\n",
    "    }\n",
    "    config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])\n",
    "\n",
    "    # RASAC\n",
    "    config[\"min_picks\"] = 6\n",
    "    config[\"min_picks_ratio\"] = 0.5\n",
    "    config[\"max_residual_time\"] = 1.0\n",
    "    config[\"max_residual_amplitude\"] = 1.0\n",
    "    config[\"min_score\"] = 0.5\n",
    "    config[\"min_s_picks\"] = 1.5\n",
    "    config[\"min_p_picks\"] = 1.5\n",
    "\n",
    "    config[\"bfgs_bounds\"] = (\n",
    "        (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x\n",
    "        (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y\n",
    "        (0, config[\"zlim_km\"][1] + 1),\n",
    "        (None, None),  # t\n",
    "    )\n",
    "\n",
    "    # %%\n",
    "    mapping_phase_type_int = {\"P\": 0, \"S\": 1}\n",
    "    picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)\n",
    "    if \"phase_amplitude\" in picks.columns:\n",
    "        picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)\n",
    "\n",
    "    # %%\n",
    "    stations[\"idx_sta\"] = np.arange(len(stations))\n",
    "    if events is None:\n",
    "        picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")\n",
    "        events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})\n",
    "        events[\"z_km\"] = 10.0  # km default depth\n",
    "        events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)\n",
    "        events[\"event_index\"] = events.index\n",
    "        events.reset_index(drop=True, inplace=True)\n",
    "        events[\"idx_eve\"] = np.arange(len(events))\n",
    "        picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)\n",
    "    else:\n",
    "        events[\"idx_eve\"] = np.arange(len(events))\n",
    "\n",
    "    picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")\n",
    "    picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")\n",
    "\n",
    "\n",
    "    # for key, value in config.items():\n",
    "    #     print(f\"{key}: {value}\")\n",
    "\n",
    "    print(f\"Number of picks: {len(picks)}\")\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "\n",
    "    # %%\n",
    "    estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])\n",
    "\n",
    "    # %%\n",
    "    MAX_SST_ITER = 8\n",
    "    events_init = events.copy()\n",
    "\n",
    "    for iter in range(MAX_SST_ITER):\n",
    "        picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)\n",
    "\n",
    "        station_term_amp = (\n",
    "            picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()\n",
    "        )\n",
    "        station_term_amp.set_index(\"idx_sta\", inplace=True)\n",
    "        stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)\n",
    "\n",
    "        station_term_time = (\n",
    "            picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()\n",
    "        )\n",
    "        station_term_time.set_index(\"idx_sta\", inplace=True)\n",
    "        stations[\"station_term_time_p\"] += (\n",
    "            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)\n",
    "        )\n",
    "        stations[\"station_term_time_s\"] += (\n",
    "            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)\n",
    "        )\n",
    "\n",
    "        if \"event_index\" not in events.columns:\n",
    "            events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n",
    "        events[[\"longitude\", \"latitude\"]] = events.apply(\n",
    "            lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n",
    "        )\n",
    "        events[\"depth_km\"] = events[\"z_km\"]\n",
    "\n",
    "        picks[\"adloc_mask\"] = picks[\"mask\"]\n",
    "        picks[\"adloc_residual_time\"] = picks[\"residual_time\"]\n",
    "        picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]\n",
    "\n",
    "        picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)\n",
    "        events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)\n",
    "        stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)\n",
    "\n",
    "    # %%\n",
    "    if \"event_index\" not in events.columns:\n",
    "        events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n",
    "    events[[\"longitude\", \"latitude\"]] = events.apply(\n",
    "        lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n",
    "    )\n",
    "    events[\"depth_km\"] = events[\"z_km\"]\n",
    "    events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "    events.sort_values([\"time\"], inplace=True)\n",
    "\n",
    "    picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})\n",
    "    picks.drop(\n",
    "        [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"\n",
    "    )\n",
    "    picks.sort_values([\"phase_time\"], inplace=True)\n",
    "\n",
    "    stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)\n",
    "    events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)\n",
    "    stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adloc_catalog = run_adloc(region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):\n",
    "\n",
    "    data_path = f\"{region}/adloc\"\n",
    "    result_path = f\"{region}/hypodd\"\n",
    "    if not os.path.exists(f\"{root_path}/{result_path}\"):\n",
    "        os.makedirs(f\"{root_path}/{result_path}\")\n",
    "\n",
    "    ## Station Format\n",
    "    stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")\n",
    "    stations.set_index(\"station_id\", inplace=True)\n",
    "\n",
    "    shift_topo = stations[\"elevation_m\"].max() / 1e3\n",
    "    converted_hypoinverse = []\n",
    "    converted_hypodd = {}\n",
    "\n",
    "    for sta, row in stations.iterrows():\n",
    "        network_code, station_code, comp_code, channel_code = sta.split(\".\")\n",
    "        station_weight = \" \"\n",
    "        lat_degree = int(row[\"latitude\"])\n",
    "        lat_minute = (row[\"latitude\"] - lat_degree) * 60\n",
    "        north = \"N\" if lat_degree >= 0 else \"S\"\n",
    "        lng_degree = int(row[\"longitude\"])\n",
    "        lng_minute = (row[\"longitude\"] - lng_degree) * 60\n",
    "        west = \"W\" if lng_degree <= 0 else \"E\"\n",
    "        elevation = row[\"elevation_m\"]\n",
    "        line_hypoinverse = f\"{station_code:<5} {network_code:<2} {comp_code[:-1]:<1}{channel_code:<3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"\n",
    "        converted_hypoinverse.append(line_hypoinverse)\n",
    "\n",
    "        # tmp_code = f\"{station_code}{channel_code}\"\n",
    "        tmp_code = f\"{station_code}\"\n",
    "        converted_hypodd[tmp_code] = f\"{tmp_code:<8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"\n",
    "\n",
    "\n",
    "    with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:\n",
    "        for k, v in converted_hypodd.items():\n",
    "            f.write(v)\n",
    "\n",
    "\n",
    "    ## Picks Format\n",
    "    picks_csv = f\"{data_path}/adloc_picks.csv\"\n",
    "    events_csv = f\"{data_path}/adloc_events.csv\"\n",
    "\n",
    "    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n",
    "    events = pd.read_csv(f\"{root_path}/{events_csv}\")\n",
    "    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")\n",
    "    events[\"time\"] = pd.to_datetime(events[\"time\"])\n",
    "    # events[\"magnitude\"] = 1.0\n",
    "    events[\"sigma_time\"] = 1.0\n",
    "\n",
    "    # events.sort_values(\"time\", inplace=True)\n",
    "    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n",
    "\n",
    "    lines = []\n",
    "    picks_by_event = picks.groupby(\"event_index\").groups\n",
    "    for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):\n",
    "        # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        event_time = event[\"time\"]\n",
    "        lat = event[\"latitude\"]\n",
    "        lng = event[\"longitude\"]\n",
    "        # dep = event[\"depth(m)\"] / 1e3 + shift_topo\n",
    "        dep = event[\"depth_km\"] + shift_topo\n",
    "        mag = event[\"magnitude\"]\n",
    "        EH = 0\n",
    "        EZ = 0\n",
    "        RMS = event[\"sigma_time\"]\n",
    "\n",
    "        year, month, day, hour, min, sec = (\n",
    "            event_time.year,\n",
    "            event_time.month,\n",
    "            event_time.day,\n",
    "            event_time.hour,\n",
    "            event_time.minute,\n",
    "            float(event_time.strftime(\"%S.%f\")),\n",
    "        )\n",
    "        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n",
    "\n",
    "        lines.append(event_line)\n",
    "\n",
    "        picks_idx = picks_by_event[event[\"event_index\"]]\n",
    "        for j in picks_idx:\n",
    "            # pick = picks.iloc[j]\n",
    "            pick = picks.loc[j]\n",
    "            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n",
    "            phase_type = pick[\"phase_type\"].upper()\n",
    "            phase_score = pick[\"phase_score\"]\n",
    "            # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n",
    "            pick_time = (pick[\"phase_time\"] - event_time).total_seconds()\n",
    "            tmp_code = f\"{station_code}\"\n",
    "            pick_line = f\"{tmp_code:<7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n",
    "            lines.append(pick_line)\n",
    "\n",
    "    with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:\n",
    "        fp.writelines(lines)\n",
    "\n",
    "    ## Run Hypodd\n",
    "    print(f\"Running Hypodd:\")\n",
    "    os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")\n",
    "\n",
    "    ## Read  catalog\n",
    "    columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]\n",
    "    catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)\n",
    "    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(\n",
    "        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',\n",
    "        axis=1,\n",
    "    )\n",
    "    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n",
    "    catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]\n",
    "    catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)\n",
    "    catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)\n",
    "    catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)\n",
    "\n",
    "    return catalog_ct_hypodd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypodd_catalog = run_hypodd(region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
