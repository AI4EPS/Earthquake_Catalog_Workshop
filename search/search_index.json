{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Earthquake Catalog Workshop","text":"<p>Welcome to the Earthquake Catalog Workshop! This repository contains materials for a workshop on earthquake catalogs, including data, scripts, and documentation.</p> <p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"#introductions","title":"Introductions","text":"<ul> <li>Instructor intros</li> <li>Workshop goals/schedule</li> <li>What is an earthquake catalog? Different end-users and scientific motivations</li> <li>Why earthquake catalogs and how to choose the right one?</li> </ul>"},{"location":"#regional-seismic-networks-official-catalogs-and-data-access-ellen-gabrielle","title":"Regional Seismic Networks: Official Catalogs and Data Access (Ellen &amp; Gabrielle)","text":"<ul> <li>How regional catalogs are made and what they include</li> <li>SCSN catalog and special datasets</li> <li>ComCat (ANSS)</li> <li>NSF Sage (Earthscope)</li> <li>Accessing waveform data and metadata - AWS Open Dataset vs FDSN Webservices</li> <li>Citing network data &amp; catalogs</li> </ul>"},{"location":"#break","title":"Break","text":""},{"location":"#building-custom-catalogs-with-modern-tools","title":"Building Custom Catalogs with Modern Tools","text":"<ul> <li>Catalog Workflow + Machine Learning (Weiqiang &amp; Clara)</li> <li>Template Matching (Eric)</li> </ul>"},{"location":"#break_1","title":"Break","text":""},{"location":"#evaluating-catalog-quality","title":"Evaluating Catalog Quality","text":"<ul> <li>Anomaly detection</li> <li>Visualization tools</li> <li>Magnitude of completeness</li> <li>Quality control</li> <li>Tips &amp; tricks</li> </ul>"},{"location":"#conclusions","title":"Conclusions","text":"<ul> <li>Discuss limitations and reasons for using different types of catalogs</li> <li>When to use STA/LTA vs. deep-learning vs. template-matching vs relocated</li> <li>Combining different methods depending on application</li> <li>How does a user choose among the many options for deep-learning pickers, event associators, event location methods?</li> </ul>"},{"location":"#discussion-questions-tutorial-help","title":"Discussion, Questions, &amp; Tutorial Help","text":"<p>If you have any questions about the workshop materials or encounter any issues, please open an issue on our GitHub repository.</p> <pre><code>@misc{earthquake_catalog_workshop_2025,\n  author = {Beauce, Eric and Tepp, Gabrielle and Yoon, Clara and Yu, Ellen and Zhu, Weiqiang},\n  title = {Building a High Resolution Earthquake Catalog from Raw Waveforms: A Step-by-Step Guide},\n  year = {2025},\n  url = {https://ai4eps.github.io/Earthquake_Catalog_Workshop/},\n  note = {Seismological Society of America (SSA) Annual Meeting, 2025}\n}\n</code></pre>"},{"location":"catalog_analysis/","title":"Evaluating Earthquake Catalog","text":"<p>      Fullscreen    </p>"},{"location":"conclusion/","title":"Conclusions","text":"<p>      Fullscreen    </p>"},{"location":"introduction/","title":"Introduction","text":"<p>      Fullscreen    </p>"},{"location":"machine_learning/","title":"Slides","text":"<p>      Fullscreen    </p>"},{"location":"seismic_network/","title":"Slides","text":"<p>      Fullscreen    </p>"},{"location":"template_matching/","title":"Slides","text":"Animation         Fullscreen"},{"location":"notebooks/quakeflow/","title":"Notebook","text":"In\u00a0[1]: Copied! <pre># !conda env create -f env.yaml --prefix .conda/quakeflow\n# !source activate .conda/quakeflow\n# !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/\n# !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../\n# !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git\n# !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git\n</pre> # !conda env create -f env.yaml --prefix .conda/quakeflow # !source activate .conda/quakeflow # !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/ # !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../ # !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git # !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git In\u00a0[2]: Copied! <pre># %%\nimport json\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom typing import Dict\nfrom glob import glob\n\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport fsspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport obspy\nimport pandas as pd\nfrom obspy.clients.fdsn.mass_downloader import (\n    CircularDomain,\n    MassDownloader,\n    Restrictions,\n)\nfrom pyproj import Proj\nfrom tqdm import tqdm\n\nfrom adloc.eikonal2d import init_eikonal2d\nfrom adloc.sacloc2d import ADLoc\nfrom adloc.utils import invert_location\nfrom gamma.utils import association, estimate_eps\n</pre> # %% import json import os import pickle from collections import defaultdict from datetime import datetime from typing import Dict from glob import glob  import cartopy.crs as ccrs import cartopy.feature as cfeature import fsspec import matplotlib.pyplot as plt import numpy as np import obspy import pandas as pd from obspy.clients.fdsn.mass_downloader import (     CircularDomain,     MassDownloader,     Restrictions, ) from pyproj import Proj from tqdm import tqdm  from adloc.eikonal2d import init_eikonal2d from adloc.sacloc2d import ADLoc from adloc.utils import invert_location from gamma.utils import association, estimate_eps  In\u00a0[3]: Copied! <pre>def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:\n\n    if not os.path.exists(f\"{root_path}/{region}\"):\n        os.makedirs(f\"{root_path}/{region}\", exist_ok=True)\n\n    regions = {\n            \"demo\": {\n                \"longitude0\": -117.504,\n                \"latitude0\": 35.705,\n                \"maxradius_degree\": 0.5,\n                \"mindepth\": 0,\n                \"maxdepth\": 30,\n                \"starttime\": \"2019-07-04T00:00:00\",\n                \"endtime\": \"2019-07-05T00:00:00\",\n                \"network\": \"CI\",\n                \"channel\": \"HH*,BH*,EH*,HN*\",\n                \"provider\": [\n                    \"SCEDC\"\n                ],\n            },\n            \"ridgecrest\": {\n                \"longitude0\": -117.504,\n                \"latitude0\": 35.705,\n                \"maxradius_degree\": 0.5,\n                \"mindepth\": 0,\n                \"maxdepth\": 30,\n                \"starttime\": \"2019-07-04T00:00:00\",\n                \"endtime\": \"2019-07-10T00:00:00\",\n                \"network\": \"CI\",\n                \"channel\": \"HH*,BH*,EH*,HN*\",\n                \"provider\": [\n                    \"SCEDC\"\n                ],\n            },\n    }\n\n    ## Set config\n    config = regions[region.lower()]\n\n    ## PhaseNet\n    config[\"phasenet\"] = {}\n    ## GaMMA\n    config[\"gamma\"] = {}\n    ## ADLoc\n    config[\"adloc\"] = {}\n    ## HypoDD\n    config[\"hypodd\"] = {}\n\n    with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:\n        json.dump(config, fp, indent=2)\n\n    print(json.dumps(config, indent=4))\n\n    return config\n</pre> def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:      if not os.path.exists(f\"{root_path}/{region}\"):         os.makedirs(f\"{root_path}/{region}\", exist_ok=True)      regions = {             \"demo\": {                 \"longitude0\": -117.504,                 \"latitude0\": 35.705,                 \"maxradius_degree\": 0.5,                 \"mindepth\": 0,                 \"maxdepth\": 30,                 \"starttime\": \"2019-07-04T00:00:00\",                 \"endtime\": \"2019-07-05T00:00:00\",                 \"network\": \"CI\",                 \"channel\": \"HH*,BH*,EH*,HN*\",                 \"provider\": [                     \"SCEDC\"                 ],             },             \"ridgecrest\": {                 \"longitude0\": -117.504,                 \"latitude0\": 35.705,                 \"maxradius_degree\": 0.5,                 \"mindepth\": 0,                 \"maxdepth\": 30,                 \"starttime\": \"2019-07-04T00:00:00\",                 \"endtime\": \"2019-07-10T00:00:00\",                 \"network\": \"CI\",                 \"channel\": \"HH*,BH*,EH*,HN*\",                 \"provider\": [                     \"SCEDC\"                 ],             },     }      ## Set config     config = regions[region.lower()]      ## PhaseNet     config[\"phasenet\"] = {}     ## GaMMA     config[\"gamma\"] = {}     ## ADLoc     config[\"adloc\"] = {}     ## HypoDD     config[\"hypodd\"] = {}      with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:         json.dump(config, fp, indent=2)      print(json.dumps(config, indent=4))      return config In\u00a0[4]: Copied! <pre>region = \"demo\"\nconfig  = set_config(region = region)\n</pre> region = \"demo\" config  = set_config(region = region) <pre>{\n    \"longitude0\": -117.504,\n    \"latitude0\": 35.705,\n    \"maxradius_degree\": 0.5,\n    \"mindepth\": 0,\n    \"maxdepth\": 30,\n    \"starttime\": \"2019-07-04T00:00:00\",\n    \"endtime\": \"2019-07-05T00:00:00\",\n    \"network\": \"CI\",\n    \"channel\": \"HH*,BH*,EH*,HN*\",\n    \"provider\": [\n        \"SCEDC\"\n    ],\n    \"phasenet\": {},\n    \"gamma\": {},\n    \"adloc\": {},\n    \"hypodd\": {}\n}\n</pre> In\u00a0[5]: Copied! <pre>def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure(figsize=(8, 8))\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        catalog['longitude'],\n        catalog['latitude'],\n        c=catalog['depth_km'],\n        cmap='viridis_r',\n        s=5,\n        alpha=0.6,\n        vmin = config[\"mindepth\"],\n        vmax = config[\"maxdepth\"]/2,\n        transform=ccrs.PlateCarree()\n    )\n\n    plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'{method} Catalog ({len(catalog)})')\n\n    fig, axes = plt.subplots(2, 1, figsize=(8, 6))\n\n    # Histogram of magnitudes\n    axes[0].hist(catalog['magnitude'], bins=20, color='C0', edgecolor='white')\n    axes[0].set_xlabel('Magnitude')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_yscale('log')\n    # axes[0].set_title(f'{method} Catalog ({len(catalog)})')\n    axes[0].grid(linestyle='--')\n\n    # Scatter plot of magnitudes over time\n    axes[1].scatter(\n        pd.to_datetime(catalog['time']),\n        catalog['magnitude'],\n        s=2**catalog['magnitude'],\n        c=\"C0\"\n    )\n    axes[1].set_xlim(pd.to_datetime(config[\"starttime\"]), pd.to_datetime(config[\"endtime\"]))\n    axes[1].set_xlabel('Time')\n    axes[1].set_ylabel('Magnitude')\n    # axes[1].set_title(f'{method} Catalog ({len(catalog)})')\n    axes[1].tick_params(axis='x', rotation=45)\n    axes[1].grid(linestyle='--')\n\n    plt.tight_layout()\n\n    plt.show()\n\ndef plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure(figsize=(8, 8))\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        stations['longitude'],\n        stations['latitude'],\n        c=\"C0\",\n        s=40,\n        marker=\"^\",\n        alpha=0.6,\n        transform=ccrs.PlateCarree()\n    )\n    if catalog is not None:\n        scatter = ax.scatter(\n            catalog['longitude'],\n            catalog['latitude'],\n            c=catalog['depth_km'],\n            cmap='viridis_r',\n            s=0.5,\n            alpha=0.6,\n            vmin = config[\"mindepth\"],\n            vmax = config[\"maxdepth\"]/2,\n            transform=ccrs.PlateCarree()\n        )\n        plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'Stations ({len(stations[\"station\"].unique())})')\n    plt.show()\n\ndef download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_path = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download catalog \n    client = obspy.clients.fdsn.Client(\"usgs\")\n    events = client.get_events(\n        starttime=config[\"starttime\"],\n        endtime=config[\"endtime\"],\n        latitude=config[\"latitude0\"],\n        longitude=config[\"longitude0\"],\n        maxradius=config[\"maxradius_degree\"],\n    )\n    print(f\"Number of events: {len(events)}\")\n\n    ## Save catalog\n    catalog = defaultdict(list)\n    for event in events:\n        if len(event.magnitudes) &gt; 0:\n            catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))\n            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n            catalog[\"longitude\"].append(event.origins[0].longitude)\n            catalog[\"latitude\"].append(event.origins[0].latitude)\n            catalog[\"depth_km\"].append(event.origins[0].depth/1e3)\n    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n    catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)\n\n    return catalog\n</pre> def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure(figsize=(8, 8))     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         catalog['longitude'],         catalog['latitude'],         c=catalog['depth_km'],         cmap='viridis_r',         s=5,         alpha=0.6,         vmin = config[\"mindepth\"],         vmax = config[\"maxdepth\"]/2,         transform=ccrs.PlateCarree()     )      plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'{method} Catalog ({len(catalog)})')      fig, axes = plt.subplots(2, 1, figsize=(8, 6))      # Histogram of magnitudes     axes[0].hist(catalog['magnitude'], bins=20, color='C0', edgecolor='white')     axes[0].set_xlabel('Magnitude')     axes[0].set_ylabel('Frequency')     axes[0].set_yscale('log')     # axes[0].set_title(f'{method} Catalog ({len(catalog)})')     axes[0].grid(linestyle='--')      # Scatter plot of magnitudes over time     axes[1].scatter(         pd.to_datetime(catalog['time']),         catalog['magnitude'],         s=2**catalog['magnitude'],         c=\"C0\"     )     axes[1].set_xlim(pd.to_datetime(config[\"starttime\"]), pd.to_datetime(config[\"endtime\"]))     axes[1].set_xlabel('Time')     axes[1].set_ylabel('Magnitude')     # axes[1].set_title(f'{method} Catalog ({len(catalog)})')     axes[1].tick_params(axis='x', rotation=45)     axes[1].grid(linestyle='--')      plt.tight_layout()      plt.show()  def plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure(figsize=(8, 8))     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         stations['longitude'],         stations['latitude'],         c=\"C0\",         s=40,         marker=\"^\",         alpha=0.6,         transform=ccrs.PlateCarree()     )     if catalog is not None:         scatter = ax.scatter(             catalog['longitude'],             catalog['latitude'],             c=catalog['depth_km'],             cmap='viridis_r',             s=0.5,             alpha=0.6,             vmin = config[\"mindepth\"],             vmax = config[\"maxdepth\"]/2,             transform=ccrs.PlateCarree()         )         plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'Stations ({len(stations[\"station\"].unique())})')     plt.show()  def download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_path = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")     # print(json.dumps(config, indent=4))      ## Download catalog      client = obspy.clients.fdsn.Client(\"usgs\")     events = client.get_events(         starttime=config[\"starttime\"],         endtime=config[\"endtime\"],         latitude=config[\"latitude0\"],         longitude=config[\"longitude0\"],         maxradius=config[\"maxradius_degree\"],     )     print(f\"Number of events: {len(events)}\")      ## Save catalog     catalog = defaultdict(list)     for event in events:         if len(event.magnitudes) &gt; 0:             catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))             catalog[\"magnitude\"].append(event.magnitudes[0].mag)             catalog[\"longitude\"].append(event.origins[0].longitude)             catalog[\"latitude\"].append(event.origins[0].latitude)             catalog[\"depth_km\"].append(event.origins[0].depth/1e3)     catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])     catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)      return catalog In\u00a0[6]: Copied! <pre>standard_catalog = download_catalog(region=region, config=config)\nplot_catalog(standard_catalog, method=\"Standard\", config=config)\n</pre> standard_catalog = download_catalog(region=region, config=config) plot_catalog(standard_catalog, method=\"Standard\", config=config) <pre>Number of events: 709\n</pre> In\u00a0[7]: Copied! <pre>def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_dir = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_dir}\"):\n        os.makedirs(f\"{root_path}/{result_dir}\")\n    if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):\n        os.makedirs(f\"{root_path}/{result_dir}/inventory/\")\n\n    ## Download stations\n    stations = obspy.core.inventory.Inventory()\n    for provider in config[\"provider\"]:\n        client = obspy.clients.fdsn.Client(provider)\n        stations += client.get_stations(\n                network=config[\"network\"],\n                station=\"*\",\n                starttime=config[\"starttime\"],\n                endtime=config[\"endtime\"],\n                latitude=config[\"latitude0\"],\n                longitude=config[\"longitude0\"],\n                maxradius=config[\"maxradius_degree\"],\n                channel=config[\"channel\"],\n                level=\"response\",\n            )\n    stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")\n    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n\n    ## Save stations\n    station_dict = defaultdict(dict)\n    for network in stations:\n        for station in network:\n            inv = stations.select(network=network.code, station=station.code)\n            inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")\n            for channel in station:\n                sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"\n                station_dict[sid] = {\n                    \"network\": network.code,\n                    \"station\": station.code,\n                    \"location\": channel.location_code,\n                    \"channel\": channel.code,\n                    \"longitude\": channel.longitude,\n                    \"latitude\": channel.latitude,\n                    \"elevation_m\": channel.elevation,\n                    \"response\": round(channel.response.instrument_sensitivity.value, 2),\n                }\n\n    # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:\n    #     json.dump(station_dict, fp, indent=2)\n\n    with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:\n        pickle.dump(stations, fp)\n\n    stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")\n    stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)\n        \n    return stations\n</pre> def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_dir = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_dir}\"):         os.makedirs(f\"{root_path}/{result_dir}\")     if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):         os.makedirs(f\"{root_path}/{result_dir}/inventory/\")      ## Download stations     stations = obspy.core.inventory.Inventory()     for provider in config[\"provider\"]:         client = obspy.clients.fdsn.Client(provider)         stations += client.get_stations(                 network=config[\"network\"],                 station=\"*\",                 starttime=config[\"starttime\"],                 endtime=config[\"endtime\"],                 latitude=config[\"latitude0\"],                 longitude=config[\"longitude0\"],                 maxradius=config[\"maxradius_degree\"],                 channel=config[\"channel\"],                 level=\"response\",             )     stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")     print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))      ## Save stations     station_dict = defaultdict(dict)     for network in stations:         for station in network:             inv = stations.select(network=network.code, station=station.code)             inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")             for channel in station:                 sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"                 station_dict[sid] = {                     \"network\": network.code,                     \"station\": station.code,                     \"location\": channel.location_code,                     \"channel\": channel.code,                     \"longitude\": channel.longitude,                     \"latitude\": channel.latitude,                     \"elevation_m\": channel.elevation,                     \"response\": round(channel.response.instrument_sensitivity.value, 2),                 }      # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:     #     json.dump(station_dict, fp, indent=2)      with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:         pickle.dump(stations, fp)      stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")     stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)              return stations In\u00a0[8]: Copied! <pre>stations = download_station(region=region, config=config)\nplot_stations(stations, catalog = standard_catalog, region=region, config=config)\n</pre> stations = download_station(region=region, config=config) plot_stations(stations, catalog = standard_catalog, region=region, config=config) <pre>Number of stations: 15\n</pre> In\u00a0[9]: Copied! <pre>def map_remote_path(provider, bucket, starttime, network, station, location, channel):\n\n    starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx\n    if provider.lower() == \"scedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        if location == \"\":\n            location = \"__\"\n        path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"\n    elif provider.lower() == \"ncedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"\n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n    return path\n\ndef download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    waveform_dir = f\"{region}/waveforms\"\n    if not os.path.exists(f\"{root_path}/{waveform_dir}\"):\n        os.makedirs(f\"{root_path}/{waveform_dir}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download from cloud\n    for provider in config[\"provider\"]:\n        if provider.lower() in [\"scedc\", \"ncedc\"]:\n            cloud = {\n                \"provider\": provider.lower(),\n                \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",\n            }\n        else:\n            continue\n\n        DELTATIME = \"1D\"\n        starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")\n        starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()\n        # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:\n        #     stations = json.load(f)\n        stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n        stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])\n        stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])\n        location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04', \n                              '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')\n        location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}\n        instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')\n        instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}\n        component_priorities = ('E', 'N', 'Z', '1', '2', '3')\n        component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}\n        stations['location_priority'] = stations['location'].map(location_priority_map)\n        stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))\n        stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))\n        stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)\n        stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)\n\n        for starttime in starttimes:\n            for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):\n                prev = \"\"\n                nch = set()\n                for _, row in station.iterrows():\n                    if len(nch) &gt;= 3:\n                        break\n                    network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]\n                    if instrument != prev:\n                        prev = instrument\n                        nch = set()\n                    mseed_path = map_remote_path(\n                        cloud[\"provider\"],\n                        cloud[\"bucket\"],\n                        starttime,\n                        network,\n                        station,\n                        location,\n                        channel,\n                    )\n                    try:\n                        if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):\n                            # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")\n                            nch.add(channel[-1])\n                            continue\n                        if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):\n                            os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")\n                        with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:\n                            data = f.read()\n                        with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:\n                            f.write(data)\n                            nch.add(channel[-1])\n                    except Exception as e:\n                        # print(f\"Failed to download {e}\")\n                        pass\n\n    # %% Download from FDSN\n    domain = CircularDomain(\n        longitude=config[\"longitude0\"],\n        latitude=config[\"latitude0\"],\n        minradius=0,\n        maxradius=config[\"maxradius_degree\"],\n    )\n\n    restrictions = Restrictions(\n        starttime=obspy.UTCDateTime(config[\"starttime\"]),\n        endtime=obspy.UTCDateTime(config[\"endtime\"]),\n        chunklength_in_sec=3600 * 24, # 1 day\n        network=config[\"network\"] if \"network\" in config else None,\n        station=config[\"station\"] if \"station\" in config else None,\n        minimum_interstation_distance_in_m=0,\n        minimum_length=0.1,\n        reject_channels_with_gaps=False,\n    )\n\n    def get_mseed_storage(network, station, location, channel, starttime, endtime):\n        mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"\n        if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):\n            # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")\n            return True\n        return f\"{root_path}/{waveform_dir}/{mseed_name}\"\n\n    mdl = MassDownloader(\n        providers=config[\"provider\"],\n        # providers=[\"IRIS\"],\n    )\n    mdl.download(\n        domain,\n        restrictions,\n        mseed_storage=get_mseed_storage,\n        stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",\n        download_chunk_size_in_mb=20,\n        threads_per_client=3,\n        print_report=False,\n    )\n    \n    return\n</pre> def map_remote_path(provider, bucket, starttime, network, station, location, channel):      starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx     if provider.lower() == \"scedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         if location == \"\":             location = \"__\"         path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"     elif provider.lower() == \"ncedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"     else:         raise ValueError(f\"Unknown provider: {provider}\")     return path  def download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      waveform_dir = f\"{region}/waveforms\"     if not os.path.exists(f\"{root_path}/{waveform_dir}\"):         os.makedirs(f\"{root_path}/{waveform_dir}\")     # print(json.dumps(config, indent=4))      ## Download from cloud     for provider in config[\"provider\"]:         if provider.lower() in [\"scedc\", \"ncedc\"]:             cloud = {                 \"provider\": provider.lower(),                 \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",             }         else:             continue          DELTATIME = \"1D\"         starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")         starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()         # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:         #     stations = json.load(f)         stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)         stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])         stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])         location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04',                                '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')         location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}         instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')         instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}         component_priorities = ('E', 'N', 'Z', '1', '2', '3')         component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}         stations['location_priority'] = stations['location'].map(location_priority_map)         stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))         stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))         stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)         stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)          for starttime in starttimes:             for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):                 prev = \"\"                 nch = set()                 for _, row in station.iterrows():                     if len(nch) &gt;= 3:                         break                     network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]                     if instrument != prev:                         prev = instrument                         nch = set()                     mseed_path = map_remote_path(                         cloud[\"provider\"],                         cloud[\"bucket\"],                         starttime,                         network,                         station,                         location,                         channel,                     )                     try:                         if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):                             # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")                             nch.add(channel[-1])                             continue                         if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):                             os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")                         with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:                             data = f.read()                         with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:                             f.write(data)                             nch.add(channel[-1])                     except Exception as e:                         # print(f\"Failed to download {e}\")                         pass      # %% Download from FDSN     domain = CircularDomain(         longitude=config[\"longitude0\"],         latitude=config[\"latitude0\"],         minradius=0,         maxradius=config[\"maxradius_degree\"],     )      restrictions = Restrictions(         starttime=obspy.UTCDateTime(config[\"starttime\"]),         endtime=obspy.UTCDateTime(config[\"endtime\"]),         chunklength_in_sec=3600 * 24, # 1 day         network=config[\"network\"] if \"network\" in config else None,         station=config[\"station\"] if \"station\" in config else None,         minimum_interstation_distance_in_m=0,         minimum_length=0.1,         reject_channels_with_gaps=False,     )      def get_mseed_storage(network, station, location, channel, starttime, endtime):         mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"         if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):             # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")             return True         return f\"{root_path}/{waveform_dir}/{mseed_name}\"      mdl = MassDownloader(         providers=config[\"provider\"],         # providers=[\"IRIS\"],     )     mdl.download(         domain,         restrictions,         mseed_storage=get_mseed_storage,         stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",         download_chunk_size_in_mb=20,         threads_per_client=3,         print_report=False,     )          return  In\u00a0[10]: Copied! <pre>download_waveform(region=region, config=config)\n</pre> download_waveform(region=region, config=config) <pre>Downloading 2019-07-04 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:00&lt;00:00, 25.37it/s]\n[2025-04-08 05:50:19,549] - obspy.clients.fdsn.mass_downloader - INFO: Initializing FDSN client(s) for SCEDC.\n[2025-04-08 05:50:19,552] - obspy.clients.fdsn.mass_downloader - INFO: Successfully initialized 1 client(s): SCEDC.\n[2025-04-08 05:50:19,553] - obspy.clients.fdsn.mass_downloader - INFO: Total acquired or preexisting stations: 0\n[2025-04-08 05:50:19,553] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Requesting unreliable availability.\n[2025-04-08 05:50:19,774] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - No data available for request.\n[2025-04-08 05:50:19,774] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - No data available.\n</pre> In\u00a0[11]: Copied! <pre>def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:\n\n    result_path = f\"{region}/phasenet\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    # %%\n    waveform_dir = f\"{region}/waveforms\"\n    mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))\n\n    # %% group 3C channels\n    mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))\n\n    # %%\n    with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:\n        fp.write(\"fname\\n\")\n        fp.write(\"\\n\".join(mseed_list))\n\n    # %%\n    model_path = \"QuakeFlow/PhaseNet/\"\n    cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"\n    # cmd += \" --sampling_rate 100\" \n    os.system(cmd)\n\n    return f\"{root_path}/{result_path}/phasenet_picks.csv\"\n</pre> def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:      result_path = f\"{region}/phasenet\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      # %%     waveform_dir = f\"{region}/waveforms\"     mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))      # %% group 3C channels     mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))      # %%     with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:         fp.write(\"fname\\n\")         fp.write(\"\\n\".join(mseed_list))      # %%     model_path = \"QuakeFlow/PhaseNet/\"     cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"     # cmd += \" --sampling_rate 100\"      os.system(cmd)      return f\"{root_path}/{result_path}/phasenet_picks.csv\"  In\u00a0[12]: Copied! <pre>phasenet_picks = run_phasenet(region=region, config=config)\n</pre> phasenet_picks = run_phasenet(region=region, config=config) <pre>2025-04-08 05:50:20.107525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-08 05:50:21,906 Pred log: local/demo/phasenet\n2025-04-08 05:50:21,907 Dataset size: 17\n2025-04-08 05:50:21.960184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-08 05:50:21.962201: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-08 05:50:21,986 Model: depths 5, filters 8, filter size 7x1, pool size: 4x1, dilation rate: 1x1\n2025-04-08 05:50:23.297573: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-08 05:50:23.379875: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n2025-04-08 05:50:23,543 restoring model QuakeFlow/PhaseNet//model/190703-214543/model_95.ckpt\nPred: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17/17 [01:33&lt;00:00,  5.49s/it]\n</pre> <pre>Done with 14088 P-picks and 14637 S-picks\n</pre> In\u00a0[13]: Copied! <pre>def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{region}/phasenet\"\n    result_path = f\"{region}/gamma\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    picks_csv = f\"{data_path}/phasenet_picks.csv\"\n    gamma_events_csv = f\"{result_path}/gamma_events.csv\"\n    gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"\n    station_json = f\"{region}/obspy/stations.json\"\n\n    ## read picks\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")\n    picks[\"id\"] = picks[\"station_id\"]\n    picks[\"timestamp\"] = picks[\"phase_time\"]\n    picks[\"amp\"] = picks[\"phase_amplitude\"]\n    picks[\"type\"] = picks[\"phase_type\"]\n    picks[\"prob\"] = picks[\"phase_score\"]\n\n    ## read stations\n    # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")\n    stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n    stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")\n    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n    # print(stations.to_string())\n\n    ## setting GaMMA configs\n    config[\"use_dbscan\"] = True\n    config[\"use_amplitude\"] = True\n    config[\"method\"] = \"BGMM\"\n    if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n        config[\"oversample_factor\"] = 5\n    if config[\"method\"] == \"GMM\":  ## GaussianMixture\n        config[\"oversample_factor\"] = 1\n\n    # earthquake location\n    config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\n    config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    zmin = config[\"mindepth\"] if \"mindepth\" in config else 0\n    zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30\n    config[\"x(km)\"] = (xmin, xmax)\n    config[\"y(km)\"] = (ymin, ymax)\n    config[\"z(km)\"] = (zmin, zmax)\n    config[\"bfgs_bounds\"] = (\n        (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n        (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n        (0, config[\"z(km)\"][1] + 1),  # z\n        (None, None),  # t\n    )\n\n    # DBSCAN\n    config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s\n    config[\"dbscan_min_samples\"] = 3\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 0.3\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\n\n    # filtering\n    config[\"min_picks_per_eq\"] = 5\n    config[\"min_p_picks_per_eq\"] = 0\n    config[\"min_s_picks_per_eq\"] = 0\n    config[\"max_sigma11\"] = 2.0  # s\n    config[\"max_sigma22\"] = 1.0  # log10(m/s)\n    config[\"max_sigma12\"] = 1.0  # covariance\n\n    ## filter picks without amplitude measurements\n    if config[\"use_amplitude\"]:\n        picks = picks[picks[\"amp\"] != -1]\n\n    # for k, v in config.items():\n    #     print(f\"{k}: {v}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n\n    # %%\n    event_idx0 = 0  ## current earthquake index\n    assignments = []\n    events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\n\n    if len(events) == 0:\n        return \n    \n    ## create catalog\n    events = pd.DataFrame(events)\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z(km)\"]\n    events.sort_values(\"time\", inplace=True)\n    with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:\n        events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n    ## add assignment to picks\n    assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n    picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n    picks.sort_values([\"phase_time\"], inplace=True)\n    with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:\n        picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n\n    # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"\n    return events\n</pre> def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{region}/phasenet\"     result_path = f\"{region}/gamma\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      picks_csv = f\"{data_path}/phasenet_picks.csv\"     gamma_events_csv = f\"{result_path}/gamma_events.csv\"     gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"     station_json = f\"{region}/obspy/stations.json\"      ## read picks     picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")     picks[\"id\"] = picks[\"station_id\"]     picks[\"timestamp\"] = picks[\"phase_time\"]     picks[\"amp\"] = picks[\"phase_amplitude\"]     picks[\"type\"] = picks[\"phase_type\"]     picks[\"prob\"] = picks[\"phase_score\"]      ## read stations     # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")     stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)     stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")     stations[[\"x(km)\", \"y(km)\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)     # print(stations.to_string())      ## setting GaMMA configs     config[\"use_dbscan\"] = True     config[\"use_amplitude\"] = True     config[\"method\"] = \"BGMM\"     if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture         config[\"oversample_factor\"] = 5     if config[\"method\"] == \"GMM\":  ## GaussianMixture         config[\"oversample_factor\"] = 1      # earthquake location     config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}     config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]     minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     zmin = config[\"mindepth\"] if \"mindepth\" in config else 0     zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30     config[\"x(km)\"] = (xmin, xmax)     config[\"y(km)\"] = (ymin, ymax)     config[\"z(km)\"] = (zmin, zmax)     config[\"bfgs_bounds\"] = (         (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x         (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y         (0, config[\"z(km)\"][1] + 1),  # z         (None, None),  # t     )      # DBSCAN     config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s     config[\"dbscan_min_samples\"] = 3      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 0.3     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}      # filtering     config[\"min_picks_per_eq\"] = 5     config[\"min_p_picks_per_eq\"] = 0     config[\"min_s_picks_per_eq\"] = 0     config[\"max_sigma11\"] = 2.0  # s     config[\"max_sigma22\"] = 1.0  # log10(m/s)     config[\"max_sigma12\"] = 1.0  # covariance      ## filter picks without amplitude measurements     if config[\"use_amplitude\"]:         picks = picks[picks[\"amp\"] != -1]      # for k, v in config.items():     #     print(f\"{k}: {v}\")      print(f\"Number of picks: {len(picks)}\")      # %%     event_idx0 = 0  ## current earthquake index     assignments = []     events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])      if len(events) == 0:         return           ## create catalog     events = pd.DataFrame(events)     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z(km)\"]     events.sort_values(\"time\", inplace=True)     with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:         events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")     ## add assignment to picks     assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])     picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})     picks.sort_values([\"phase_time\"], inplace=True)     with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:         picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")      # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"     return events  In\u00a0[14]: Copied! <pre>gamma_catalog = run_gamma(region=region, config=config)\n</pre> gamma_catalog = run_gamma(region=region, config=config) <pre>Number of picks: 28725\nEikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 2.289\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.052\nAssociating 637 clusters with 15 CPUs\n.............................................................................\nAssociated 100 events\n........................................................\nAssociated 200 events\n.........................................\nAssociated 300 events\n.....................................................\nAssociated 400 events\n....................................................\nAssociated 500 events\n.......................................\nAssociated 600 events\n............................................\nAssociated 700 events\n...................................................\nAssociated 800 events\n.................................................\nAssociated 900 events\n..............................................\nAssociated 1000 events\n.............................................\nAssociated 1100 events\n..............................................\nAssociated 1200 events\n...................................\nAssociated 1300 events\n...</pre> In\u00a0[15]: Copied! <pre>plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config)\n</pre> plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config) In\u00a0[16]: Copied! <pre>def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{root_path}/{region}/gamma\"\n    result_path = f\"{root_path}/{region}/adloc\"\n    figure_path = f\"{root_path}/{region}/adloc/figures\"\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n    if not os.path.exists(figure_path):\n        os.makedirs(figure_path)\n\n    picks_file = f\"{data_path}/gamma_picks.csv\"\n    events_file = f\"{data_path}/gamma_events.csv\"\n    stations_file = f\"{root_path}/{region}/obspy/stations.csv\"\n\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")\n\n    ## read picks and associated events\n    picks = pd.read_csv(picks_file)\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])\n     # drop unnecessary columns\n    picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")\n    if os.path.exists(events_file):\n        events = pd.read_csv(events_file)\n        events[\"time\"] = pd.to_datetime(events[\"time\"])\n        events[[\"x_km\", \"y_km\"]] = events.apply(\n            lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n        )\n        events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0\n    else:\n        events = None\n\n    ## read stations\n    # stations = pd.read_json(stations_file, orient=\"index\")\n    stations = pd.read_csv(stations_file, na_filter=False)\n    stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000\n    if \"station_term_time_p\" not in stations.columns:\n        stations[\"station_term_time_p\"] = 0.0\n    if \"station_term_time_s\" not in stations.columns:\n        stations[\"station_term_time_s\"] = 0.0\n    if \"station_term_amplitude\" not in stations.columns:\n        stations[\"station_term_amplitude\"] = 0.0\n    stations[[\"x_km\", \"y_km\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n\n    ## setting ADLoc configs\n    config[\"use_amplitude\"] = True\n\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    config[\"xlim_km\"] = (xmin, xmax)\n    config[\"ylim_km\"] = (ymin, ymax)\n    config[\"zlim_km\"] = (zmin, zmax)\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    # Northern California (Gil7)\n    # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]\n    # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]\n    # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]\n    h = 0.3\n    vel = {\"Z\": zz, \"P\": vp, \"S\": vs}\n    config[\"eikonal\"] = {\n        \"vel\": vel,\n        \"h\": h,\n        \"xlim_km\": config[\"xlim_km\"],\n        \"ylim_km\": config[\"ylim_km\"],\n        \"zlim_km\": config[\"zlim_km\"],\n    }\n    config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])\n\n    # RASAC\n    config[\"min_picks\"] = 6\n    config[\"min_picks_ratio\"] = 0.5\n    config[\"max_residual_time\"] = 1.0\n    config[\"max_residual_amplitude\"] = 1.0\n    config[\"min_score\"] = 0.5\n    config[\"min_s_picks\"] = 1.5\n    config[\"min_p_picks\"] = 1.5\n\n    config[\"bfgs_bounds\"] = (\n        (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x\n        (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y\n        (0, config[\"zlim_km\"][1] + 1),\n        (None, None),  # t\n    )\n\n    # %%\n    mapping_phase_type_int = {\"P\": 0, \"S\": 1}\n    picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)\n    if \"phase_amplitude\" in picks.columns:\n        picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)\n\n    # %%\n    stations[\"idx_sta\"] = np.arange(len(stations))\n    if events is None:\n        picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")\n        events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})\n        events[\"z_km\"] = 10.0  # km default depth\n        events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)\n        events[\"event_index\"] = events.index\n        events.reset_index(drop=True, inplace=True)\n        events[\"idx_eve\"] = np.arange(len(events))\n        picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)\n    else:\n        events[\"idx_eve\"] = np.arange(len(events))\n\n    picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")\n    picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")\n\n\n    # for key, value in config.items():\n    #     print(f\"{key}: {value}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n    print(f\"Number of events: {len(events)}\")\n\n    # %%\n    estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])\n\n    # %%\n    MAX_SST_ITER = 8\n    events_init = events.copy()\n\n    for iter in range(MAX_SST_ITER):\n        picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)\n\n        station_term_amp = (\n            picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()\n        )\n        station_term_amp.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)\n\n        station_term_time = (\n            picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()\n        )\n        station_term_time.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_time_p\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)\n        )\n        stations[\"station_term_time_s\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)\n        )\n\n        if \"event_index\" not in events.columns:\n            events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n        events[[\"longitude\", \"latitude\"]] = events.apply(\n            lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n        )\n        events[\"depth_km\"] = events[\"z_km\"]\n\n        picks[\"adloc_mask\"] = picks[\"mask\"]\n        picks[\"adloc_residual_time\"] = picks[\"residual_time\"]\n        picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]\n\n        picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)\n        events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)\n        stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)\n\n    # %%\n    if \"event_index\" not in events.columns:\n        events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z_km\"]\n    events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n    events.sort_values([\"time\"], inplace=True)\n\n    picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})\n    picks.drop(\n        [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"\n    )\n    picks.sort_values([\"phase_time\"], inplace=True)\n\n    stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n\n    picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)\n    events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)\n    stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)\n\n    return events\n</pre> def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{root_path}/{region}/gamma\"     result_path = f\"{root_path}/{region}/adloc\"     figure_path = f\"{root_path}/{region}/adloc/figures\"     if not os.path.exists(result_path):         os.makedirs(result_path)     if not os.path.exists(figure_path):         os.makedirs(figure_path)      picks_file = f\"{data_path}/gamma_picks.csv\"     events_file = f\"{data_path}/gamma_events.csv\"     stations_file = f\"{root_path}/{region}/obspy/stations.csv\"      proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")      ## read picks and associated events     picks = pd.read_csv(picks_file)     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])      # drop unnecessary columns     picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")     if os.path.exists(events_file):         events = pd.read_csv(events_file)         events[\"time\"] = pd.to_datetime(events[\"time\"])         events[[\"x_km\", \"y_km\"]] = events.apply(             lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1         )         events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0     else:         events = None      ## read stations     # stations = pd.read_json(stations_file, orient=\"index\")     stations = pd.read_csv(stations_file, na_filter=False)     stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000     if \"station_term_time_p\" not in stations.columns:         stations[\"station_term_time_p\"] = 0.0     if \"station_term_time_s\" not in stations.columns:         stations[\"station_term_time_s\"] = 0.0     if \"station_term_amplitude\" not in stations.columns:         stations[\"station_term_amplitude\"] = 0.0     stations[[\"x_km\", \"y_km\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)      ## setting ADLoc configs     config[\"use_amplitude\"] = True      minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     config[\"xlim_km\"] = (xmin, xmax)     config[\"ylim_km\"] = (ymin, ymax)     config[\"zlim_km\"] = (zmin, zmax)      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     # Northern California (Gil7)     # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]     # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]     # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]     h = 0.3     vel = {\"Z\": zz, \"P\": vp, \"S\": vs}     config[\"eikonal\"] = {         \"vel\": vel,         \"h\": h,         \"xlim_km\": config[\"xlim_km\"],         \"ylim_km\": config[\"ylim_km\"],         \"zlim_km\": config[\"zlim_km\"],     }     config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])      # RASAC     config[\"min_picks\"] = 6     config[\"min_picks_ratio\"] = 0.5     config[\"max_residual_time\"] = 1.0     config[\"max_residual_amplitude\"] = 1.0     config[\"min_score\"] = 0.5     config[\"min_s_picks\"] = 1.5     config[\"min_p_picks\"] = 1.5      config[\"bfgs_bounds\"] = (         (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x         (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y         (0, config[\"zlim_km\"][1] + 1),         (None, None),  # t     )      # %%     mapping_phase_type_int = {\"P\": 0, \"S\": 1}     picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)     if \"phase_amplitude\" in picks.columns:         picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)      # %%     stations[\"idx_sta\"] = np.arange(len(stations))     if events is None:         picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")         events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})         events[\"z_km\"] = 10.0  # km default depth         events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)         events[\"event_index\"] = events.index         events.reset_index(drop=True, inplace=True)         events[\"idx_eve\"] = np.arange(len(events))         picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)     else:         events[\"idx_eve\"] = np.arange(len(events))      picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")     picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")       # for key, value in config.items():     #     print(f\"{key}: {value}\")      print(f\"Number of picks: {len(picks)}\")     print(f\"Number of events: {len(events)}\")      # %%     estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])      # %%     MAX_SST_ITER = 8     events_init = events.copy()      for iter in range(MAX_SST_ITER):         picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)          station_term_amp = (             picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()         )         station_term_amp.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)          station_term_time = (             picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()         )         station_term_time.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_time_p\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)         )         stations[\"station_term_time_s\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)         )          if \"event_index\" not in events.columns:             events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]         events[[\"longitude\", \"latitude\"]] = events.apply(             lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1         )         events[\"depth_km\"] = events[\"z_km\"]          picks[\"adloc_mask\"] = picks[\"mask\"]         picks[\"adloc_residual_time\"] = picks[\"residual_time\"]         picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]          picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)         events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)         stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)      # %%     if \"event_index\" not in events.columns:         events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z_km\"]     events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")     events.sort_values([\"time\"], inplace=True)      picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})     picks.drop(         [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"     )     picks.sort_values([\"phase_time\"], inplace=True)      stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")      picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)     events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)     stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)      return events In\u00a0[17]: Copied! <pre>adloc_catalog = run_adloc(region=region, config=config)\n</pre> adloc_catalog = run_adloc(region=region, config=config) <pre>Eikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 1.892\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.045\nNumber of picks: 25237\nNumber of events: 1330\n</pre> <pre>Iter 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:23&lt;00:00, 57.11it/s]</pre> <pre>ADLoc locates 1064 events outof 1330 events\nusing 22734 picks outof 25237 picks\n</pre> <pre>\nIter 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:22&lt;00:00, 59.77it/s]</pre> <pre>ADLoc locates 1043 events outof 1330 events\nusing 22514 picks outof 25237 picks\n</pre> <pre>\nIter 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:21&lt;00:00, 61.01it/s]</pre> <pre>ADLoc locates 1045 events outof 1330 events\nusing 22545 picks outof 25237 picks\n</pre> <pre>\nIter 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:22&lt;00:00, 60.03it/s]</pre> <pre>ADLoc locates 1042 events outof 1330 events\nusing 22495 picks outof 25237 picks\n</pre> <pre>\nIter 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:21&lt;00:00, 60.77it/s]</pre> <pre>ADLoc locates 1040 events outof 1330 events\nusing 22488 picks outof 25237 picks\n</pre> <pre>\nIter 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:22&lt;00:00, 60.45it/s]</pre> <pre>ADLoc locates 1039 events outof 1330 events\nusing 22479 picks outof 25237 picks\n</pre> <pre>\nIter 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:21&lt;00:00, 60.75it/s]</pre> <pre>ADLoc locates 1040 events outof 1330 events\nusing 22446 picks outof 25237 picks\n</pre> <pre>\nIter 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1330/1330 [00:22&lt;00:00, 60.03it/s]</pre> <pre>ADLoc locates 1037 events outof 1330 events\nusing 22437 picks outof 25237 picks\n</pre> <pre>\n</pre> In\u00a0[18]: Copied! <pre>plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config)\n</pre> plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config) <p>Credit: Felix Waldhauser</p> In\u00a0[19]: Copied! <pre>def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):\n\n    data_path = f\"{region}/adloc\"\n    result_path = f\"{region}/hypodd\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    ## Station Format\n    stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")\n    stations.set_index(\"station_id\", inplace=True)\n\n    shift_topo = stations[\"elevation_m\"].max() / 1e3\n    converted_hypoinverse = []\n    converted_hypodd = {}\n\n    for sta, row in stations.iterrows():\n        network_code, station_code, comp_code, channel_code = sta.split(\".\")\n        station_weight = \" \"\n        lat_degree = int(row[\"latitude\"])\n        lat_minute = (row[\"latitude\"] - lat_degree) * 60\n        north = \"N\" if lat_degree &gt;= 0 else \"S\"\n        lng_degree = int(row[\"longitude\"])\n        lng_minute = (row[\"longitude\"] - lng_degree) * 60\n        west = \"W\" if lng_degree &lt;= 0 else \"E\"\n        elevation = row[\"elevation_m\"]\n        line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"\n        converted_hypoinverse.append(line_hypoinverse)\n\n        # tmp_code = f\"{station_code}{channel_code}\"\n        tmp_code = f\"{station_code}\"\n        converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"\n\n\n    with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:\n        for k, v in converted_hypodd.items():\n            f.write(v)\n\n\n    ## Picks Format\n    picks_csv = f\"{data_path}/adloc_picks.csv\"\n    events_csv = f\"{data_path}/adloc_events.csv\"\n\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    events = pd.read_csv(f\"{root_path}/{events_csv}\")\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")\n    events[\"time\"] = pd.to_datetime(events[\"time\"])\n    # events[\"magnitude\"] = 1.0\n    events[\"sigma_time\"] = 1.0\n\n    # events.sort_values(\"time\", inplace=True)\n    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n\n    lines = []\n    picks_by_event = picks.groupby(\"event_index\").groups\n    for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):\n        # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n        event_time = event[\"time\"]\n        lat = event[\"latitude\"]\n        lng = event[\"longitude\"]\n        # dep = event[\"depth(m)\"] / 1e3 + shift_topo\n        dep = event[\"depth_km\"] + shift_topo\n        mag = event[\"magnitude\"]\n        EH = 0\n        EZ = 0\n        RMS = event[\"sigma_time\"]\n\n        year, month, day, hour, min, sec = (\n            event_time.year,\n            event_time.month,\n            event_time.day,\n            event_time.hour,\n            event_time.minute,\n            float(event_time.strftime(\"%S.%f\")),\n        )\n        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n\n        lines.append(event_line)\n\n        picks_idx = picks_by_event[event[\"event_index\"]]\n        for j in picks_idx:\n            # pick = picks.iloc[j]\n            pick = picks.loc[j]\n            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n            phase_type = pick[\"phase_type\"].upper()\n            phase_score = pick[\"phase_score\"]\n            # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n            pick_time = (pick[\"phase_time\"] - event_time).total_seconds()\n            tmp_code = f\"{station_code}\"\n            pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n            lines.append(pick_line)\n\n    with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:\n        fp.writelines(lines)\n\n    ## Run Hypodd\n    print(f\"Running Hypodd:\")\n    os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")\n\n    ## Read  catalog\n    columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]\n    catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(\n        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',\n        axis=1,\n    )\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n    catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]\n    catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)\n    catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)\n    catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)\n\n    return catalog_ct_hypodd\n</pre> def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):      data_path = f\"{region}/adloc\"     result_path = f\"{region}/hypodd\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      ## Station Format     stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")     stations.set_index(\"station_id\", inplace=True)      shift_topo = stations[\"elevation_m\"].max() / 1e3     converted_hypoinverse = []     converted_hypodd = {}      for sta, row in stations.iterrows():         network_code, station_code, comp_code, channel_code = sta.split(\".\")         station_weight = \" \"         lat_degree = int(row[\"latitude\"])         lat_minute = (row[\"latitude\"] - lat_degree) * 60         north = \"N\" if lat_degree &gt;= 0 else \"S\"         lng_degree = int(row[\"longitude\"])         lng_minute = (row[\"longitude\"] - lng_degree) * 60         west = \"W\" if lng_degree &lt;= 0 else \"E\"         elevation = row[\"elevation_m\"]         line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"         converted_hypoinverse.append(line_hypoinverse)          # tmp_code = f\"{station_code}{channel_code}\"         tmp_code = f\"{station_code}\"         converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"       with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:         for k, v in converted_hypodd.items():             f.write(v)       ## Picks Format     picks_csv = f\"{data_path}/adloc_picks.csv\"     events_csv = f\"{data_path}/adloc_events.csv\"      picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     events = pd.read_csv(f\"{root_path}/{events_csv}\")     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")     events[\"time\"] = pd.to_datetime(events[\"time\"])     # events[\"magnitude\"] = 1.0     events[\"sigma_time\"] = 1.0      # events.sort_values(\"time\", inplace=True)     picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]      lines = []     picks_by_event = picks.groupby(\"event_index\").groups     for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):         # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")         event_time = event[\"time\"]         lat = event[\"latitude\"]         lng = event[\"longitude\"]         # dep = event[\"depth(m)\"] / 1e3 + shift_topo         dep = event[\"depth_km\"] + shift_topo         mag = event[\"magnitude\"]         EH = 0         EZ = 0         RMS = event[\"sigma_time\"]          year, month, day, hour, min, sec = (             event_time.year,             event_time.month,             event_time.day,             event_time.hour,             event_time.minute,             float(event_time.strftime(\"%S.%f\")),         )         event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"          lines.append(event_line)          picks_idx = picks_by_event[event[\"event_index\"]]         for j in picks_idx:             # pick = picks.iloc[j]             pick = picks.loc[j]             network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")             phase_type = pick[\"phase_type\"].upper()             phase_score = pick[\"phase_score\"]             # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()             pick_time = (pick[\"phase_time\"] - event_time).total_seconds()             tmp_code = f\"{station_code}\"             pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"             lines.append(pick_line)      with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:         fp.writelines(lines)      ## Run Hypodd     print(f\"Running Hypodd:\")     os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")      ## Read  catalog     columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]     catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(         lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',         axis=1,     )     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))     catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]     catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)     catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)     catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)      return catalog_ct_hypodd In\u00a0[20]: Copied! <pre>hypodd_catalog = run_hypodd(region=region)\n</pre> hypodd_catalog = run_hypodd(region=region) <pre>Convert catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1037/1037 [00:01&lt;00:00, 521.34it/s]\n+ WORKING_DIR=/workspaces/Earthquake_Catalog_Workshop/notebooks\n+ '[' 2 -eq 2 ']'\n+ root_path=local\n+ region=demo\n+ data_path=local/demo/hypodd\n+ '[' '!' -d local/demo/hypodd ']'\n+ cd local/demo/hypodd\n+ '[' '!' -d HypoDD ']'\n+ cat\n+ cat\n+ ./HypoDD/src/ph2dt/ph2dt ph2dt.inp\n</pre> <pre>Running Hypodd:\nstarting ph2dt (v2.1b - 08/2012)...     Tue Apr  8 05:59:13 2025\u0000\n\nreading data ...\n &gt; stations =           15\n &gt; events total =         1037\n &gt; events selected =         1004\n &gt; phases =        22764\nforming dtimes...\n &gt; stations selected =           14\n &gt; P-phase pairs total =       277169\n &gt; S-phase pairs total =       304154\n &gt; outliers =        21865  (           3 %)\n &gt; phases at stations not in station list =            0\n &gt; phases at distances larger than MAXDIST =            0\n &gt; P-phase pairs selected =       246115  (          88 %)\n &gt; S-phase pairs selected =       278561  (          91 %)\n &gt; weakly linked events =           45  (           4 %)\n &gt; linked event pairs =        30204\n &gt; average links per pair =           17\n &gt; average offset (km) betw. linked events =    2.58532858    \n &gt; average offset (km) betw. strongly linked events =    2.58532858    \n &gt; maximum offset (km) betw. strongly linked events =    9.97964859    \n\nDone.  Tue Apr  8 05:59:14 2025\u0000\n\nOutput files: dt.ct; event.dat; event.sel; station.sel; ph2dt.log\nph2dt parameters were: \n(minwght,maxdist,maxsep,maxngh,minlnk,minobs,maxobs)\n 0.00  200.000   10.000  50   8   8 100\nstarting hypoDD (v2.1beta - 06/15/2016)...   Tue Apr  8 05:59:14 2025\u0003\nINPUT FILES:\ncross dtime data:  \ncatalog dtime data: dt.ct\nevents: event.sel\nstations: stations.dat\nOUTPUT FILES:\ninitial locations: hypodd_ct.loc\nrelocated events: hypodd_ct.reloc\nevent pair residuals: hypodd.res\nstation residuals: hypodd.sta\nsource parameters: hypodd.src\n Relocate all clusters\n Relocate all events\nUse local layered 1D model.\nReading data ...   Tue Apr  8 05:59:14 2025 \n# events =  1004\n# stations &lt; maxdist =     15\n# stations w/ neg. elevation (set to 0) =    0\n</pre> <pre>+ ./HypoDD/src/hypoDD/hypoDD ct.inp\n</pre> <pre># catalog P dtimes =  246115\n# catalog S dtimes =  278561\n# dtimes total =   524676\n# events after dtime match =        989\n# stations =     14\n\nno clustering performed.\n\nRELOCATION OF CLUSTER: 1     Tue Apr  8 05:59:15 2025\u0003\n----------------------\nInitial trial sources =   989\n1D ray tracing.\n\n  IT   EV  CT    RMSCT   RMSST   DX   DY   DZ   DT   OS  AQ  CND\n        %   %   ms     %    ms    m    m    m   ms    m \n 1    100  99  156 -11.6     0  245  195 2242   64    0  22  250\n 2  1  98  97  153  -1.7   312  241  189 1548   61  357   0  245\n 3     98  95  134 -12.3   312  194  107  731   32  357  13  254\n 4  2  96  93  130  -2.9   277  213  103  584   30   21   0  274\n 5     96  92  116 -11.0   277  115   70 1000   18   21   8  269\n 6     96  91  115  -1.3   277  113   69  588   18   21   1  265\n 7  3  95  91  114  -0.4   255  114   69  585   18  376   0  265\n 8     95  90  108  -5.2   255   93   50  534   16  376   8  260\n 9  4  95  89  107  -0.7   246   93   50  475   15  825   0  259\n10     93  69   95 -11.7   246   72   59  555   17  825   4  224\n11  5  92  68   90  -5.5   193   74   61  363   17  830   0  223\n12  6  92  67   85  -5.5   186   49   37  322   11  862   0  219\n13     91  65   82  -3.7   186   34   27  224    7  862   1  218\n14  7  91  65   81  -0.8   178   35   27  166    7 1094   0  217\n15     91  64   80  -1.8   178   28   19  561    5 1094   6  205\n16     90  64   79  -0.6   178   27   19  442    5 1094   1  202\n17  8  90  64   79  -0.2   174   27   19  435    5  876   0  204\n18     87  27   67 -15.3   174   44   37  146    9  876   1  132\n19  9  87  27   61  -8.7   111   44   37  138    9 1237   0  131\n20     86  25   56  -8.7   111   28   24   83    5 1237   1  131\n21 10  86  25   55  -2.0   100   28   24   82    5 1223   0  128\n22     85  24   52  -4.1   100   19   15   67    4 1223   1  126\n23 11  85  24   52  -1.2    95   19   15   56    4 1226   0  126\n24     85  24   51  -2.3    95   16   11   46    3 1226   1  123\n25 12  85  23   50  -0.7    93   16   11   46    3 1253   0  123\n26 13  84  22   41 -17.6    72   17   14   47    3 1250   0  121\n27 14  84  21   38  -9.1    66   14   11   43    3 1250   0  118\n28 15  84  20   35  -5.9    61   12    9   40    2 1255   0  114\n29     84  20   34  -4.5    61   11    8   44    2 1255   1  111\n30 16  84  19   33  -1.8    58   12    9   45    2 1235   0  111\n\nwriting out results ...\n   Program hypoDD finished. Tue Apr  8 05:59:46 2025\u0003\n</pre> <pre>+ cd /workspaces/Earthquake_Catalog_Workshop/notebooks\n</pre> In\u00a0[21]: Copied! <pre>plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)\n</pre> plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)"},{"location":"notebooks/quakeflow/#earthquake-catalog-workshop","title":"Earthquake Catalog Workshop\u00b6","text":"<p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"notebooks/quakeflow/#machine-learning-part","title":"Machine Learning Part\u00b6","text":"<ol> <li><p>Download data using Obpsy and NCEDC/SCEDC AWS Public Dataset</p> <p>FDSN web service client for ObsPy</p> <p>NCEDC AWS Public Dataset</p> <p>SCEDC AWS Public Dataset</p> <p>Event Dataset (CEED)</p> <p>CEED paper</p> </li> <li><p>PhaseNet for P/S phase picking</p> <p>PhaseNet github page</p> <p>PhaseNet paper</p> </li> <li><p>GaMMA for phase association</p> <p>GaMMA github page</p> <p>GaMMA paper</p> </li> <li><p>ADLoc for earthquake location</p> <p>ADLoc github page</p> <p>ADLoc paper</p> </li> <li><p>HypoDD for earthquake relocation</p> <p>HypoDD github page</p> <p>HypoDD paper</p> </li> <li><p>QuakeFlow</p> <p>QuakeFlow github page</p> <p>QuakeFlow paper</p> </li> </ol>"},{"location":"notebooks/quakeflow/#setup-environment","title":"Setup Environment\u00b6","text":""},{"location":"notebooks/quakeflow/#setup-configurations","title":"Setup configurations\u00b6","text":""},{"location":"notebooks/quakeflow/#download-the-standard-catalog-for-comparison","title":"Download the standard catalog for comparison\u00b6","text":""},{"location":"notebooks/quakeflow/#download-stations","title":"Download stations\u00b6","text":""},{"location":"notebooks/quakeflow/#download-waveform-data","title":"Download waveform data\u00b6","text":""},{"location":"notebooks/quakeflow/#run-phasenet-to-pick-ps-picks","title":"Run PhaseNet to pick P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow/#run-gamma-to-associate-ps-picks","title":"Run GaMMA to associate P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow/#run-adloc-to-locate-absolute-earthquake-locations","title":"Run ADLoc to locate absolute earthquake locations\u00b6","text":""},{"location":"notebooks/quakeflow/#run-hypodd-to-relocate-relative-earthquake-locations","title":"Run HypoDD to relocate relative earthquake locations\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/","title":"Quakeflow hawaii","text":"In\u00a0[1]: Copied! <pre># !conda env create -f env.yaml --prefix .conda/quakeflow\n# !source activate .conda/quakeflow\n# !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/\n# !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../\n# !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git\n# !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git\n</pre> # !conda env create -f env.yaml --prefix .conda/quakeflow # !source activate .conda/quakeflow # !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/ # !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../ # !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git # !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git In\u00a0[2]: Copied! <pre># %%\nimport json\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom typing import Dict\nfrom glob import glob\n\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport fsspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport obspy\nimport obspy.clients.fdsn\nimport pandas as pd\nfrom obspy.clients.fdsn.mass_downloader import (\n    CircularDomain,\n    MassDownloader,\n    Restrictions,\n)\nfrom pyproj import Proj\nfrom tqdm import tqdm\n\nfrom adloc.eikonal2d import init_eikonal2d\nfrom adloc.sacloc2d import ADLoc\nfrom adloc.utils import invert_location\nfrom gamma.utils import association, estimate_eps\n</pre> # %% import json import os import pickle from collections import defaultdict from datetime import datetime from typing import Dict from glob import glob  import cartopy.crs as ccrs import cartopy.feature as cfeature import fsspec import matplotlib.pyplot as plt import numpy as np import obspy import obspy.clients.fdsn import pandas as pd from obspy.clients.fdsn.mass_downloader import (     CircularDomain,     MassDownloader,     Restrictions, ) from pyproj import Proj from tqdm import tqdm  from adloc.eikonal2d import init_eikonal2d from adloc.sacloc2d import ADLoc from adloc.utils import invert_location from gamma.utils import association, estimate_eps  In\u00a0[3]: Copied! <pre>def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:\n\n    if not os.path.exists(f\"{root_path}/{region}\"):\n        os.makedirs(f\"{root_path}/{region}\", exist_ok=True)\n\n    regions = {\n        \"demo\": {\n            \"longitude0\": -117.504,\n            \"latitude0\": 35.705,\n            \"maxradius_degree\": 0.5,\n            \"mindepth\": 0,\n            \"maxdepth\": 30,\n            \"starttime\": \"2019-07-04T00:00:00\",\n            \"endtime\": \"2019-07-05T00:00:00\",\n            \"network\": \"CI\",\n            \"channel\": \"HH*,BH*,EH*,HN*\",\n            \"provider\": [\"SCEDC\"]\n        },\n        \"ridgecrest\": {\n            \"longitude0\": -117.504,\n            \"latitude0\": 35.705,\n            \"maxradius_degree\": 0.5,\n            \"mindepth\": 0,\n            \"maxdepth\": 30,\n            \"starttime\": \"2019-07-04T00:00:00\",\n            \"endtime\": \"2019-07-10T00:00:00\",\n            \"network\": \"CI\",\n            \"channel\": \"HH*,BH*,EH*,HN*\",\n            \"provider\": [\"SCEDC\"]\n        },\n        \"hawaii\": {\n            \"longitude0\": -155.32,\n            \"latitude0\": 19.39,\n            \"maxradius_degree\": 0.5,\n            \"mindepth\": 0,\n            \"maxdepth\": 60,\n            \"starttime\": \"2024-02-09T00:00:00\",\n            \"endtime\": \"2024-02-11T00:00:00\",\n            \"network\": \"HV,PT\",\n            \"channel\": \"HH*,EH*\",\n            \"provider\": [\"IRIS\"]\n        }\n    }\n\n    ## Set config\n    config = regions[region.lower()]\n\n    ## PhaseNet\n    config[\"phasenet\"] = {}\n    ## GaMMA\n    config[\"gamma\"] = {}\n    ## ADLoc\n    config[\"adloc\"] = {}\n    ## HypoDD\n    config[\"hypodd\"] = {}\n\n    with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:\n        json.dump(config, fp, indent=2)\n\n    print(json.dumps(config, indent=4))\n\n    return config\n</pre> def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:      if not os.path.exists(f\"{root_path}/{region}\"):         os.makedirs(f\"{root_path}/{region}\", exist_ok=True)      regions = {         \"demo\": {             \"longitude0\": -117.504,             \"latitude0\": 35.705,             \"maxradius_degree\": 0.5,             \"mindepth\": 0,             \"maxdepth\": 30,             \"starttime\": \"2019-07-04T00:00:00\",             \"endtime\": \"2019-07-05T00:00:00\",             \"network\": \"CI\",             \"channel\": \"HH*,BH*,EH*,HN*\",             \"provider\": [\"SCEDC\"]         },         \"ridgecrest\": {             \"longitude0\": -117.504,             \"latitude0\": 35.705,             \"maxradius_degree\": 0.5,             \"mindepth\": 0,             \"maxdepth\": 30,             \"starttime\": \"2019-07-04T00:00:00\",             \"endtime\": \"2019-07-10T00:00:00\",             \"network\": \"CI\",             \"channel\": \"HH*,BH*,EH*,HN*\",             \"provider\": [\"SCEDC\"]         },         \"hawaii\": {             \"longitude0\": -155.32,             \"latitude0\": 19.39,             \"maxradius_degree\": 0.5,             \"mindepth\": 0,             \"maxdepth\": 60,             \"starttime\": \"2024-02-09T00:00:00\",             \"endtime\": \"2024-02-11T00:00:00\",             \"network\": \"HV,PT\",             \"channel\": \"HH*,EH*\",             \"provider\": [\"IRIS\"]         }     }      ## Set config     config = regions[region.lower()]      ## PhaseNet     config[\"phasenet\"] = {}     ## GaMMA     config[\"gamma\"] = {}     ## ADLoc     config[\"adloc\"] = {}     ## HypoDD     config[\"hypodd\"] = {}      with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:         json.dump(config, fp, indent=2)      print(json.dumps(config, indent=4))      return config In\u00a0[4]: Copied! <pre>region = \"hawaii\"\nconfig  = set_config(region = region)\n</pre> region = \"hawaii\" config  = set_config(region = region) <pre>{\n    \"longitude0\": -155.32,\n    \"latitude0\": 19.39,\n    \"maxradius_degree\": 0.5,\n    \"mindepth\": 0,\n    \"maxdepth\": 60,\n    \"starttime\": \"2024-02-09T00:00:00\",\n    \"endtime\": \"2024-02-11T00:00:00\",\n    \"network\": \"HV,PT\",\n    \"channel\": \"HH*,EH*\",\n    \"provider\": [\n        \"IRIS\"\n    ],\n    \"phasenet\": {},\n    \"gamma\": {},\n    \"adloc\": {},\n    \"hypodd\": {}\n}\n</pre> In\u00a0[5]: Copied! <pre>def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure()\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        catalog['longitude'],\n        catalog['latitude'],\n        c=catalog['depth_km'],\n        cmap='viridis_r',\n        s=1,\n        alpha=0.6,\n        vmin = config[\"mindepth\"],\n        vmax = config[\"maxdepth\"]/2,\n        transform=ccrs.PlateCarree()\n    )\n\n    plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'{method} Catalog ({len(catalog)})')\n    plt.show()\n\ndef plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure()\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        stations['longitude'],\n        stations['latitude'],\n        c=\"C0\",\n        s=40,\n        marker=\"^\",\n        alpha=0.6,\n        transform=ccrs.PlateCarree()\n    )\n    if catalog is not None:\n        scatter = ax.scatter(\n            catalog['longitude'],\n            catalog['latitude'],\n            c=catalog['depth_km'],\n            cmap='viridis_r',\n            s=1,\n            alpha=0.6,\n            vmin = config[\"mindepth\"],\n            vmax = config[\"maxdepth\"]/2,\n            transform=ccrs.PlateCarree()\n        )\n        plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'Stations ({len(stations[\"station\"].unique())})')\n    plt.show()\n\ndef download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_path = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download catalog \n    client = obspy.clients.fdsn.Client(\"usgs\")\n    events = client.get_events(\n        starttime=config[\"starttime\"],\n        endtime=config[\"endtime\"],\n        latitude=config[\"latitude0\"],\n        longitude=config[\"longitude0\"],\n        maxradius=config[\"maxradius_degree\"],\n    )\n    print(f\"Number of events: {len(events)}\")\n\n    ## Save catalog\n    catalog = defaultdict(list)\n    for event in events:\n        if len(event.magnitudes) &gt; 0:\n            catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))\n            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n            catalog[\"longitude\"].append(event.origins[0].longitude)\n            catalog[\"latitude\"].append(event.origins[0].latitude)\n            catalog[\"depth_km\"].append(event.origins[0].depth/1e3)\n    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n    catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)\n\n    return catalog\n</pre> def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure()     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         catalog['longitude'],         catalog['latitude'],         c=catalog['depth_km'],         cmap='viridis_r',         s=1,         alpha=0.6,         vmin = config[\"mindepth\"],         vmax = config[\"maxdepth\"]/2,         transform=ccrs.PlateCarree()     )      plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'{method} Catalog ({len(catalog)})')     plt.show()  def plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure()     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         stations['longitude'],         stations['latitude'],         c=\"C0\",         s=40,         marker=\"^\",         alpha=0.6,         transform=ccrs.PlateCarree()     )     if catalog is not None:         scatter = ax.scatter(             catalog['longitude'],             catalog['latitude'],             c=catalog['depth_km'],             cmap='viridis_r',             s=1,             alpha=0.6,             vmin = config[\"mindepth\"],             vmax = config[\"maxdepth\"]/2,             transform=ccrs.PlateCarree()         )         plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'Stations ({len(stations[\"station\"].unique())})')     plt.show()  def download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_path = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")     # print(json.dumps(config, indent=4))      ## Download catalog      client = obspy.clients.fdsn.Client(\"usgs\")     events = client.get_events(         starttime=config[\"starttime\"],         endtime=config[\"endtime\"],         latitude=config[\"latitude0\"],         longitude=config[\"longitude0\"],         maxradius=config[\"maxradius_degree\"],     )     print(f\"Number of events: {len(events)}\")      ## Save catalog     catalog = defaultdict(list)     for event in events:         if len(event.magnitudes) &gt; 0:             catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))             catalog[\"magnitude\"].append(event.magnitudes[0].mag)             catalog[\"longitude\"].append(event.origins[0].longitude)             catalog[\"latitude\"].append(event.origins[0].latitude)             catalog[\"depth_km\"].append(event.origins[0].depth/1e3)     catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])     catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)      return catalog In\u00a0[6]: Copied! <pre>standard_catalog = download_catalog(region=region, config=config)\nplot_catalog(standard_catalog, method=\"Standard\", config=config)\n</pre> standard_catalog = download_catalog(region=region, config=config) plot_catalog(standard_catalog, method=\"Standard\", config=config) <pre>Number of events: 159\n</pre> In\u00a0[7]: Copied! <pre>def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_dir = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_dir}\"):\n        os.makedirs(f\"{root_path}/{result_dir}\")\n    if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):\n        os.makedirs(f\"{root_path}/{result_dir}/inventory/\")\n\n    ## Download stations\n    stations = obspy.core.inventory.Inventory()\n    for provider in config[\"provider\"]:\n        client = obspy.clients.fdsn.Client(provider)\n        stations += client.get_stations(\n                network=config[\"network\"],\n                station=\"*\",\n                starttime=config[\"starttime\"],\n                endtime=config[\"endtime\"],\n                latitude=config[\"latitude0\"],\n                longitude=config[\"longitude0\"],\n                maxradius=config[\"maxradius_degree\"],\n                channel=config[\"channel\"],\n                level=\"response\",\n            )\n    stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")\n    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n\n    ## Save stations\n    station_dict = defaultdict(dict)\n    for network in stations:\n        for station in network:\n            inv = stations.select(network=network.code, station=station.code)\n            inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")\n            for channel in station:\n                sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"\n                station_dict[sid] = {\n                    \"network\": network.code,\n                    \"station\": station.code,\n                    \"location\": channel.location_code,\n                    \"channel\": channel.code,\n                    \"longitude\": channel.longitude,\n                    \"latitude\": channel.latitude,\n                    \"elevation_m\": channel.elevation,\n                    \"response\": round(channel.response.instrument_sensitivity.value, 2),\n                }\n\n    # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:\n    #     json.dump(station_dict, fp, indent=2)\n\n    with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:\n        pickle.dump(stations, fp)\n\n    stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")\n    stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)\n        \n    return stations\n</pre> def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_dir = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_dir}\"):         os.makedirs(f\"{root_path}/{result_dir}\")     if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):         os.makedirs(f\"{root_path}/{result_dir}/inventory/\")      ## Download stations     stations = obspy.core.inventory.Inventory()     for provider in config[\"provider\"]:         client = obspy.clients.fdsn.Client(provider)         stations += client.get_stations(                 network=config[\"network\"],                 station=\"*\",                 starttime=config[\"starttime\"],                 endtime=config[\"endtime\"],                 latitude=config[\"latitude0\"],                 longitude=config[\"longitude0\"],                 maxradius=config[\"maxradius_degree\"],                 channel=config[\"channel\"],                 level=\"response\",             )     stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")     print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))      ## Save stations     station_dict = defaultdict(dict)     for network in stations:         for station in network:             inv = stations.select(network=network.code, station=station.code)             inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")             for channel in station:                 sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"                 station_dict[sid] = {                     \"network\": network.code,                     \"station\": station.code,                     \"location\": channel.location_code,                     \"channel\": channel.code,                     \"longitude\": channel.longitude,                     \"latitude\": channel.latitude,                     \"elevation_m\": channel.elevation,                     \"response\": round(channel.response.instrument_sensitivity.value, 2),                 }      # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:     #     json.dump(station_dict, fp, indent=2)      with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:         pickle.dump(stations, fp)      stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")     stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)              return stations In\u00a0[8]: Copied! <pre>stations = download_station(region=region, config=config)\nplot_stations(stations, catalog = standard_catalog, region=region, config=config)\n</pre> stations = download_station(region=region, config=config) plot_stations(stations, catalog = standard_catalog, region=region, config=config) <pre>Number of stations: 57\n</pre> In\u00a0[9]: Copied! <pre>def map_remote_path(provider, bucket, starttime, network, station, location, channel):\n\n    starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx\n    if provider.lower() == \"scedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        if location == \"\":\n            location = \"__\"\n        path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"\n    elif provider.lower() == \"ncedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"\n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n    return path\n\ndef download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    waveform_dir = f\"{region}/waveforms\"\n    if not os.path.exists(f\"{root_path}/{waveform_dir}\"):\n        os.makedirs(f\"{root_path}/{waveform_dir}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download from cloud\n    for provider in config[\"provider\"]:\n        if provider.lower() in [\"scedc\", \"ncedc\"]:\n            cloud = {\n                \"provider\": provider.lower(),\n                \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",\n            }\n        else:\n            continue\n\n        DELTATIME = \"1D\"\n        starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")\n        starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()\n        # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:\n        #     stations = json.load(f)\n        stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n        stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])\n        stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])\n        location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04', \n                              '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')\n        location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}\n        instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')\n        instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}\n        component_priorities = ('E', 'N', 'Z', '1', '2', '3')\n        component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}\n        stations['location_priority'] = stations['location'].map(location_priority_map)\n        stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))\n        stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))\n        stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)\n        stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)\n\n        for starttime in starttimes:\n            for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):\n                prev = \"\"\n                nch = set()\n                for _, row in station.iterrows():\n                    if len(nch) &gt;= 3:\n                        break\n                    network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]\n                    if instrument != prev:\n                        prev = instrument\n                        nch = set()\n                    mseed_path = map_remote_path(\n                        cloud[\"provider\"],\n                        cloud[\"bucket\"],\n                        starttime,\n                        network,\n                        station,\n                        location,\n                        channel,\n                    )\n                    try:\n                        if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):\n                            # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")\n                            nch.add(channel[-1])\n                            continue\n                        if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):\n                            os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")\n                        with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:\n                            data = f.read()\n                        with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:\n                            f.write(data)\n                            nch.add(channel[-1])\n                    except Exception as e:\n                        # print(f\"Failed to download {e}\")\n                        pass\n\n    # %% Download from FDSN\n    domain = CircularDomain(\n        longitude=config[\"longitude0\"],\n        latitude=config[\"latitude0\"],\n        minradius=0,\n        maxradius=config[\"maxradius_degree\"],\n    )\n\n    restrictions = Restrictions(\n        starttime=obspy.UTCDateTime(config[\"starttime\"]),\n        endtime=obspy.UTCDateTime(config[\"endtime\"]),\n        chunklength_in_sec=3600 * 24, # 1 day\n        network=config[\"network\"] if \"network\" in config else None,\n        station=config[\"station\"] if \"station\" in config else None,\n        minimum_interstation_distance_in_m=0,\n        minimum_length=0.1,\n        reject_channels_with_gaps=False,\n    )\n\n    def get_mseed_storage(network, station, location, channel, starttime, endtime):\n        mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"\n        if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):\n            # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")\n            return True\n        return f\"{root_path}/{waveform_dir}/{mseed_name}\"\n\n    mdl = MassDownloader(\n        providers=config[\"provider\"],\n        # providers=[\"IRIS\"],\n    )\n    mdl.download(\n        domain,\n        restrictions,\n        mseed_storage=get_mseed_storage,\n        stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",\n        download_chunk_size_in_mb=20,\n        threads_per_client=3,\n        print_report=False,\n    )\n    \n    return\n</pre> def map_remote_path(provider, bucket, starttime, network, station, location, channel):      starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx     if provider.lower() == \"scedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         if location == \"\":             location = \"__\"         path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"     elif provider.lower() == \"ncedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"     else:         raise ValueError(f\"Unknown provider: {provider}\")     return path  def download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      waveform_dir = f\"{region}/waveforms\"     if not os.path.exists(f\"{root_path}/{waveform_dir}\"):         os.makedirs(f\"{root_path}/{waveform_dir}\")     # print(json.dumps(config, indent=4))      ## Download from cloud     for provider in config[\"provider\"]:         if provider.lower() in [\"scedc\", \"ncedc\"]:             cloud = {                 \"provider\": provider.lower(),                 \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",             }         else:             continue          DELTATIME = \"1D\"         starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")         starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()         # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:         #     stations = json.load(f)         stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)         stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])         stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])         location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04',                                '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')         location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}         instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')         instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}         component_priorities = ('E', 'N', 'Z', '1', '2', '3')         component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}         stations['location_priority'] = stations['location'].map(location_priority_map)         stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))         stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))         stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)         stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)          for starttime in starttimes:             for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):                 prev = \"\"                 nch = set()                 for _, row in station.iterrows():                     if len(nch) &gt;= 3:                         break                     network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]                     if instrument != prev:                         prev = instrument                         nch = set()                     mseed_path = map_remote_path(                         cloud[\"provider\"],                         cloud[\"bucket\"],                         starttime,                         network,                         station,                         location,                         channel,                     )                     try:                         if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):                             # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")                             nch.add(channel[-1])                             continue                         if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):                             os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")                         with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:                             data = f.read()                         with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:                             f.write(data)                             nch.add(channel[-1])                     except Exception as e:                         # print(f\"Failed to download {e}\")                         pass      # %% Download from FDSN     domain = CircularDomain(         longitude=config[\"longitude0\"],         latitude=config[\"latitude0\"],         minradius=0,         maxradius=config[\"maxradius_degree\"],     )      restrictions = Restrictions(         starttime=obspy.UTCDateTime(config[\"starttime\"]),         endtime=obspy.UTCDateTime(config[\"endtime\"]),         chunklength_in_sec=3600 * 24, # 1 day         network=config[\"network\"] if \"network\" in config else None,         station=config[\"station\"] if \"station\" in config else None,         minimum_interstation_distance_in_m=0,         minimum_length=0.1,         reject_channels_with_gaps=False,     )      def get_mseed_storage(network, station, location, channel, starttime, endtime):         mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"         if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):             # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")             return True         return f\"{root_path}/{waveform_dir}/{mseed_name}\"      mdl = MassDownloader(         providers=config[\"provider\"],         # providers=[\"IRIS\"],     )     mdl.download(         domain,         restrictions,         mseed_storage=get_mseed_storage,         stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",         download_chunk_size_in_mb=20,         threads_per_client=3,         print_report=False,     )          return  In\u00a0[10]: Copied! <pre>download_waveform(region=region, config=config)\n</pre> download_waveform(region=region, config=config) <pre>[2025-04-07 19:25:08,525] - obspy.clients.fdsn.mass_downloader - INFO: Initializing FDSN client(s) for IRIS.\n[2025-04-07 19:25:08,539] - obspy.clients.fdsn.mass_downloader - INFO: Successfully initialized 1 client(s): IRIS.\n[2025-04-07 19:25:08,540] - obspy.clients.fdsn.mass_downloader - INFO: Total acquired or preexisting stations: 0\n[2025-04-07 19:25:08,541] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Requesting reliable availability.\n[2025-04-07 19:25:09,365] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully requested availability (0.82 seconds)\n[2025-04-07 19:25:09,403] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Found 57 stations (134 channels).\n[2025-04-07 19:25:09,405] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Will attempt to download data from 57 stations.\n[2025-04-07 19:25:09,415] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Status for 72 time intervals/channels before downloading: IGNORE\n[2025-04-07 19:25:09,417] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Status for 196 time intervals/channels before downloading: NEEDS_DOWNLOADING\n[2025-04-07 19:25:26,488] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:27,065] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:36,537] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:44,817] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:45,144] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:45,553] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:50,382] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:50,853] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:52,595] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:56,688] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:25:56,974] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:00,699] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:03,825] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:04,603] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:07,087] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:09,629] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:10,042] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:10,451] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:10,855] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:11,277] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:11,520] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:11,631] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:12,085] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:12,688] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:15,980] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:18,027] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:18,387] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:31,819] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:40,080] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:40,508] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:40,934] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:41,362] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:41,789] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:42,266] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:42,642] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:47,604] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:48,030] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:48,056] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:26:48,457] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:48,886] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:49,314] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:49,740] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:50,166] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:26:58,301] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:05,039] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:07,170] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:10,619] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:14,771] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:23,863] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:34,077] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:39,258] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:49,643] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:27:50,070] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:27:58,233] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:05,432] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:06,567] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:19,481] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:21,135] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:31,410] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:37,064] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:46,591] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:54,033] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:28:57,928] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:13,092] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:20,099] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:20,363] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:38,764] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:39,194] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:39,216] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:39,622] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:40,054] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:40,483] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:40,912] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:41,338] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:29:42,687] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:29:52,145] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:30:02,841] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:30:06,288] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:30:28,952] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:30:29,862] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:00,697] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:01,449] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:03,718] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:25,767] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:26,216] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:31:26,613] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:31:29,855] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:30,464] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:33,584] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:36,165] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:52,579] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:59,438] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:31:59,849] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:21,310] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:23,853] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:27,097] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:27,913] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:29,680] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:33,658] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:34,102] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:49,269] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:32:55,334] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:03,911] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:16,214] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:18,934] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:34,012] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:37,589] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:38,853] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:39,970] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:43,194] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:43,291] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:47,591] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:48,104] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:48,314] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:33:48,582] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:34:03,675] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:03,971] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:14,012] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:18,645] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:34,903] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:39,672] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:34:44,941] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:06,525] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:06,554] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:10,665] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:21,787] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:38,115] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:39,406] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:47,415] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:50,543] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:53,256] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:55,871] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:35:58,370] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:04,429] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:10,400] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:14,850] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:15,612] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:16,717] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:18,817] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:19,626] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:23,428] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:23,652] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:32,706] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:46,821] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:52,208] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:36:59,884] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:00,311] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:00,738] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:01,168] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:01,594] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:02,021] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:02,448] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:37:09,610] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:12,170] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:15,227] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:17,427] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:24,552] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:32,159] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:44,029] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:52,626] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:37:57,465] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:12,170] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:24,618] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:24,969] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:46,806] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:49,822] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:38:51,952] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:04,933] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:06,655] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:19,793] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:22,159] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:22,223] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:34,676] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:36,814] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:49,311] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:39:51,420] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:04,041] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:10,884] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:18,860] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:35,512] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:38,455] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:40:52,243] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:41:04,912] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:41:20,948] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:41:31,792] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:41:49,022] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:41:57,650] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:42:15,542] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:42:21,181] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:42:41,446] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:42:44,230] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:43:06,436] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:43:09,168] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:43:09,620] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - No data available for request.\nHTTP Status code: 204\n[2025-04-07 19:43:15,810] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:43:19,389] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 1 channels (of 1)\n[2025-04-07 19:43:19,390] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Launching basic QC checks...\n[2025-04-07 19:43:21,135] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Downloaded 1575.5 MB [1480.12 KB/sec] of data, 0.0 MB of which were discarded afterwards.\n[2025-04-07 19:43:21,136] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Status for 35 time intervals/channels after downloading: DOWNLOAD_FAILED\n[2025-04-07 19:43:21,136] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Status for 161 time intervals/channels after downloading: DOWNLOADED\n[2025-04-07 19:43:21,137] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Status for 72 time intervals/channels after downloading: IGNORE\n[2025-04-07 19:43:21,234] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.DESD.xml'.\n[2025-04-07 19:43:21,237] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.AHUD.xml'.\n[2025-04-07 19:43:21,251] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.HOVE.xml'.\n[2025-04-07 19:43:21,315] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.HPUD.xml'.\n[2025-04-07 19:43:21,333] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.DEVL.xml'.\n[2025-04-07 19:43:21,336] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.AIND.xml'.\n[2025-04-07 19:43:21,379] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.HTCD.xml'.\n[2025-04-07 19:43:21,406] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.ALEP.xml'.\n[2025-04-07 19:43:21,432] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.ERZ3.xml'.\n[2025-04-07 19:43:21,462] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.JCUZ.xml'.\n[2025-04-07 19:43:21,495] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.CPKD.xml'.\n[2025-04-07 19:43:21,532] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.HAT.xml'.\n[2025-04-07 19:43:21,548] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.JOKA.xml'.\n[2025-04-07 19:43:21,558] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.DAND.xml'.\n[2025-04-07 19:43:21,612] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KAED.xml'.\n[2025-04-07 19:43:21,616] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.HLPD.xml'.\n[2025-04-07 19:43:21,622] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KLUD.xml'.\n[2025-04-07 19:43:21,667] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KUPD.xml'.\n[2025-04-07 19:43:21,714] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KIND.xml'.\n[2025-04-07 19:43:21,715] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.NAHU.xml'.\n[2025-04-07 19:43:21,731] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.MITD.xml'.\n[2025-04-07 19:43:21,799] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KKO.xml'.\n[2025-04-07 19:43:21,801] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.NPOC.xml'.\n[2025-04-07 19:43:21,837] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.MLOD.xml'.\n[2025-04-07 19:43:21,843] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KKUD.xml'.\n[2025-04-07 19:43:21,902] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.OTLD.xml'.\n[2025-04-07 19:43:21,920] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.NAGD.xml'.\n[2025-04-07 19:43:21,943] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.KLPN.xml'.\n[2025-04-07 19:43:21,967] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.PLAD.xml'.\n[2025-04-07 19:43:21,984] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.P07.xml'.\n[2025-04-07 19:43:22,026] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.RIMD.xml'.\n[2025-04-07 19:43:22,066] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.PPLD.xml'.\n[2025-04-07 19:43:22,070] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.PAUD.xml'.\n[2025-04-07 19:43:22,126] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.RSDD.xml'.\n[2025-04-07 19:43:22,137] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.TOUO.xml'.\n[2025-04-07 19:43:22,201] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.TRAD.xml'.\n[2025-04-07 19:43:22,201] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.PUHI.xml'.\n[2025-04-07 19:43:22,207] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.SBL.xml'.\n[2025-04-07 19:43:22,284] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.UWB.xml'.\n[2025-04-07 19:43:22,291] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.SDH.xml'.\n[2025-04-07 19:43:22,302] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.R931.xml'.\n[2025-04-07 19:43:22,356] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.SWRD.xml'.\n[2025-04-07 19:43:22,364] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.RCOD.xml'.\n[2025-04-07 19:43:22,367] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.UWE.xml'.\n[2025-04-07 19:43:22,421] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.WOOD.xml'.\n[2025-04-07 19:43:22,452] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.WILD.xml'.\n[2025-04-07 19:43:22,519] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/HV.WRM.xml'.\n[2025-04-07 19:43:22,583] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/PT.HILB.xml'.\n[2025-04-07 19:43:22,652] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/PT.KHU.xml'.\n[2025-04-07 19:43:22,716] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Successfully downloaded 'local/hawaii/waveforms/stations/PT.MLOA.xml'.\n[2025-04-07 19:43:22,769] - obspy.clients.fdsn.mass_downloader - INFO: Client 'IRIS' - Downloaded 50 station files [2.7 MB] in 1.6 seconds [1759.35 KB/sec].\n</pre> In\u00a0[11]: Copied! <pre>def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:\n\n    result_path = f\"{region}/phasenet\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    # %%\n    waveform_dir = f\"{region}/waveforms\"\n    mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))\n\n    # %% group 3C channels\n    mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))\n\n    # %%\n    with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:\n        fp.write(\"fname\\n\")\n        fp.write(\"\\n\".join(mseed_list))\n\n    # %%\n    model_path = \"QuakeFlow/PhaseNet/\"\n    cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"\n    # cmd += \" --sampling_rate 100\" \n    os.system(cmd)\n\n    return f\"{root_path}/{result_path}/phasenet_picks.csv\"\n</pre> def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:      result_path = f\"{region}/phasenet\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      # %%     waveform_dir = f\"{region}/waveforms\"     mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))      # %% group 3C channels     mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))      # %%     with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:         fp.write(\"fname\\n\")         fp.write(\"\\n\".join(mseed_list))      # %%     model_path = \"QuakeFlow/PhaseNet/\"     cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"     # cmd += \" --sampling_rate 100\"      os.system(cmd)      return f\"{root_path}/{result_path}/phasenet_picks.csv\"  In\u00a0[12]: Copied! <pre>phasenet_picks = run_phasenet(region=region, config=config)\n</pre> phasenet_picks = run_phasenet(region=region, config=config) <pre>2025-04-07 19:43:23.105108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-07 19:43:25,208 Pred log: local/hawaii/phasenet\n2025-04-07 19:43:25,208 Dataset size: 99\n2025-04-07 19:43:25.261385: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-07 19:43:25.263471: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-07 19:43:25,285 Model: depths 5, filters 8, filter size 7x1, pool size: 4x1, dilation rate: 1x1\n2025-04-07 19:43:26.497089: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-07 19:43:26.573390: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n2025-04-07 19:43:26,737 restoring model QuakeFlow/PhaseNet//model/190703-214543/model_95.ckpt\nPred: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 99/99 [09:05&lt;00:00,  5.51s/it]\n</pre> <pre>Done with 42489 P-picks and 38254 S-picks\n</pre> In\u00a0[13]: Copied! <pre>def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{region}/phasenet\"\n    result_path = f\"{region}/gamma\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    picks_csv = f\"{data_path}/phasenet_picks.csv\"\n    gamma_events_csv = f\"{result_path}/gamma_events.csv\"\n    gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"\n    station_json = f\"{region}/obspy/stations.json\"\n\n    ## read picks\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")\n    picks[\"id\"] = picks[\"station_id\"]\n    picks[\"timestamp\"] = picks[\"phase_time\"]\n    picks[\"amp\"] = picks[\"phase_amplitude\"]\n    picks[\"type\"] = picks[\"phase_type\"]\n    picks[\"prob\"] = picks[\"phase_score\"]\n\n    ## read stations\n    # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")\n    stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n    stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")\n    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n    # print(stations.to_string())\n\n    ## setting GaMMA configs\n    config[\"use_dbscan\"] = True\n    config[\"use_amplitude\"] = True\n    config[\"method\"] = \"BGMM\"\n    if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n        config[\"oversample_factor\"] = 5\n    if config[\"method\"] == \"GMM\":  ## GaussianMixture\n        config[\"oversample_factor\"] = 1\n\n    # earthquake location\n    config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\n    config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    zmin = config[\"mindepth\"] if \"mindepth\" in config else 0\n    zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30\n    config[\"x(km)\"] = (xmin, xmax)\n    config[\"y(km)\"] = (ymin, ymax)\n    config[\"z(km)\"] = (zmin, zmax)\n    config[\"bfgs_bounds\"] = (\n        (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n        (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n        (0, config[\"z(km)\"][1] + 1),  # z\n        (None, None),  # t\n    )\n\n    # DBSCAN\n    config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s\n    config[\"dbscan_min_samples\"] = 3\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 0.3\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\n\n    # filtering\n    config[\"min_picks_per_eq\"] = 5\n    config[\"min_p_picks_per_eq\"] = 0\n    config[\"min_s_picks_per_eq\"] = 0\n    config[\"max_sigma11\"] = 2.0  # s\n    config[\"max_sigma22\"] = 1.0  # log10(m/s)\n    config[\"max_sigma12\"] = 1.0  # covariance\n\n    ## filter picks without amplitude measurements\n    if config[\"use_amplitude\"]:\n        picks = picks[picks[\"amp\"] != -1]\n\n    # for k, v in config.items():\n    #     print(f\"{k}: {v}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n\n    # %%\n    event_idx0 = 0  ## current earthquake index\n    assignments = []\n    events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\n\n    if len(events) == 0:\n        return \n    \n    ## create catalog\n    events = pd.DataFrame(events)\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z(km)\"]\n    events.sort_values(\"time\", inplace=True)\n    with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:\n        events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n    ## add assignment to picks\n    assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n    picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n    picks.sort_values([\"phase_time\"], inplace=True)\n    with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:\n        picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n\n    # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"\n    return events\n</pre> def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{region}/phasenet\"     result_path = f\"{region}/gamma\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      picks_csv = f\"{data_path}/phasenet_picks.csv\"     gamma_events_csv = f\"{result_path}/gamma_events.csv\"     gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"     station_json = f\"{region}/obspy/stations.json\"      ## read picks     picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")     picks[\"id\"] = picks[\"station_id\"]     picks[\"timestamp\"] = picks[\"phase_time\"]     picks[\"amp\"] = picks[\"phase_amplitude\"]     picks[\"type\"] = picks[\"phase_type\"]     picks[\"prob\"] = picks[\"phase_score\"]      ## read stations     # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")     stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)     stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")     stations[[\"x(km)\", \"y(km)\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)     # print(stations.to_string())      ## setting GaMMA configs     config[\"use_dbscan\"] = True     config[\"use_amplitude\"] = True     config[\"method\"] = \"BGMM\"     if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture         config[\"oversample_factor\"] = 5     if config[\"method\"] == \"GMM\":  ## GaussianMixture         config[\"oversample_factor\"] = 1      # earthquake location     config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}     config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]     minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     zmin = config[\"mindepth\"] if \"mindepth\" in config else 0     zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30     config[\"x(km)\"] = (xmin, xmax)     config[\"y(km)\"] = (ymin, ymax)     config[\"z(km)\"] = (zmin, zmax)     config[\"bfgs_bounds\"] = (         (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x         (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y         (0, config[\"z(km)\"][1] + 1),  # z         (None, None),  # t     )      # DBSCAN     config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s     config[\"dbscan_min_samples\"] = 3      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 0.3     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}      # filtering     config[\"min_picks_per_eq\"] = 5     config[\"min_p_picks_per_eq\"] = 0     config[\"min_s_picks_per_eq\"] = 0     config[\"max_sigma11\"] = 2.0  # s     config[\"max_sigma22\"] = 1.0  # log10(m/s)     config[\"max_sigma12\"] = 1.0  # covariance      ## filter picks without amplitude measurements     if config[\"use_amplitude\"]:         picks = picks[picks[\"amp\"] != -1]      # for k, v in config.items():     #     print(f\"{k}: {v}\")      print(f\"Number of picks: {len(picks)}\")      # %%     event_idx0 = 0  ## current earthquake index     assignments = []     events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])      if len(events) == 0:         return           ## create catalog     events = pd.DataFrame(events)     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z(km)\"]     events.sort_values(\"time\", inplace=True)     with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:         events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")     ## add assignment to picks     assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])     picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})     picks.sort_values([\"phase_time\"], inplace=True)     with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:         picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")      # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"     return events  In\u00a0[14]: Copied! <pre>gamma_catalog = run_gamma(region=region, config=config)\n</pre> gamma_catalog = run_gamma(region=region, config=config) <pre>Number of picks: 80743\nEikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 2.354\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.095\nAssociating 2263 clusters with 15 CPUs\n.............................................................................................................\nAssociated 100 events.\n.....................................................................................\nAssociated 200 events\n..................................................................................\nAssociated 300 events\n...............................................................................................\nAssociated 400 events\n..........................................................................................\nAssociated 500 events\n.......................................................................................................\nAssociated 600 events\n......................................................................................................\nAssociated 700 events\n....................................................................................................\nAssociated 800 events\n...................................................................................................\nAssociated 900 events\n..........................................................................................................\nAssociated 1000 events\n......................................................................................\nAssociated 1100 events\n.............................................................................................\nAssociated 1200 events\n..................................................................................................\nAssociated 1300 events\n...........................................................................................\nAssociated 1400 events\n...................................................................................................\nAssociated 1500 events\n...........................................................................\nAssociated 1600 events\n..................................................................................\nAssociated 1700 events\n................................................................................................\nAssociated 1800 events\n............................................................................................\nAssociated 1900 events\n...........................................................................................................\nAssociated 2000 events\n......................................................................................\nAssociated 2100 events\n....................................................................................................\nAssociated 2200 events.\n..............................................................................................\nAssociated 2300 events\n......................................................................................\nAssociated 2400 events\n.....</pre> In\u00a0[15]: Copied! <pre>plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config)\n</pre> plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config) In\u00a0[16]: Copied! <pre>def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{root_path}/{region}/gamma\"\n    result_path = f\"{root_path}/{region}/adloc\"\n    figure_path = f\"{root_path}/{region}/adloc/figures\"\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n    if not os.path.exists(figure_path):\n        os.makedirs(figure_path)\n\n    picks_file = f\"{data_path}/gamma_picks.csv\"\n    events_file = f\"{data_path}/gamma_events.csv\"\n    stations_file = f\"{root_path}/{region}/obspy/stations.csv\"\n\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")\n\n    ## read picks and associated events\n    picks = pd.read_csv(picks_file)\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])\n     # drop unnecessary columns\n    picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")\n    if os.path.exists(events_file):\n        events = pd.read_csv(events_file)\n        events[\"time\"] = pd.to_datetime(events[\"time\"])\n        events[[\"x_km\", \"y_km\"]] = events.apply(\n            lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n        )\n        events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0\n    else:\n        events = None\n\n    ## read stations\n    # stations = pd.read_json(stations_file, orient=\"index\")\n    stations = pd.read_csv(stations_file, na_filter=False)\n    stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000\n    if \"station_term_time_p\" not in stations.columns:\n        stations[\"station_term_time_p\"] = 0.0\n    if \"station_term_time_s\" not in stations.columns:\n        stations[\"station_term_time_s\"] = 0.0\n    if \"station_term_amplitude\" not in stations.columns:\n        stations[\"station_term_amplitude\"] = 0.0\n    stations[[\"x_km\", \"y_km\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n\n    ## setting ADLoc configs\n    config[\"use_amplitude\"] = True\n\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    config[\"xlim_km\"] = (xmin, xmax)\n    config[\"ylim_km\"] = (ymin, ymax)\n    config[\"zlim_km\"] = (zmin, zmax)\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    # Northern California (Gil7)\n    # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]\n    # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]\n    # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]\n    h = 0.3\n    vel = {\"Z\": zz, \"P\": vp, \"S\": vs}\n    config[\"eikonal\"] = {\n        \"vel\": vel,\n        \"h\": h,\n        \"xlim_km\": config[\"xlim_km\"],\n        \"ylim_km\": config[\"ylim_km\"],\n        \"zlim_km\": config[\"zlim_km\"],\n    }\n    config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])\n\n    # RASAC\n    config[\"min_picks\"] = 6\n    config[\"min_picks_ratio\"] = 0.5\n    config[\"max_residual_time\"] = 1.0\n    config[\"max_residual_amplitude\"] = 1.0\n    config[\"min_score\"] = 0.5\n    config[\"min_s_picks\"] = 1.5\n    config[\"min_p_picks\"] = 1.5\n\n    config[\"bfgs_bounds\"] = (\n        (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x\n        (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y\n        (0, config[\"zlim_km\"][1] + 1),\n        (None, None),  # t\n    )\n\n    # %%\n    mapping_phase_type_int = {\"P\": 0, \"S\": 1}\n    picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)\n    if \"phase_amplitude\" in picks.columns:\n        picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)\n\n    # %%\n    stations[\"idx_sta\"] = np.arange(len(stations))\n    if events is None:\n        picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")\n        events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})\n        events[\"z_km\"] = 10.0  # km default depth\n        events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)\n        events[\"event_index\"] = events.index\n        events.reset_index(drop=True, inplace=True)\n        events[\"idx_eve\"] = np.arange(len(events))\n        picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)\n    else:\n        events[\"idx_eve\"] = np.arange(len(events))\n\n    picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")\n    picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")\n\n\n    # for key, value in config.items():\n    #     print(f\"{key}: {value}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n    print(f\"Number of events: {len(events)}\")\n\n    # %%\n    estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])\n\n    # %%\n    MAX_SST_ITER = 8\n    events_init = events.copy()\n\n    for iter in range(MAX_SST_ITER):\n        picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)\n\n        station_term_amp = (\n            picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()\n        )\n        station_term_amp.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)\n\n        station_term_time = (\n            picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()\n        )\n        station_term_time.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_time_p\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)\n        )\n        stations[\"station_term_time_s\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)\n        )\n\n        if \"event_index\" not in events.columns:\n            events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n        events[[\"longitude\", \"latitude\"]] = events.apply(\n            lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n        )\n        events[\"depth_km\"] = events[\"z_km\"]\n\n        picks[\"adloc_mask\"] = picks[\"mask\"]\n        picks[\"adloc_residual_time\"] = picks[\"residual_time\"]\n        picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]\n\n        picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)\n        events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)\n        stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)\n\n    # %%\n    if \"event_index\" not in events.columns:\n        events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z_km\"]\n    events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n    events.sort_values([\"time\"], inplace=True)\n\n    picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})\n    picks.drop(\n        [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"\n    )\n    picks.sort_values([\"phase_time\"], inplace=True)\n\n    stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n\n    picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)\n    events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)\n    stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)\n\n    return events\n</pre> def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{root_path}/{region}/gamma\"     result_path = f\"{root_path}/{region}/adloc\"     figure_path = f\"{root_path}/{region}/adloc/figures\"     if not os.path.exists(result_path):         os.makedirs(result_path)     if not os.path.exists(figure_path):         os.makedirs(figure_path)      picks_file = f\"{data_path}/gamma_picks.csv\"     events_file = f\"{data_path}/gamma_events.csv\"     stations_file = f\"{root_path}/{region}/obspy/stations.csv\"      proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")      ## read picks and associated events     picks = pd.read_csv(picks_file)     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])      # drop unnecessary columns     picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")     if os.path.exists(events_file):         events = pd.read_csv(events_file)         events[\"time\"] = pd.to_datetime(events[\"time\"])         events[[\"x_km\", \"y_km\"]] = events.apply(             lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1         )         events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0     else:         events = None      ## read stations     # stations = pd.read_json(stations_file, orient=\"index\")     stations = pd.read_csv(stations_file, na_filter=False)     stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000     if \"station_term_time_p\" not in stations.columns:         stations[\"station_term_time_p\"] = 0.0     if \"station_term_time_s\" not in stations.columns:         stations[\"station_term_time_s\"] = 0.0     if \"station_term_amplitude\" not in stations.columns:         stations[\"station_term_amplitude\"] = 0.0     stations[[\"x_km\", \"y_km\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)      ## setting ADLoc configs     config[\"use_amplitude\"] = True      minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     config[\"xlim_km\"] = (xmin, xmax)     config[\"ylim_km\"] = (ymin, ymax)     config[\"zlim_km\"] = (zmin, zmax)      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     # Northern California (Gil7)     # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]     # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]     # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]     h = 0.3     vel = {\"Z\": zz, \"P\": vp, \"S\": vs}     config[\"eikonal\"] = {         \"vel\": vel,         \"h\": h,         \"xlim_km\": config[\"xlim_km\"],         \"ylim_km\": config[\"ylim_km\"],         \"zlim_km\": config[\"zlim_km\"],     }     config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])      # RASAC     config[\"min_picks\"] = 6     config[\"min_picks_ratio\"] = 0.5     config[\"max_residual_time\"] = 1.0     config[\"max_residual_amplitude\"] = 1.0     config[\"min_score\"] = 0.5     config[\"min_s_picks\"] = 1.5     config[\"min_p_picks\"] = 1.5      config[\"bfgs_bounds\"] = (         (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x         (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y         (0, config[\"zlim_km\"][1] + 1),         (None, None),  # t     )      # %%     mapping_phase_type_int = {\"P\": 0, \"S\": 1}     picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)     if \"phase_amplitude\" in picks.columns:         picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)      # %%     stations[\"idx_sta\"] = np.arange(len(stations))     if events is None:         picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")         events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})         events[\"z_km\"] = 10.0  # km default depth         events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)         events[\"event_index\"] = events.index         events.reset_index(drop=True, inplace=True)         events[\"idx_eve\"] = np.arange(len(events))         picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)     else:         events[\"idx_eve\"] = np.arange(len(events))      picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")     picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")       # for key, value in config.items():     #     print(f\"{key}: {value}\")      print(f\"Number of picks: {len(picks)}\")     print(f\"Number of events: {len(events)}\")      # %%     estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])      # %%     MAX_SST_ITER = 8     events_init = events.copy()      for iter in range(MAX_SST_ITER):         picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)          station_term_amp = (             picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()         )         station_term_amp.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)          station_term_time = (             picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()         )         station_term_time.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_time_p\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)         )         stations[\"station_term_time_s\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)         )          if \"event_index\" not in events.columns:             events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]         events[[\"longitude\", \"latitude\"]] = events.apply(             lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1         )         events[\"depth_km\"] = events[\"z_km\"]          picks[\"adloc_mask\"] = picks[\"mask\"]         picks[\"adloc_residual_time\"] = picks[\"residual_time\"]         picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]          picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)         events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)         stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)      # %%     if \"event_index\" not in events.columns:         events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z_km\"]     events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")     events.sort_values([\"time\"], inplace=True)      picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})     picks.drop(         [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"     )     picks.sort_values([\"phase_time\"], inplace=True)      stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")      picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)     events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)     stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)      return events In\u00a0[17]: Copied! <pre>adloc_catalog = run_adloc(region=region, config=config)\n</pre> adloc_catalog = run_adloc(region=region, config=config) <pre>Eikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 1.918\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.100\nNumber of picks: 47260\nNumber of events: 2413\n</pre> <pre>Iter 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:44&lt;00:00, 54.77it/s]</pre> <pre>ADLoc locates 1268 events outof 2413 events\nusing 37592 picks outof 47260 picks\n</pre> <pre>\nIter 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:43&lt;00:00, 55.27it/s]</pre> <pre>ADLoc locates 1210 events outof 2413 events\nusing 36353 picks outof 47260 picks\n</pre> <pre>\nIter 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:44&lt;00:00, 53.63it/s]</pre> <pre>ADLoc locates 1120 events outof 2413 events\nusing 34391 picks outof 47260 picks\n</pre> <pre>\nIter 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:46&lt;00:00, 52.30it/s]</pre> <pre>ADLoc locates 1076 events outof 2413 events\nusing 33153 picks outof 47260 picks\n</pre> <pre>\nIter 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:46&lt;00:00, 51.54it/s]</pre> <pre>ADLoc locates 1054 events outof 2413 events\nusing 32595 picks outof 47260 picks\n</pre> <pre>\nIter 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:46&lt;00:00, 51.70it/s]</pre> <pre>ADLoc locates 1040 events outof 2413 events\nusing 32315 picks outof 47260 picks\n</pre> <pre>\nIter 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:46&lt;00:00, 51.46it/s]</pre> <pre>ADLoc locates 1036 events outof 2413 events\nusing 32170 picks outof 47260 picks\n</pre> <pre>\nIter 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2413/2413 [00:47&lt;00:00, 50.73it/s]</pre> <pre>ADLoc locates 1035 events outof 2413 events\nusing 32118 picks outof 47260 picks\n</pre> <pre>\n</pre> In\u00a0[18]: Copied! <pre>plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config)\n</pre> plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config) <p>Credit: Felix Waldhauser</p> In\u00a0[19]: Copied! <pre>def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):\n\n    data_path = f\"{region}/adloc\"\n    result_path = f\"{region}/hypodd\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    ## Station Format\n    stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")\n    stations.set_index(\"station_id\", inplace=True)\n\n    shift_topo = stations[\"elevation_m\"].max() / 1e3\n    converted_hypoinverse = []\n    converted_hypodd = {}\n\n    for sta, row in stations.iterrows():\n        network_code, station_code, comp_code, channel_code = sta.split(\".\")\n        station_weight = \" \"\n        lat_degree = int(row[\"latitude\"])\n        lat_minute = (row[\"latitude\"] - lat_degree) * 60\n        north = \"N\" if lat_degree &gt;= 0 else \"S\"\n        lng_degree = int(row[\"longitude\"])\n        lng_minute = (row[\"longitude\"] - lng_degree) * 60\n        west = \"W\" if lng_degree &lt;= 0 else \"E\"\n        elevation = row[\"elevation_m\"]\n        line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"\n        converted_hypoinverse.append(line_hypoinverse)\n\n        # tmp_code = f\"{station_code}{channel_code}\"\n        tmp_code = f\"{station_code}\"\n        converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"\n\n\n    with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:\n        for k, v in converted_hypodd.items():\n            f.write(v)\n\n\n    ## Picks Format\n    picks_csv = f\"{data_path}/adloc_picks.csv\"\n    events_csv = f\"{data_path}/adloc_events.csv\"\n\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    events = pd.read_csv(f\"{root_path}/{events_csv}\")\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")\n    events[\"time\"] = pd.to_datetime(events[\"time\"])\n    # events[\"magnitude\"] = 1.0\n    events[\"sigma_time\"] = 1.0\n\n    # events.sort_values(\"time\", inplace=True)\n    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n\n    lines = []\n    picks_by_event = picks.groupby(\"event_index\").groups\n    for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):\n        # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n        event_time = event[\"time\"]\n        lat = event[\"latitude\"]\n        lng = event[\"longitude\"]\n        # dep = event[\"depth(m)\"] / 1e3 + shift_topo\n        dep = event[\"depth_km\"] + shift_topo\n        mag = event[\"magnitude\"]\n        EH = 0\n        EZ = 0\n        RMS = event[\"sigma_time\"]\n\n        year, month, day, hour, min, sec = (\n            event_time.year,\n            event_time.month,\n            event_time.day,\n            event_time.hour,\n            event_time.minute,\n            float(event_time.strftime(\"%S.%f\")),\n        )\n        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n\n        lines.append(event_line)\n\n        picks_idx = picks_by_event[event[\"event_index\"]]\n        for j in picks_idx:\n            # pick = picks.iloc[j]\n            pick = picks.loc[j]\n            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n            phase_type = pick[\"phase_type\"].upper()\n            phase_score = pick[\"phase_score\"]\n            # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n            pick_time = (pick[\"phase_time\"] - event_time).total_seconds()\n            tmp_code = f\"{station_code}\"\n            pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n            lines.append(pick_line)\n\n    with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:\n        fp.writelines(lines)\n\n    ## Run Hypodd\n    print(f\"Running Hypodd:\")\n    os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")\n\n    ## Read  catalog\n    columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]\n    catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(\n        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',\n        axis=1,\n    )\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n    catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]\n    catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)\n    catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)\n    catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)\n\n    return catalog_ct_hypodd\n</pre> def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):      data_path = f\"{region}/adloc\"     result_path = f\"{region}/hypodd\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      ## Station Format     stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")     stations.set_index(\"station_id\", inplace=True)      shift_topo = stations[\"elevation_m\"].max() / 1e3     converted_hypoinverse = []     converted_hypodd = {}      for sta, row in stations.iterrows():         network_code, station_code, comp_code, channel_code = sta.split(\".\")         station_weight = \" \"         lat_degree = int(row[\"latitude\"])         lat_minute = (row[\"latitude\"] - lat_degree) * 60         north = \"N\" if lat_degree &gt;= 0 else \"S\"         lng_degree = int(row[\"longitude\"])         lng_minute = (row[\"longitude\"] - lng_degree) * 60         west = \"W\" if lng_degree &lt;= 0 else \"E\"         elevation = row[\"elevation_m\"]         line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"         converted_hypoinverse.append(line_hypoinverse)          # tmp_code = f\"{station_code}{channel_code}\"         tmp_code = f\"{station_code}\"         converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"       with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:         for k, v in converted_hypodd.items():             f.write(v)       ## Picks Format     picks_csv = f\"{data_path}/adloc_picks.csv\"     events_csv = f\"{data_path}/adloc_events.csv\"      picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     events = pd.read_csv(f\"{root_path}/{events_csv}\")     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")     events[\"time\"] = pd.to_datetime(events[\"time\"])     # events[\"magnitude\"] = 1.0     events[\"sigma_time\"] = 1.0      # events.sort_values(\"time\", inplace=True)     picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]      lines = []     picks_by_event = picks.groupby(\"event_index\").groups     for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):         # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")         event_time = event[\"time\"]         lat = event[\"latitude\"]         lng = event[\"longitude\"]         # dep = event[\"depth(m)\"] / 1e3 + shift_topo         dep = event[\"depth_km\"] + shift_topo         mag = event[\"magnitude\"]         EH = 0         EZ = 0         RMS = event[\"sigma_time\"]          year, month, day, hour, min, sec = (             event_time.year,             event_time.month,             event_time.day,             event_time.hour,             event_time.minute,             float(event_time.strftime(\"%S.%f\")),         )         event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"          lines.append(event_line)          picks_idx = picks_by_event[event[\"event_index\"]]         for j in picks_idx:             # pick = picks.iloc[j]             pick = picks.loc[j]             network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")             phase_type = pick[\"phase_type\"].upper()             phase_score = pick[\"phase_score\"]             # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()             pick_time = (pick[\"phase_time\"] - event_time).total_seconds()             tmp_code = f\"{station_code}\"             pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"             lines.append(pick_line)      with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:         fp.writelines(lines)      ## Run Hypodd     print(f\"Running Hypodd:\")     os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")      ## Read  catalog     columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]     catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(         lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',         axis=1,     )     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))     catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]     catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)     catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)     catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)      return catalog_ct_hypodd In\u00a0[20]: Copied! <pre>hypodd_catalog = run_hypodd(region=region)\n</pre> hypodd_catalog = run_hypodd(region=region) <pre>Convert catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1035/1035 [00:02&lt;00:00, 360.66it/s]\n+ WORKING_DIR=/workspaces/Earthquake_Catalog_Workshop/notebooks\n+ '[' 2 -eq 2 ']'\n+ root_path=local\n+ region=hawaii\n+ data_path=local/hawaii/hypodd\n+ '[' '!' -d local/hawaii/hypodd ']'\n+ cd local/hawaii/hypodd\n+ '[' '!' -d HypoDD ']'\n+ git clone https://github.com/zhuwq0/HypoDD.git\nCloning into 'HypoDD'...\n</pre> <pre>Running Hypodd:\n</pre> <pre>+ export PATH=/workspaces/Earthquake_Catalog_Workshop/.conda/quakeflow/bin:/opt/conda/condabin:/vscode/bin/linux-x64/4437686ffebaf200fa4a6e6e67f735f3edf24ada/bin/remote-cli:/home/codespace/.local/bin:/home/codespace/.dotnet:/home/codespace/nvm/current/bin:/home/codespace/.php/current/bin:/home/codespace/.python/current/bin:/home/codespace/java/current/bin:/home/codespace/.ruby/current/bin:/home/codespace/.local/bin:/usr/local/python/current/bin:/usr/local/py-utils/bin:/usr/local/jupyter:/usr/local/oryx:/usr/local/go/bin:/go/bin:/usr/local/sdkman/bin:/usr/local/sdkman/candidates/java/current/bin:/usr/local/sdkman/candidates/gradle/current/bin:/usr/local/sdkman/candidates/maven/current/bin:/usr/local/sdkman/candidates/ant/current/bin:/usr/local/rvm/gems/default/bin:/usr/local/rvm/gems/default@global/bin:/usr/local/rvm/rubies/default/bin:/usr/local/share/rbenv/bin:/usr/local/php/current/bin:/opt/conda/bin:/usr/local/nvs:/usr/local/share/nvm/current/bin:/usr/local/hugo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/share/dotnet:/home/codespace/.dotnet/tools:/usr/local/rvm/bin:/workspaces/Earthquake_Catalog_Workshop/notebooks/local/hawaii/hypodd/HypoDD\n+ PATH=/workspaces/Earthquake_Catalog_Workshop/.conda/quakeflow/bin:/opt/conda/condabin:/vscode/bin/linux-x64/4437686ffebaf200fa4a6e6e67f735f3edf24ada/bin/remote-cli:/home/codespace/.local/bin:/home/codespace/.dotnet:/home/codespace/nvm/current/bin:/home/codespace/.php/current/bin:/home/codespace/.python/current/bin:/home/codespace/java/current/bin:/home/codespace/.ruby/current/bin:/home/codespace/.local/bin:/usr/local/python/current/bin:/usr/local/py-utils/bin:/usr/local/jupyter:/usr/local/oryx:/usr/local/go/bin:/go/bin:/usr/local/sdkman/bin:/usr/local/sdkman/candidates/java/current/bin:/usr/local/sdkman/candidates/gradle/current/bin:/usr/local/sdkman/candidates/maven/current/bin:/usr/local/sdkman/candidates/ant/current/bin:/usr/local/rvm/gems/default/bin:/usr/local/rvm/gems/default@global/bin:/usr/local/rvm/rubies/default/bin:/usr/local/share/rbenv/bin:/usr/local/php/current/bin:/opt/conda/bin:/usr/local/nvs:/usr/local/share/nvm/current/bin:/usr/local/hugo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/share/dotnet:/home/codespace/.dotnet/tools:/usr/local/rvm/bin:/workspaces/Earthquake_Catalog_Workshop/notebooks/local/hawaii/hypodd/HypoDD\n+ make -C HypoDD/src/\n+ cat\n+ cat\n+ ./HypoDD/src/ph2dt/ph2dt ph2dt.inp\n</pre> <pre>starting ph2dt (v2.1b - 08/2012)...     Mon Apr  7 20:08:31 2025\u0000\n\nreading data ...\n &gt; stations =           57\n &gt; events total =         1035\n &gt; events selected =          981\n &gt; phases =        32315\nforming dtimes...\n &gt; stations selected =           46\n &gt; P-phase pairs total =       321342\n &gt; S-phase pairs total =       321965\n &gt; outliers =        10531  (           1 %)\n &gt; phases at stations not in station list =            0\n &gt; phases at distances larger than MAXDIST =            0\n &gt; P-phase pairs selected =       284371  (          88 %)\n &gt; S-phase pairs selected =       287128  (          89 %)\n &gt; weakly linked events =          299  (          30 %)\n &gt; linked event pairs =        25364\n &gt; average links per pair =           22\n &gt; average offset (km) betw. linked events =    2.79277945    \n &gt; average offset (km) betw. strongly linked events =    2.79277945    \n &gt; maximum offset (km) betw. strongly linked events =    9.99962521    \n\nDone.  Mon Apr  7 20:08:32 2025\u0000\n\nOutput files: dt.ct; event.dat; event.sel; station.sel; ph2dt.log\nph2dt parameters were: \n(minwght,maxdist,maxsep,maxngh,minlnk,minobs,maxobs)\n 0.00  200.000   10.000  50   8   8 100\nstarting hypoDD (v2.1beta - 06/15/2016)...   Mon Apr  7 20:08:32 2025\u0003\nINPUT FILES:\ncross dtime data:  \ncatalog dtime data: dt.ct\nevents: event.sel\nstations: stations.dat\nOUTPUT FILES:\ninitial locations: hypodd_ct.loc\nrelocated events: hypodd_ct.reloc\nevent pair residuals: hypodd.res\nstation residuals: hypodd.sta\nsource parameters: hypodd.src\n Relocate all clusters\n Relocate all events\nUse local layered 1D model.\nReading data ...   Mon Apr  7 20:08:32 2025\u0000\n# events =   981\n# stations &lt; maxdist =     57\n# stations w/ neg. elevation (set to 0) =    0\n</pre> <pre>+ ./HypoDD/src/hypoDD/hypoDD ct.inp\n</pre> <pre># catalog P dtimes =  284371\n# catalog S dtimes =  287128\n# dtimes total =   571499\n# events after dtime match =        936\n# stations =     46\n\nno clustering performed.\n\nRELOCATION OF CLUSTER: 1     Mon Apr  7 20:08:34 2025\u0003\n----------------------\nInitial trial sources =   936\n1D ray tracing.\n\n  IT   EV  CT    RMSCT   RMSST   DX   DY   DZ   DT   OS  AQ  CND\n        %   %   ms     %    ms    m    m    m   ms    m \n 1    100  98   93 -18.3     0  584  629  833   96    0   2  398\n 2  1 100  98   91  -1.5   329  575  623  806   96  106   0  393\n 3     99  95   70 -23.9   329  185  262  302   34  106   2  373\n 4  2  99  95   67  -2.9   230  185  261  284   34  148   0  376\n 5     99  94   61  -9.4   230  104  166  225   19  148   2  370\n 6  3  99  93   60  -1.3   204  104  166  213   19  169   0  369\n 7  4  99  93   58  -4.5   191   78  108  136   11  236   0  353\n 8  5  91  77   46 -20.1   137   85  121  111   12  162   0  303\n 9  6  90  75   41 -11.9   111   42   71   58    8  177   0  301\n10  7  90  74   38  -5.7   105   29   49   41    5  180   0  292\n11  8  90  73   37  -3.1   102   20   34   31    4  175   0  280\n12  9  83  52   29 -21.2    72   40   60   37    6  155   0  232\n13 10  83  49   25 -13.1    60   24   37   28    4  156   0  221\n14 11  83  48   24  -6.4    56   16   27   22    3  156   0  212\n15 12  83  47   23  -3.7    53   12   20   18    2  162   0  211\n16 13  83  44   19 -18.1    41   14   22   19    3  156   0  201\n17 14  82  42   17  -8.9    41   12   19   17    2  151   0  197\n18 15  82  41   16  -5.4    40   11   17   15    2  147   0  194\n19 16  82  40   16  -3.7    40    9   14   13    2  145   0  188\n\nwriting out results ...\n   Program hypoDD finished. Mon Apr  7 20:09:12 2025\u0003\n</pre> <pre>+ cd /workspaces/Earthquake_Catalog_Workshop/notebooks\n</pre> In\u00a0[21]: Copied! <pre>plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)\n</pre> plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)"},{"location":"notebooks/quakeflow_hawaii/#earthquake-catalog-workshop","title":"Earthquake Catalog Workshop\u00b6","text":"<p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"notebooks/quakeflow_hawaii/#machine-learning-part","title":"Machine Learning Part\u00b6","text":"<ol> <li><p>Download data using Obpsy and NCEDC/SCEDC AWS Public Dataset</p> <p>FDSN web service client for ObsPy</p> <p>NCEDC AWS Public Dataset</p> <p>SCEDC AWS Public Dataset</p> <p>Event Dataset (CEED)</p> <p>CEED paper</p> </li> <li><p>PhaseNet for P/S phase picking</p> <p>PhaseNet github page</p> <p>PhaseNet paper</p> </li> <li><p>GaMMA for phase association</p> <p>GaMMA github page</p> <p>GaMMA paper</p> </li> <li><p>ADLoc for earthquake location</p> <p>ADLoc github page</p> <p>ADLoc paper</p> </li> <li><p>HypoDD for earthquake relocation</p> <p>HypoDD github page</p> <p>HypoDD paper</p> </li> <li><p>QuakeFlow</p> <p>QuakeFlow github page</p> <p>QuakeFlow paper</p> </li> </ol>"},{"location":"notebooks/quakeflow_hawaii/#setup-environment","title":"Setup Environment\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#setup-configurations","title":"Setup configurations\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#download-the-standard-catalog-for-comparison","title":"Download the standard catalog for comparison\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#download-stations","title":"Download stations\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#download-waveform-data","title":"Download waveform data\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#run-phasenet-to-pick-ps-picks","title":"Run PhaseNet to pick P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#run-gamma-to-associate-ps-picks","title":"Run GaMMA to associate P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#run-adloc-to-locate-absolute-earthquake-locations","title":"Run ADLoc to locate absolute earthquake locations\u00b6","text":""},{"location":"notebooks/quakeflow_hawaii/#run-hypodd-to-relocate-relative-earthquake-locations","title":"Run HypoDD to relocate relative earthquake locations\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/","title":"Quakeflow ridgecrest","text":"In\u00a0[1]: Copied! <pre># !conda env create -f env.yaml --prefix .conda/quakeflow\n# !source activate .conda/quakeflow\n# !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/\n# !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../\n# !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git\n# !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git\n</pre> # !conda env create -f env.yaml --prefix .conda/quakeflow # !source activate .conda/quakeflow # !git clone --recursive https://github.com/AI4EPS/QuakeFlow.git notebooks/QuakeFlow/ # !cd notebooks/QuakeFlow/PhaseNet &amp;&amp; git checkout master &amp;&amp; cd ../ # !python -m pip install -U git+https://github.com/AI4EPS/GaMMA.git # !python -m pip install -U git+https://github.com/AI4EPS/ADLoc.git In\u00a0[2]: Copied! <pre># %%\nimport json\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom typing import Dict\nfrom glob import glob\n\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport fsspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport obspy\nimport obspy.clients.fdsn\nimport pandas as pd\nfrom obspy.clients.fdsn.mass_downloader import (\n    CircularDomain,\n    MassDownloader,\n    Restrictions,\n)\nfrom pyproj import Proj\nfrom tqdm import tqdm\n\nfrom adloc.eikonal2d import init_eikonal2d\nfrom adloc.sacloc2d import ADLoc\nfrom adloc.utils import invert_location\nfrom gamma.utils import association, estimate_eps\n</pre> # %% import json import os import pickle from collections import defaultdict from datetime import datetime from typing import Dict from glob import glob  import cartopy.crs as ccrs import cartopy.feature as cfeature import fsspec import matplotlib.pyplot as plt import numpy as np import obspy import obspy.clients.fdsn import pandas as pd from obspy.clients.fdsn.mass_downloader import (     CircularDomain,     MassDownloader,     Restrictions, ) from pyproj import Proj from tqdm import tqdm  from adloc.eikonal2d import init_eikonal2d from adloc.sacloc2d import ADLoc from adloc.utils import invert_location from gamma.utils import association, estimate_eps  In\u00a0[3]: Copied! <pre>def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:\n\n    if not os.path.exists(f\"{root_path}/{region}\"):\n        os.makedirs(f\"{root_path}/{region}\", exist_ok=True)\n\n    regions = {\n            \"demo\": {\n                \"longitude0\": -117.504,\n                \"latitude0\": 35.705,\n                \"maxradius_degree\": 0.5,\n                \"mindepth\": 0,\n                \"maxdepth\": 30,\n                \"starttime\": \"2019-07-04T00:00:00\",\n                \"endtime\": \"2019-07-05T00:00:00\",\n                \"network\": \"CI\",\n                \"channel\": \"HH*,BH*,EH*,HN*\",\n                \"provider\": [\n                    \"SCEDC\"\n                ],\n            },\n            \"ridgecrest\": {\n                \"longitude0\": -117.504,\n                \"latitude0\": 35.705,\n                \"maxradius_degree\": 0.5,\n                \"mindepth\": 0,\n                \"maxdepth\": 30,\n                \"starttime\": \"2019-07-04T00:00:00\",\n                \"endtime\": \"2019-07-10T00:00:00\",\n                \"network\": \"CI\",\n                \"channel\": \"HH*,BH*,EH*,HN*\",\n                \"provider\": [\n                    \"SCEDC\"\n                ],\n            },\n    }\n\n    ## Set config\n    config = regions[region.lower()]\n\n    ## PhaseNet\n    config[\"phasenet\"] = {}\n    ## GaMMA\n    config[\"gamma\"] = {}\n    ## ADLoc\n    config[\"adloc\"] = {}\n    ## HypoDD\n    config[\"hypodd\"] = {}\n\n    with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:\n        json.dump(config, fp, indent=2)\n\n    print(json.dumps(config, indent=4))\n\n    return config\n</pre> def set_config(root_path: str = \"local\", region: str = \"demo\") -&gt; Dict:      if not os.path.exists(f\"{root_path}/{region}\"):         os.makedirs(f\"{root_path}/{region}\", exist_ok=True)      regions = {             \"demo\": {                 \"longitude0\": -117.504,                 \"latitude0\": 35.705,                 \"maxradius_degree\": 0.5,                 \"mindepth\": 0,                 \"maxdepth\": 30,                 \"starttime\": \"2019-07-04T00:00:00\",                 \"endtime\": \"2019-07-05T00:00:00\",                 \"network\": \"CI\",                 \"channel\": \"HH*,BH*,EH*,HN*\",                 \"provider\": [                     \"SCEDC\"                 ],             },             \"ridgecrest\": {                 \"longitude0\": -117.504,                 \"latitude0\": 35.705,                 \"maxradius_degree\": 0.5,                 \"mindepth\": 0,                 \"maxdepth\": 30,                 \"starttime\": \"2019-07-04T00:00:00\",                 \"endtime\": \"2019-07-10T00:00:00\",                 \"network\": \"CI\",                 \"channel\": \"HH*,BH*,EH*,HN*\",                 \"provider\": [                     \"SCEDC\"                 ],             },     }      ## Set config     config = regions[region.lower()]      ## PhaseNet     config[\"phasenet\"] = {}     ## GaMMA     config[\"gamma\"] = {}     ## ADLoc     config[\"adloc\"] = {}     ## HypoDD     config[\"hypodd\"] = {}      with open(f\"{root_path}/{region}/config.json\", \"w\") as fp:         json.dump(config, fp, indent=2)      print(json.dumps(config, indent=4))      return config In\u00a0[4]: Copied! <pre>region = \"ridgecrest\"\nconfig  = set_config(region = region)\n</pre> region = \"ridgecrest\" config  = set_config(region = region) <pre>{\n    \"longitude0\": -117.504,\n    \"latitude0\": 35.705,\n    \"maxradius_degree\": 0.5,\n    \"mindepth\": 0,\n    \"maxdepth\": 30,\n    \"starttime\": \"2019-07-04T00:00:00\",\n    \"endtime\": \"2019-07-10T00:00:00\",\n    \"network\": \"CI\",\n    \"channel\": \"HH*,BH*,EH*,HN*\",\n    \"provider\": [\n        \"SCEDC\"\n    ],\n    \"phasenet\": {},\n    \"gamma\": {},\n    \"adloc\": {},\n    \"hypodd\": {}\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure()\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        catalog['longitude'],\n        catalog['latitude'],\n        c=catalog['depth_km'],\n        cmap='viridis_r',\n        s=1,\n        alpha=0.6,\n        vmin = config[\"mindepth\"],\n        vmax = config[\"maxdepth\"]/2,\n        transform=ccrs.PlateCarree()\n    )\n\n    plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'{method} Catalog ({len(catalog)})')\n    plt.show()\n\ndef plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):\n    ## Plot earthquake locations\n    fig = plt.figure()\n    ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))\n    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n    ax.add_feature(cfeature.COASTLINE, linewidth=0.2)\n\n    scatter = ax.scatter(\n        stations['longitude'],\n        stations['latitude'],\n        c=\"C0\",\n        s=40,\n        marker=\"^\",\n        alpha=0.6,\n        transform=ccrs.PlateCarree()\n    )\n    if catalog is not None:\n        scatter = ax.scatter(\n            catalog['longitude'],\n            catalog['latitude'],\n            c=catalog['depth_km'],\n            cmap='viridis_r',\n            s=1,\n            alpha=0.6,\n            vmin = config[\"mindepth\"],\n            vmax = config[\"maxdepth\"]/2,\n            transform=ccrs.PlateCarree()\n        )\n        plt.colorbar(scatter, label='Depth (km)')\n    ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])\n\n    # Add gridlines\n    gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n    gl.top_labels = False\n    gl.right_labels = False\n\n    plt.title(f'Stations ({len(stations[\"station\"].unique())})')\n    plt.show()\n\ndef download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_path = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download catalog \n    client = obspy.clients.fdsn.Client(\"usgs\")\n    events = client.get_events(\n        starttime=config[\"starttime\"],\n        endtime=config[\"endtime\"],\n        latitude=config[\"latitude0\"],\n        longitude=config[\"longitude0\"],\n        maxradius=config[\"maxradius_degree\"],\n    )\n    print(f\"Number of events: {len(events)}\")\n\n    ## Save catalog\n    catalog = defaultdict(list)\n    for event in events:\n        if len(event.magnitudes) &gt; 0:\n            catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))\n            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n            catalog[\"longitude\"].append(event.origins[0].longitude)\n            catalog[\"latitude\"].append(event.origins[0].latitude)\n            catalog[\"depth_km\"].append(event.origins[0].depth/1e3)\n    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n    catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)\n\n    return catalog\n</pre> def plot_catalog(catalog: pd.DataFrame, method=\"Standard\", region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure()     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         catalog['longitude'],         catalog['latitude'],         c=catalog['depth_km'],         cmap='viridis_r',         s=1,         alpha=0.6,         vmin = config[\"mindepth\"],         vmax = config[\"maxdepth\"]/2,         transform=ccrs.PlateCarree()     )      plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"]/2, config[\"longitude0\"] + config[\"maxradius_degree\"]/2, config[\"latitude0\"] - config[\"maxradius_degree\"]/2, config[\"latitude0\"] + config[\"maxradius_degree\"]/2])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'{method} Catalog ({len(catalog)})')     plt.show()  def plot_stations(stations: pd.DataFrame, catalog: pd.DataFrame=None, region: str = \"demo\", config: Dict = {}):     ## Plot earthquake locations     fig = plt.figure()     ax = plt.axes(projection=ccrs.LambertAzimuthalEqualArea(central_longitude=config[\"longitude0\"], central_latitude=config[\"latitude0\"]))     ax.add_feature(cfeature.LAND, facecolor='lightgray')     ax.add_feature(cfeature.OCEAN, facecolor='lightblue')     ax.add_feature(cfeature.COASTLINE, linewidth=0.2)      scatter = ax.scatter(         stations['longitude'],         stations['latitude'],         c=\"C0\",         s=40,         marker=\"^\",         alpha=0.6,         transform=ccrs.PlateCarree()     )     if catalog is not None:         scatter = ax.scatter(             catalog['longitude'],             catalog['latitude'],             c=catalog['depth_km'],             cmap='viridis_r',             s=1,             alpha=0.6,             vmin = config[\"mindepth\"],             vmax = config[\"maxdepth\"]/2,             transform=ccrs.PlateCarree()         )         plt.colorbar(scatter, label='Depth (km)')     ax.set_extent([config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"], config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]])      # Add gridlines     gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')     gl.top_labels = False     gl.right_labels = False      plt.title(f'Stations ({len(stations[\"station\"].unique())})')     plt.show()  def download_catalog(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_path = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")     # print(json.dumps(config, indent=4))      ## Download catalog      client = obspy.clients.fdsn.Client(\"usgs\")     events = client.get_events(         starttime=config[\"starttime\"],         endtime=config[\"endtime\"],         latitude=config[\"latitude0\"],         longitude=config[\"longitude0\"],         maxradius=config[\"maxradius_degree\"],     )     print(f\"Number of events: {len(events)}\")      ## Save catalog     catalog = defaultdict(list)     for event in events:         if len(event.magnitudes) &gt; 0:             catalog[\"time\"].append(event.origins[0].time.datetime.strftime(\"%Y-%m-%dT%H:%M:%S.%f\"))             catalog[\"magnitude\"].append(event.magnitudes[0].mag)             catalog[\"longitude\"].append(event.origins[0].longitude)             catalog[\"latitude\"].append(event.origins[0].latitude)             catalog[\"depth_km\"].append(event.origins[0].depth/1e3)     catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])     catalog.to_csv(f\"{root_path}/{result_path}/catalog.csv\", index=False)      return catalog In\u00a0[6]: Copied! <pre>standard_catalog = download_catalog(region=region, config=config)\nplot_catalog(standard_catalog, method=\"Standard\", config=config)\n</pre> standard_catalog = download_catalog(region=region, config=config) plot_catalog(standard_catalog, method=\"Standard\", config=config) <pre>Number of events: 11321\n</pre> In\u00a0[7]: Copied! <pre>def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    result_dir = f\"{region}/obspy\"\n    if not os.path.exists(f\"{root_path}/{result_dir}\"):\n        os.makedirs(f\"{root_path}/{result_dir}\")\n    if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):\n        os.makedirs(f\"{root_path}/{result_dir}/inventory/\")\n\n    ## Download stations\n    stations = obspy.core.inventory.Inventory()\n    for provider in config[\"provider\"]:\n        client = obspy.clients.fdsn.Client(provider)\n        stations += client.get_stations(\n                network=config[\"network\"],\n                station=\"*\",\n                starttime=config[\"starttime\"],\n                endtime=config[\"endtime\"],\n                latitude=config[\"latitude0\"],\n                longitude=config[\"longitude0\"],\n                maxradius=config[\"maxradius_degree\"],\n                channel=config[\"channel\"],\n                level=\"response\",\n            )\n    stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")\n    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n\n    ## Save stations\n    station_dict = defaultdict(dict)\n    for network in stations:\n        for station in network:\n            inv = stations.select(network=network.code, station=station.code)\n            inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")\n            for channel in station:\n                sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"\n                station_dict[sid] = {\n                    \"network\": network.code,\n                    \"station\": station.code,\n                    \"location\": channel.location_code,\n                    \"channel\": channel.code,\n                    \"longitude\": channel.longitude,\n                    \"latitude\": channel.latitude,\n                    \"elevation_m\": channel.elevation,\n                    \"response\": round(channel.response.instrument_sensitivity.value, 2),\n                }\n\n    # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:\n    #     json.dump(station_dict, fp, indent=2)\n\n    with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:\n        pickle.dump(stations, fp)\n\n    stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")\n    stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)\n        \n    return stations\n</pre> def download_station(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      result_dir = f\"{region}/obspy\"     if not os.path.exists(f\"{root_path}/{result_dir}\"):         os.makedirs(f\"{root_path}/{result_dir}\")     if not os.path.exists(f\"{root_path}/{result_dir}/inventory/\"):         os.makedirs(f\"{root_path}/{result_dir}/inventory/\")      ## Download stations     stations = obspy.core.inventory.Inventory()     for provider in config[\"provider\"]:         client = obspy.clients.fdsn.Client(provider)         stations += client.get_stations(                 network=config[\"network\"],                 station=\"*\",                 starttime=config[\"starttime\"],                 endtime=config[\"endtime\"],                 latitude=config[\"latitude0\"],                 longitude=config[\"longitude0\"],                 maxradius=config[\"maxradius_degree\"],                 channel=config[\"channel\"],                 level=\"response\",             )     stations.write(f\"{root_path}/{result_dir}/inventory.xml\", format=\"STATIONXML\")     print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))      ## Save stations     station_dict = defaultdict(dict)     for network in stations:         for station in network:             inv = stations.select(network=network.code, station=station.code)             inv.write(f\"{root_path}/{result_dir}/inventory/{network.code}.{station.code}.xml\", format=\"STATIONXML\")             for channel in station:                 sid = f\"{network.code}.{station.code}.{channel.location_code}.{channel.code}\"                 station_dict[sid] = {                     \"network\": network.code,                     \"station\": station.code,                     \"location\": channel.location_code,                     \"channel\": channel.code,                     \"longitude\": channel.longitude,                     \"latitude\": channel.latitude,                     \"elevation_m\": channel.elevation,                     \"response\": round(channel.response.instrument_sensitivity.value, 2),                 }      # with open(f\"{root_path}/{result_dir}/stations.json\", \"w\") as fp:     #     json.dump(station_dict, fp, indent=2)      with open(f\"{root_path}/{result_dir}/stations.pkl\", \"wb\") as fp:         pickle.dump(stations, fp)      stations = pd.DataFrame.from_dict(station_dict, orient=\"index\")     stations.to_csv(f\"{root_path}/{result_dir}/stations.csv\", index=False)              return stations In\u00a0[8]: Copied! <pre>stations = download_station(region=region, config=config)\nplot_stations(stations, catalog = standard_catalog, region=region, config=config)\n</pre> stations = download_station(region=region, config=config) plot_stations(stations, catalog = standard_catalog, region=region, config=config) <pre>Number of stations: 15\n</pre> In\u00a0[\u00a0]: Copied! <pre>def map_remote_path(provider, bucket, starttime, network, station, location, channel):\n\n    starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx\n    if provider.lower() == \"scedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        if location == \"\":\n            location = \"__\"\n        path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"\n    elif provider.lower() == \"ncedc\":\n        year = starttime.strftime(\"%Y\")\n        dayofyear = starttime.strftime(\"%j\")\n        path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"\n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n    return path\n\ndef download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    waveform_dir = f\"{region}/waveforms\"\n    if not os.path.exists(f\"{root_path}/{waveform_dir}\"):\n        os.makedirs(f\"{root_path}/{waveform_dir}\")\n    # print(json.dumps(config, indent=4))\n\n    ## Download from cloud\n    for provider in config[\"provider\"]:\n        if provider.lower() in [\"scedc\", \"ncedc\"]:\n            cloud = {\n                \"provider\": provider.lower(),\n                \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",\n            }\n        else:\n            continue\n\n        DELTATIME = \"1D\"\n        starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")\n        starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()\n        # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:\n        #     stations = json.load(f)\n        stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n        stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])\n        stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])\n        location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04', \n                              '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')\n        location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}\n        instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')\n        instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}\n        component_priorities = ('E', 'N', 'Z', '1', '2', '3')\n        component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}\n        stations['location_priority'] = stations['location'].map(location_priority_map)\n        stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))\n        stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))\n        stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)\n        stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)\n\n        for starttime in starttimes:\n            for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):\n                prev = \"\"\n                nch = set()\n                for _, row in station.iterrows():\n                    if len(nch) &gt;= 3:\n                        break\n                    network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]\n                    if instrument != prev:\n                        prev = instrument\n                        nch = set()\n                    mseed_path = map_remote_path(\n                        cloud[\"provider\"],\n                        cloud[\"bucket\"],\n                        starttime,\n                        network,\n                        station,\n                        location,\n                        channel,\n                    )\n                    try:\n                        if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):\n                            # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")\n                            nch.add(channel[-1])\n                            continue\n                        if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):\n                            os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")\n                        with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:\n                            data = f.read()\n                        with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:\n                            f.write(data)\n                            nch.add(channel[-1])\n                    except Exception as e:\n                        # print(f\"Failed to download {e}\")\n                        pass\n\n    # %% Download from FDSN\n    domain = CircularDomain(\n        longitude=config[\"longitude0\"],\n        latitude=config[\"latitude0\"],\n        minradius=0,\n        maxradius=config[\"maxradius_degree\"],\n    )\n\n    restrictions = Restrictions(\n        starttime=obspy.UTCDateTime(config[\"starttime\"]),\n        endtime=obspy.UTCDateTime(config[\"endtime\"]),\n        chunklength_in_sec=3600 * 24, # 1 day\n        network=config[\"network\"] if \"network\" in config else None,\n        station=config[\"station\"] if \"station\" in config else None,\n        minimum_interstation_distance_in_m=0,\n        minimum_length=0.1,\n        reject_channels_with_gaps=False,\n    )\n\n    def get_mseed_storage(network, station, location, channel, starttime, endtime):\n        mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"\n        if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):\n            # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")\n            return True\n        return f\"{root_path}/{waveform_dir}/{mseed_name}\"\n\n    mdl = MassDownloader(\n        providers=config[\"provider\"],\n        # providers=[\"IRIS\"],\n    )\n    mdl.download(\n        domain,\n        restrictions,\n        mseed_storage=get_mseed_storage,\n        stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",\n        download_chunk_size_in_mb=20,\n        threads_per_client=3,\n        print_report=False,\n    )\n    \n    return\n</pre> def map_remote_path(provider, bucket, starttime, network, station, location, channel):      starttime = pd.Timestamp(starttime).round(\"h\").to_pydatetime()  # in case of 2021-01-01T23:59:xxx     if provider.lower() == \"scedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         if location == \"\":             location = \"__\"         path = f\"s3://{bucket}/{year}/{year}_{dayofyear}/{network}{station:_&lt;5}{channel}{location:_&lt;2}_{year}{dayofyear}.ms\"     elif provider.lower() == \"ncedc\":         year = starttime.strftime(\"%Y\")         dayofyear = starttime.strftime(\"%j\")         path = f\"s3://{bucket}/{network}/{year}/{year}.{dayofyear}/{station}.{network}.{channel}.{location}.D.{year}.{dayofyear}\"     else:         raise ValueError(f\"Unknown provider: {provider}\")     return path  def download_waveform(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      waveform_dir = f\"{region}/waveforms\"     if not os.path.exists(f\"{root_path}/{waveform_dir}\"):         os.makedirs(f\"{root_path}/{waveform_dir}\")     # print(json.dumps(config, indent=4))      ## Download from cloud     for provider in config[\"provider\"]:         if provider.lower() in [\"scedc\", \"ncedc\"]:             cloud = {                 \"provider\": provider.lower(),                 \"bucket\": f\"{provider.lower()}-pds/continuous_waveforms\",             }         else:             continue          DELTATIME = \"1D\"         starttime = datetime.fromisoformat(config[\"starttime\"]).strftime(\"%Y-%m-%d\")         starttimes = pd.date_range(starttime, config[\"endtime\"], freq=DELTATIME, tz=\"UTC\", inclusive=\"left\").to_list()         # with open(f'{root_path}/{region}/obspy/stations.json', 'r') as f:         #     stations = json.load(f)         stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)         stations[\"instrument\"] = stations['channel'].apply(lambda x: x[:-1])         stations[\"component\"] = stations['channel'].apply(lambda x: x[-1])         location_priorities = ('', '00', '10', '01', '20', '02', '30', '03', '40', '04',                                '50', '05', '60', '06', '70', '07', '80', '08', '90', '09')         location_priority_map = {loc: i for i, loc in enumerate(location_priorities)}         instrument_priorities = ('HH', 'BH', 'MH', 'EH', 'LH', 'HL', 'BL', 'ML', 'EL', 'LL', 'SH')         instrument_priority_map = {ch: i for i, ch in enumerate(instrument_priorities)}         component_priorities = ('E', 'N', 'Z', '1', '2', '3')         component_priority_map = {ch: i for i, ch in enumerate(component_priorities)}         stations['location_priority'] = stations['location'].map(location_priority_map)         stations['instrument_priority'] = stations['instrument'].apply(lambda x: instrument_priority_map.get(x, len(instrument_priorities)))         stations['component_priority'] = stations['component'].apply(lambda x: component_priority_map.get(x, len(component_priorities)))         stations.sort_values(['network', 'station', 'location_priority', 'instrument_priority', \"component_priority\"], inplace=True)         stations.drop(['location_priority', 'instrument_priority', 'component_priority'], axis=1, inplace=True)          for starttime in starttimes:             for _, station in tqdm(stations.groupby([\"network\", \"station\"]), desc=f\"Downloading {starttime}\"):                 prev = \"\"                 nch = set()                 for _, row in station.iterrows():                     if len(nch) &gt;= 3:                         break                     network, station, location, channel, instrument = row[\"network\"], row[\"station\"], row[\"location\"], row[\"channel\"], row[\"instrument\"]                     if instrument != prev:                         prev = instrument                         nch = set()                     mseed_path = map_remote_path(                         cloud[\"provider\"],                         cloud[\"bucket\"],                         starttime,                         network,                         station,                         location,                         channel,                     )                     try:                         if os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"):                             # print(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed downloaded.\")                             nch.add(channel[-1])                             continue                         if not os.path.exists(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\"):                             os.makedirs(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}\")                         with fsspec.open(f\"{mseed_path}\", \"rb\", s3={\"anon\": True}) as f:                             data = f.read()                         with open(f\"{root_path}/{waveform_dir}/{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\", \"wb\") as f:                             f.write(data)                             nch.add(channel[-1])                     except Exception as e:                         # print(f\"Failed to download {e}\")                         pass      # %% Download from FDSN     domain = CircularDomain(         longitude=config[\"longitude0\"],         latitude=config[\"latitude0\"],         minradius=0,         maxradius=config[\"maxradius_degree\"],     )      restrictions = Restrictions(         starttime=obspy.UTCDateTime(config[\"starttime\"]),         endtime=obspy.UTCDateTime(config[\"endtime\"]),         chunklength_in_sec=3600 * 24, # 1 day         network=config[\"network\"] if \"network\" in config else None,         station=config[\"station\"] if \"station\" in config else None,         minimum_interstation_distance_in_m=0,         minimum_length=0.1,         reject_channels_with_gaps=False,     )      def get_mseed_storage(network, station, location, channel, starttime, endtime):         mseed_name = f\"{starttime.strftime('%Y/%j')}/{network}.{station}.{location}.{channel}.mseed\"         if os.path.exists(f\"{root_path}/{waveform_dir}/{mseed_name}\"):             # print(f\"{root_path}/{waveform_dir}/{mseed_name} downloaded.\")             return True         return f\"{root_path}/{waveform_dir}/{mseed_name}\"      mdl = MassDownloader(         providers=config[\"provider\"],         # providers=[\"IRIS\"],     )     mdl.download(         domain,         restrictions,         mseed_storage=get_mseed_storage,         stationxml_storage=f\"{root_path}/{waveform_dir}/stations\",         download_chunk_size_in_mb=20,         threads_per_client=3,         print_report=False,     )          return  In\u00a0[10]: Copied! <pre>download_waveform(region=region, config=config)\n</pre> download_waveform(region=region, config=config) <pre>Downloading 2019-07-04 00:00:00+00:00:   0%|          | 0/15 [00:00&lt;?, ?it/s]</pre> <pre>Downloading 2019-07-04 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:15&lt;00:00,  1.03s/it]\nDownloading 2019-07-05 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:17&lt;00:00,  1.18s/it]\nDownloading 2019-07-06 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:22&lt;00:00,  1.47s/it]\nDownloading 2019-07-07 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:20&lt;00:00,  1.34s/it]\nDownloading 2019-07-08 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:19&lt;00:00,  1.33s/it]\nDownloading 2019-07-09 00:00:00+00:00: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15/15 [00:19&lt;00:00,  1.28s/it]\n[2025-04-07 06:45:31,183] - obspy.clients.fdsn.mass_downloader - INFO: Initializing FDSN client(s) for SCEDC.\n[2025-04-07 06:45:31,186] - obspy.clients.fdsn.mass_downloader - INFO: Successfully initialized 1 client(s): SCEDC.\n[2025-04-07 06:45:31,187] - obspy.clients.fdsn.mass_downloader - INFO: Total acquired or preexisting stations: 0\n[2025-04-07 06:45:31,187] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Requesting unreliable availability.\n[2025-04-07 06:45:38,616] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Successfully requested availability (7.43 seconds)\n[2025-04-07 06:45:38,852] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Found 14 stations (36 channels).\n[2025-04-07 06:45:38,854] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Will attempt to download data from 14 stations.\n[2025-04-07 06:45:38,856] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - Status for 216 time intervals/channels before downloading: IGNORE\n[2025-04-07 06:45:38,857] - obspy.clients.fdsn.mass_downloader - INFO: Client 'SCEDC' - No station information to download.\n</pre> In\u00a0[11]: Copied! <pre>def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:\n\n    result_path = f\"{region}/phasenet\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    # %%\n    waveform_dir = f\"{region}/waveforms\"\n    mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))\n\n    # %% group 3C channels\n    mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))\n\n    # %%\n    with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:\n        fp.write(\"fname\\n\")\n        fp.write(\"\\n\".join(mseed_list))\n\n    # %%\n    model_path = \"QuakeFlow/PhaseNet/\"\n    cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"\n    # cmd += \" --sampling_rate 100\" \n    os.system(cmd)\n\n    return f\"{root_path}/{result_path}/phasenet_picks.csv\"\n</pre> def run_phasenet(root_path: str = \"local\", region: str = \"demo\", config: Dict = {} ) -&gt; str:      result_path = f\"{region}/phasenet\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      # %%     waveform_dir = f\"{region}/waveforms\"     mseed_list = sorted(glob(f\"{root_path}/{waveform_dir}/????/???/*.mseed\"))      # %% group 3C channels     mseed_list = sorted(list(set([x.split(\".mseed\")[0][:-1] + \"*.mseed\" for x in mseed_list])))      # %%     with open(f\"{root_path}/{result_path}/mseed_list.csv\", \"w\") as fp:         fp.write(\"fname\\n\")         fp.write(\"\\n\".join(mseed_list))      # %%     model_path = \"QuakeFlow/PhaseNet/\"     cmd = f\"python {model_path}/phasenet/predict.py --model={model_path}/model/190703-214543 --data_dir=./ --data_list={root_path}/{result_path}/mseed_list.csv --response_xml={root_path}/{region}/obspy/inventory.xml --format=mseed --amplitude --highpass_filter=1.0 --result_dir={root_path}/{result_path} --result_fname=phasenet_picks --batch_size=1\"     # cmd += \" --sampling_rate 100\"      os.system(cmd)      return f\"{root_path}/{result_path}/phasenet_picks.csv\"  In\u00a0[12]: Copied! <pre>phasenet_picks = run_phasenet(region=region, config=config)\n</pre> phasenet_picks = run_phasenet(region=region, config=config) <pre>2025-04-07 06:45:40.695926: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-07 06:45:42,446 Pred log: local/ridgecrest/phasenet\n2025-04-07 06:45:42,446 Dataset size: 102\n2025-04-07 06:45:42.498743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-07 06:45:42.500855: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-07 06:45:42,522 Model: depths 5, filters 8, filter size 7x1, pool size: 4x1, dilation rate: 1x1\n2025-04-07 06:45:43.801005: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2025-04-07 06:45:43.880995: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n2025-04-07 06:45:44,043 restoring model QuakeFlow/PhaseNet//model/190703-214543/model_95.ckpt\nPred: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102/102 [09:35&lt;00:00,  5.64s/it]\n</pre> <pre>Done with 265357 P-picks and 270609 S-picks\n</pre> In\u00a0[\u00a0]: Copied! <pre>def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{region}/phasenet\"\n    result_path = f\"{region}/gamma\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    picks_csv = f\"{data_path}/phasenet_picks.csv\"\n    gamma_events_csv = f\"{result_path}/gamma_events.csv\"\n    gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"\n    station_json = f\"{region}/obspy/stations.json\"\n\n    ## read picks\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")\n    picks[\"id\"] = picks[\"station_id\"]\n    picks[\"timestamp\"] = picks[\"phase_time\"]\n    picks[\"amp\"] = picks[\"phase_amplitude\"]\n    picks[\"type\"] = picks[\"phase_type\"]\n    picks[\"prob\"] = picks[\"phase_score\"]\n\n    ## read stations\n    # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")\n    stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)\n    stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")\n    stations[[\"x(km)\", \"y(km)\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n    # print(stations.to_string())\n\n    ## setting GaMMA configs\n    config[\"use_dbscan\"] = True\n    config[\"use_amplitude\"] = True\n    config[\"method\"] = \"BGMM\"\n    if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture\n        config[\"oversample_factor\"] = 5\n    if config[\"method\"] == \"GMM\":  ## GaussianMixture\n        config[\"oversample_factor\"] = 1\n\n    # earthquake location\n    config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}\n    config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    zmin = config[\"mindepth\"] if \"mindepth\" in config else 0\n    zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30\n    config[\"x(km)\"] = (xmin, xmax)\n    config[\"y(km)\"] = (ymin, ymax)\n    config[\"z(km)\"] = (zmin, zmax)\n    config[\"bfgs_bounds\"] = (\n        (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x\n        (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y\n        (0, config[\"z(km)\"][1] + 1),  # z\n        (None, None),  # t\n    )\n\n    # DBSCAN\n    config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s\n    config[\"dbscan_min_samples\"] = 3\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    h = 0.3\n    vel = {\"z\": zz, \"p\": vp, \"s\": vs}\n    config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}\n\n    # filtering\n    config[\"min_picks_per_eq\"] = 5\n    config[\"min_p_picks_per_eq\"] = 0\n    config[\"min_s_picks_per_eq\"] = 0\n    config[\"max_sigma11\"] = 2.0  # s\n    config[\"max_sigma22\"] = 1.0  # log10(m/s)\n    config[\"max_sigma12\"] = 1.0  # covariance\n\n    ## filter picks without amplitude measurements\n    if config[\"use_amplitude\"]:\n        picks = picks[picks[\"amp\"] != -1]\n\n    # for k, v in config.items():\n    #     print(f\"{k}: {v}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n\n    # %%\n    event_idx0 = 0  ## current earthquake index\n    assignments = []\n    events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])\n\n    if len(events) == 0:\n        return \n    \n    ## create catalog\n    events = pd.DataFrame(events)\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z(km)\"]\n    events.sort_values(\"time\", inplace=True)\n    with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:\n        events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n    ## add assignment to picks\n    assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])\n    picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})\n    picks.sort_values([\"phase_time\"], inplace=True)\n    with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:\n        picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")\n\n    # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"\n    return events\n</pre> def run_gamma(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{region}/phasenet\"     result_path = f\"{region}/gamma\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      picks_csv = f\"{data_path}/phasenet_picks.csv\"     gamma_events_csv = f\"{result_path}/gamma_events.csv\"     gamma_picks_csv = f\"{result_path}/gamma_picks.csv\"     station_json = f\"{region}/obspy/stations.json\"      ## read picks     picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     picks.drop(columns=[\"event_index\"], inplace=True, errors=\"ignore\")     picks[\"id\"] = picks[\"station_id\"]     picks[\"timestamp\"] = picks[\"phase_time\"]     picks[\"amp\"] = picks[\"phase_amplitude\"]     picks[\"type\"] = picks[\"phase_type\"]     picks[\"prob\"] = picks[\"phase_score\"]      ## read stations     # stations = pd.read_json(f\"{root_path}/{station_json}\", orient=\"index\")     stations = pd.read_csv(f\"{root_path}/{region}/obspy/stations.csv\", na_filter=False)     stations[\"id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']} +units=km\")     stations[[\"x(km)\", \"y(km)\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z(km)\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)     # print(stations.to_string())      ## setting GaMMA configs     config[\"use_dbscan\"] = True     config[\"use_amplitude\"] = True     config[\"method\"] = \"BGMM\"     if config[\"method\"] == \"BGMM\":  ## BayesianGaussianMixture         config[\"oversample_factor\"] = 5     if config[\"method\"] == \"GMM\":  ## GaussianMixture         config[\"oversample_factor\"] = 1      # earthquake location     config[\"vel\"] = {\"p\": 6.0, \"s\": 6.0 / 1.75}     config[\"dims\"] = [\"x(km)\", \"y(km)\", \"z(km)\"]     minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     # zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     zmin = config[\"mindepth\"] if \"mindepth\" in config else 0     zmax = config[\"maxdepth\"] if \"maxdepth\" in config else 30     config[\"x(km)\"] = (xmin, xmax)     config[\"y(km)\"] = (ymin, ymax)     config[\"z(km)\"] = (zmin, zmax)     config[\"bfgs_bounds\"] = (         (config[\"x(km)\"][0] - 1, config[\"x(km)\"][1] + 1),  # x         (config[\"y(km)\"][0] - 1, config[\"y(km)\"][1] + 1),  # y         (0, config[\"z(km)\"][1] + 1),  # z         (None, None),  # t     )      # DBSCAN     config[\"dbscan_eps\"] = estimate_eps(stations, config[\"vel\"][\"p\"])  # s     config[\"dbscan_min_samples\"] = 3      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     h = 0.3     vel = {\"z\": zz, \"p\": vp, \"s\": vs}     config[\"eikonal\"] = {\"vel\": vel, \"h\": h, \"xlim\": config[\"x(km)\"], \"ylim\": config[\"y(km)\"], \"zlim\": config[\"z(km)\"]}      # filtering     config[\"min_picks_per_eq\"] = 5     config[\"min_p_picks_per_eq\"] = 0     config[\"min_s_picks_per_eq\"] = 0     config[\"max_sigma11\"] = 2.0  # s     config[\"max_sigma22\"] = 1.0  # log10(m/s)     config[\"max_sigma12\"] = 1.0  # covariance      ## filter picks without amplitude measurements     if config[\"use_amplitude\"]:         picks = picks[picks[\"amp\"] != -1]      # for k, v in config.items():     #     print(f\"{k}: {v}\")      print(f\"Number of picks: {len(picks)}\")      # %%     event_idx0 = 0  ## current earthquake index     assignments = []     events, assignments = association(picks, stations, config, event_idx0, config[\"method\"])      if len(events) == 0:         return           ## create catalog     events = pd.DataFrame(events)     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(longitude=x[\"x(km)\"], latitude=x[\"y(km)\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z(km)\"]     events.sort_values(\"time\", inplace=True)     with open(f\"{root_path}/{gamma_events_csv}\", \"w\") as fp:         events.to_csv(fp, index=False, float_format=\"%.3f\", date_format=\"%Y-%m-%dT%H:%M:%S.%f\")     ## add assignment to picks     assignments = pd.DataFrame(assignments, columns=[\"pick_index\", \"event_index\", \"gamma_score\"])     picks = picks.join(assignments.set_index(\"pick_index\")).fillna(-1).astype({\"event_index\": int})     picks.sort_values([\"phase_time\"], inplace=True)     with open(f\"{root_path}/{gamma_picks_csv}\", \"w\") as fp:         picks.to_csv(fp, index=False, date_format=\"%Y-%m-%dT%H:%M:%S.%f\")      # return f\"{root_path}/{result_path}/gamma_picks.csv\", f\"{root_path}/{result_path}/gamma_events.csv\"     return events  In\u00a0[14]: Copied! <pre>gamma_catalog = run_gamma(region=region, config=config)\n</pre> gamma_catalog = run_gamma(region=region, config=config) <pre>Number of picks: 535966\nEikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 2.243\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.044\nSplitting 461 picks using eps=6.19\nSplitting 436 picks using eps=6.19\nSplitting 389 picks using eps=6.19\nSplitting 359 picks using eps=6.19\nSplitting 508 picks using eps=6.19\nSplitting 362 picks using eps=6.19\nAssociating 11463 clusters with 15 CPUs\n.................................................................\nAssociated 100 events\n........................................\nAssociated 200 events\n.....................................\nAssociated 300 events\n....................................\nAssociated 400 events\n.....................................\nAssociated 500 events\n.........................\nAssociated 600 events\n........................................\nAssociated 700 events\n.............................................\nAssociated 800 events\n........................................\nAssociated 900 events\n................................\nAssociated 1000 events\n.................................................\nAssociated 1100 events\n.............................................\nAssociated 1200 events\n............................................\nAssociated 1300 events\n...........................\nInitialization 1 did not converge.\n..................\nAssociated 1400 events\n........................................\nAssociated 1500 events\n...................................\nAssociated 1600 events\n....................................\nAssociated 1700 events\n......................................................\nAssociated 1800 events\n..............................................\nAssociated 1900 events\n........................................\nAssociated 2000 events\n...............................................\nAssociated 2100 events\n.........\nInitialization 1 did not converge.\n...........................\nAssociated 2200 events\n..................................\nAssociated 2300 events\n.....................................\nAssociated 2400 events\n........................................\nAssociated 2500 events\n.................................\nInitialization 1 did not converge.\n.....\nAssociated 2600 events\n..........................................\nAssociated 2700 events\n.........................................\nAssociated 2800 events\n......................................\nAssociated 2900 events\n.........................\nAssociated 3000 events\n...................................\nAssociated 3100 events\n.....................................\nAssociated 3200 events\n..........................................\nAssociated 3300 events\n.......................................\nAssociated 3400 events\n......................................\nAssociated 3500 events\n.......................................\nAssociated 3600 events\n..............................\nAssociated 3700 events\n.....................................\nAssociated 3800 events\n...........................\nAssociated 3900 events\n..........................................\nAssociated 4000 events\n.........................................\nAssociated 4100 events\n................................................\nAssociated 4200 events\n.....................................\nAssociated 4300 events\n..........................................\nAssociated 4400 events\n...........................................\nAssociated 4500 events\n..........................................\nAssociated 4600 events\n....................................\nAssociated 4700 events\n...............................................\nAssociated 4800 events\n.............................\nInitialization 1 did not converge.\n.........\nAssociated 4900 events\n.....................................\nAssociated 5000 events\n..........................................\nAssociated 5100 events\n................................................\nAssociated 5200 events\n.........................................\nAssociated 5300 events\n...............................\nAssociated 5400 events\n................................................\nAssociated 5500 events\n...................................\nAssociated 5600 events\n............................\nAssociated 5700 events\n....................................\nAssociated 5800 events\n......................................\nAssociated 5900 events\n.......................................\nAssociated 6000 events\n....................................\nAssociated 6100 events\n..........................................\nAssociated 6200 events\n............................................\nAssociated 6300 events\n..........................................\nAssociated 6400 events\n......................................\nAssociated 6500 events\n................................\nAssociated 6600 events\n..............................................\nAssociated 6700 events\n........................................\nAssociated 6800 events\n.............................\nAssociated 6900 events\n...............................................\nAssociated 7000 events\n..............................................\nAssociated 7100 events.\n............................................\nAssociated 7200 events\n.......................................\nAssociated 7300 events\n...............................\nAssociated 7400 events\n.......\nInitialization 1 did not converge.\n...............................\nAssociated 7500 events\n.............\nInitialization 1 did not converge.\n........\nInitialization 1 did not converge.\n.....................\nAssociated 7600 events\n...........................................\nAssociated 7700 events\n................................................\nAssociated 7800 events\n..............................\nAssociated 7900 events\n............................\nAssociated 8000 events\n........................................\nAssociated 8100 events\n......................................\nAssociated 8200 events\n............................................\nAssociated 8300 events\n...........\nInitialization 1 did not converge.\n................................\nAssociated 8400 events\n..................................\nAssociated 8500 events\n......................................\nAssociated 8600 events\n..................................\nInitialization 1 did not converge.\nAssociated 8700 events\n\n..................................\nAssociated 8800 events\n...................................................\nAssociated 8900 events\n.........................................\nAssociated 9000 events\n............................................\nAssociated 9100 events\n.....................................\nAssociated 9200 events\n....................................\nAssociated 9300 events\n......................................\nAssociated 9400 events\n.......................\nInitialization 1 did not converge.\n............\nAssociated 9500 events\n............................................\nAssociated 9600 events\n.....................................\nAssociated 9700 events\n......................................\nAssociated 9800 events\n.................\nInitialization 1 did not converge.\n....................\nAssociated 9900 events\n.....................................\nAssociated 10000 events\n....................................\nAssociated 10100 events\n..........................................\nAssociated 10200 events\n..................................\nAssociated 10300 events\n.................................\nAssociated 10400 events\n.......................................\nAssociated 10500 events\n.................................\nAssociated 10600 events\n.................................\nAssociated 10700 events.\n..........................................\nAssociated 10800 events\n....................................\nAssociated 10900 events\n......................................\nAssociated 11000 events\n.................................\nAssociated 11100 events\n....................................\nAssociated 11200 events\n...................................\nAssociated 11300 events\n.................................................\nAssociated 11400 events\n............................................\nAssociated 11500 events\n......................................\nAssociated 11600 events\n....................................\nAssociated 11700 events\n........................................\nAssociated 11800 events\n.............................................\nAssociated 11900 events\n...\nInitialization 1 did not converge.\n......................................\nAssociated 12000 events\n.................................\nInitialization 1 did not converge.\n......\nAssociated 12100 events\n........................................\nAssociated 12200 events\n...........................................\nAssociated 12300 events\n...............................................\nAssociated 12400 events\n...................................\nAssociated 12500 events\n........................................\nAssociated 12600 events\n........................................\nAssociated 12700 events\n................................\nAssociated 12800 events\n............................................\nAssociated 12900 events\n.................................\nAssociated 13000 events\n............................................\nAssociated 13100 events\n.........................................\nAssociated 13200 events.\n............................................\nAssociated 13300 events\n.................................\nAssociated 13400 events\n...............................................\nAssociated 13500 events\n..................................\nAssociated 13600 events\n................................\nAssociated 13700 events\n............................\nAssociated 13800 events\n.........................................\nAssociated 13900 events\n....................................\nAssociated 14000 events\n...................................\nAssociated 14100 events\n.............................................\nAssociated 14200 events\n........................\nAssociated 14300 events\n.............................\nAssociated 14400 events\n.............................\nAssociated 14500 events\n.....................................\nAssociated 14600 events\n...............................\nAssociated 14700 events\n...........................................\nAssociated 14800 events\n.............................\nInitialization 1 did not converge.\n.................\nAssociated 14900 events\n........................................\nAssociated 15000 events\n...........................................\nAssociated 15100 events\n.............................\nAssociated 15200 events\n.........................................\nAssociated 15300 events\n............................\nAssociated 15400 events\n....................................\nInitialization 1 did not converge.\n.........\nAssociated 15500 events\n....................................\nAssociated 15600 events\n......................................\nAssociated 15700 events\n......\nInitialization 1 did not converge.\n....................................\nAssociated 15800 events\n..................................\nAssociated 15900 events\n........................................\nAssociated 16000 events\n.....................................\nAssociated 16100 events\n......................................\nAssociated 16200 events\n................................\nAssociated 16300 events\n.....................................\nAssociated 16400 events\n.................................\nAssociated 16500 events\n....................................\nAssociated 16600 events\n............................................\nAssociated 16700 events\n...............................................\nAssociated 16800 events\n.....................................\nAssociated 16900 events\n.......................................\nAssociated 17000 events\n..........................................\nAssociated 17100 events\n..............................................\nAssociated 17200 events\n.................................\nAssociated 17300 events\n.....................................\nAssociated 17400 events\n......................................\nAssociated 17500 events\n.........................................\nAssociated 17600 events\n.......................................\nAssociated 17700 events\n...........................................\nAssociated 17800 events\n...................................\nAssociated 17900 events\n.................................\nAssociated 18000 events\n.............................................\nAssociated 18100 events\n...............................................\nAssociated 18200 events\n...............................................\nAssociated 18300 events\n......................................\nAssociated 18400 events\n..........................................\nAssociated 18500 events\n...................................\nAssociated 18600 events\n..................................\nAssociated 18700 events\n.....................................\nAssociated 18800 events\n........................................\nAssociated 18900 events\n..........................................\nAssociated 19000 events\n.................................\nAssociated 19100 events\n............................................\nAssociated 19200 events\n..........................................\nAssociated 19300 events\n...............................\nAssociated 19400 events\n..................................\nAssociated 19500 events\n.......................................\nAssociated 19600 events\n..........................................\nAssociated 19700 events\n.................................\nAssociated 19800 events\n......................................................\nAssociated 19900 events\n............................................\nAssociated 20000 events\n.......................................\nAssociated 20100 events\n..................................\nAssociated 20200 events\n.................................\nAssociated 20300 events\n........................................\nAssociated 20400 events\n......................................\nAssociated 20500 events\n..........................................\nAssociated 20600 events\n...................................\nAssociated 20700 events\n...............................\nAssociated 20800 events\n.....................................\nAssociated 20900 events\n......................................\nAssociated 21000 events\n..........................................\nAssociated 21100 events\n...............................................\nAssociated 21200 events\n...............................................\nAssociated 21300 events\n.........................................\nAssociated 21400 events\n.....................................\nAssociated 21500 events\n..............................................\nAssociated 21600 events\n...............................................\nAssociated 21700 events\n...................................\nAssociated 21800 events\n..............................\nAssociated 21900 events\n............................\nAssociated 22000 events\n.....................................\nAssociated 22100 events\n............................................\nAssociated 22200 events\n...................................\nAssociated 22300 events\n.............................................\nAssociated 22400 events\n............................\nAssociated 22500 events\n...........................\nAssociated 22600 events\n................................\nAssociated 22700 events\n........................................\nAssociated 22800 events\n.............................\nAssociated 22900 events\n............................................\nAssociated 23000 events\n........\nInitialization 1 did not converge.\n..................................\nAssociated 23100 events\n.............................................\nAssociated 23200 events\n........................................\nAssociated 23300 events\n........................................\nAssociated 23400 events\n...............................\nAssociated 23500 events\n.................................................\nAssociated 23600 events\n.............................\nAssociated 23700 events\n..................................\nAssociated 23800 events\n..................................\nAssociated 23900 events\n..............................................\nAssociated 24000 events\n....................................\nAssociated 24100 events\n...............................................\nInitialization 1 did not converge.\n\nAssociated 24200 events\n...................................\nAssociated 24300 events\n......................................\nAssociated 24400 events\n................................\nAssociated 24500 events\n.....................................\nAssociated 24600 events\n.................................\nAssociated 24700 events\n..............................................\nAssociated 24800 events\n.........................................\nAssociated 24900 events\n..............................\nAssociated 25000 events\n...........................................\nAssociated 25100 events\n........................................\nAssociated 25200 events\n................................\nAssociated 25300 events\n............................\nAssociated 25400 events\n..................................\nAssociated 25500 events\n.......................................\nAssociated 25600 events\n................................................\nAssociated 25700 events\n.............................................\nAssociated 25800 events\n....................................\nAssociated 25900 events\n.............................\nAssociated 26000 events\n....\nInitialization 1 did not converge.\n.....................................\nAssociated 26100 events\n............................................\nAssociated 26200 events\n.............................\nAssociated 26300 events\n..............................\nAssociated 26400 events\n.........................................\nAssociated 26500 events\n.....................................\nAssociated 26600 events\n.............................\nAssociated 26700 events\n......................\nAssociated 26800 events\n......................................\nAssociated 26900 events\n.....................................\nAssociated 27000 events\n......................................\nAssociated 27100 events\n.........................................\nAssociated 27200 events\n....................\nInitialization 1 did not converge.\n...................\nAssociated 27300 events\n.....................................\nAssociated 27400 events\n..........................................\nAssociated 27500 events\n.........................................\nAssociated 27600 events\n..........................................\nAssociated 27700 events\n.................................................\nAssociated 27800 events\n..........................\nAssociated 27900 events\n..............................................\nAssociated 28000 events\n..............................................\nAssociated 28100 events\n...............................................\nAssociated 28200 events\n...................................\nAssociated 28300 events\n........................................\nAssociated 28400 events\n.................................\nAssociated 28500 events\n................................\nAssociated 28600 events\n.............................................\nAssociated 28700 events\n.................................\nAssociated 28800 events\n...............................\nAssociated 28900 events\n.....................................\nAssociated 29000 events\n..\nInitialization 1 did not converge.\n.......................................\nAssociated 29100 events\n.......................................\nAssociated 29200 events\n..............................\nInitialization 1 did not converge.\n.\nAssociated 29300 events\n.................................\nAssociated 29400 events\n........................................\nAssociated 29500 events\n......................................\nAssociated 29600 events\n.............................\nAssociated 29700 events\n....................................</pre> In\u00a0[15]: Copied! <pre>plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config)\n</pre> plot_catalog(gamma_catalog, \"GaMMA\", region=region, config=config) In\u00a0[\u00a0]: Copied! <pre>def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):\n\n    data_path = f\"{root_path}/{region}/gamma\"\n    result_path = f\"{root_path}/{region}/adloc\"\n    figure_path = f\"{root_path}/{region}/adloc/figures\"\n    if not os.path.exists(result_path):\n        os.makedirs(result_path)\n    if not os.path.exists(figure_path):\n        os.makedirs(figure_path)\n\n    picks_file = f\"{data_path}/gamma_picks.csv\"\n    events_file = f\"{data_path}/gamma_events.csv\"\n    stations_file = f\"{root_path}/{region}/obspy/stations.csv\"\n\n    proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")\n\n    ## read picks and associated events\n    picks = pd.read_csv(picks_file)\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])\n     # drop unnecessary columns\n    picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")\n    if os.path.exists(events_file):\n        events = pd.read_csv(events_file)\n        events[\"time\"] = pd.to_datetime(events[\"time\"])\n        events[[\"x_km\", \"y_km\"]] = events.apply(\n            lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n        )\n        events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0\n    else:\n        events = None\n\n    ## read stations\n    # stations = pd.read_json(stations_file, orient=\"index\")\n    stations = pd.read_csv(stations_file, na_filter=False)\n    stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)\n    stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()\n    stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000\n    if \"station_term_time_p\" not in stations.columns:\n        stations[\"station_term_time_p\"] = 0.0\n    if \"station_term_time_s\" not in stations.columns:\n        stations[\"station_term_time_s\"] = 0.0\n    if \"station_term_amplitude\" not in stations.columns:\n        stations[\"station_term_amplitude\"] = 0.0\n    stations[[\"x_km\", \"y_km\"]] = stations.apply(\n        lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1\n    )\n    stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)\n\n    ## setting ADLoc configs\n    config[\"use_amplitude\"] = True\n\n    minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]\n    minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]\n    xmin, ymin = proj(minlon, minlat)\n    xmax, ymax = proj(maxlon, maxlat)\n    zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]\n    config[\"xlim_km\"] = (xmin, xmax)\n    config[\"ylim_km\"] = (ymin, ymax)\n    config[\"zlim_km\"] = (zmin, zmax)\n\n    ## Eikonal for 1D velocity model\n    zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]\n    vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]\n    vp_vs_ratio = 1.73\n    vs = [v / vp_vs_ratio for v in vp]\n    # Northern California (Gil7)\n    # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]\n    # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]\n    # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]\n    h = 0.3\n    vel = {\"Z\": zz, \"P\": vp, \"S\": vs}\n    config[\"eikonal\"] = {\n        \"vel\": vel,\n        \"h\": h,\n        \"xlim_km\": config[\"xlim_km\"],\n        \"ylim_km\": config[\"ylim_km\"],\n        \"zlim_km\": config[\"zlim_km\"],\n    }\n    config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])\n\n    # RASAC\n    config[\"min_picks\"] = 6\n    config[\"min_picks_ratio\"] = 0.5\n    config[\"max_residual_time\"] = 1.0\n    config[\"max_residual_amplitude\"] = 1.0\n    config[\"min_score\"] = 0.5\n    config[\"min_s_picks\"] = 1.5\n    config[\"min_p_picks\"] = 1.5\n\n    config[\"bfgs_bounds\"] = (\n        (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x\n        (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y\n        (0, config[\"zlim_km\"][1] + 1),\n        (None, None),  # t\n    )\n\n    # %%\n    mapping_phase_type_int = {\"P\": 0, \"S\": 1}\n    picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)\n    if \"phase_amplitude\" in picks.columns:\n        picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)\n\n    # %%\n    stations[\"idx_sta\"] = np.arange(len(stations))\n    if events is None:\n        picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")\n        events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})\n        events[\"z_km\"] = 10.0  # km default depth\n        events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)\n        events[\"event_index\"] = events.index\n        events.reset_index(drop=True, inplace=True)\n        events[\"idx_eve\"] = np.arange(len(events))\n        picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)\n    else:\n        events[\"idx_eve\"] = np.arange(len(events))\n\n    picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")\n    picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")\n\n\n    # for key, value in config.items():\n    #     print(f\"{key}: {value}\")\n\n    print(f\"Number of picks: {len(picks)}\")\n    print(f\"Number of events: {len(events)}\")\n\n    # %%\n    estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])\n\n    # %%\n    MAX_SST_ITER = 8\n    events_init = events.copy()\n\n    for iter in range(MAX_SST_ITER):\n        picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)\n\n        station_term_amp = (\n            picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()\n        )\n        station_term_amp.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)\n\n        station_term_time = (\n            picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()\n        )\n        station_term_time.set_index(\"idx_sta\", inplace=True)\n        stations[\"station_term_time_p\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)\n        )\n        stations[\"station_term_time_s\"] += (\n            stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)\n        )\n\n        if \"event_index\" not in events.columns:\n            events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n        events[[\"longitude\", \"latitude\"]] = events.apply(\n            lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n        )\n        events[\"depth_km\"] = events[\"z_km\"]\n\n        picks[\"adloc_mask\"] = picks[\"mask\"]\n        picks[\"adloc_residual_time\"] = picks[\"residual_time\"]\n        picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]\n\n        picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)\n        events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)\n        stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)\n\n    # %%\n    if \"event_index\" not in events.columns:\n        events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]\n    events[[\"longitude\", \"latitude\"]] = events.apply(\n        lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1\n    )\n    events[\"depth_km\"] = events[\"z_km\"]\n    events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n    events.sort_values([\"time\"], inplace=True)\n\n    picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})\n    picks.drop(\n        [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"\n    )\n    picks.sort_values([\"phase_time\"], inplace=True)\n\n    stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")\n\n    picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)\n    events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)\n    stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)\n\n    return events\n</pre> def run_adloc(root_path: str = \"local\", region: str = \"demo\", config: Dict = {}):      data_path = f\"{root_path}/{region}/gamma\"     result_path = f\"{root_path}/{region}/adloc\"     figure_path = f\"{root_path}/{region}/adloc/figures\"     if not os.path.exists(result_path):         os.makedirs(result_path)     if not os.path.exists(figure_path):         os.makedirs(figure_path)      picks_file = f\"{data_path}/gamma_picks.csv\"     events_file = f\"{data_path}/gamma_events.csv\"     stations_file = f\"{root_path}/{region}/obspy/stations.csv\"      proj = Proj(f\"+proj=aeqd +lon_0={config['longitude0']} +lat_0={config['latitude0']}  +units=km\")      ## read picks and associated events     picks = pd.read_csv(picks_file)     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"])      # drop unnecessary columns     picks.drop([\"id\", \"timestamp\", \"type\", \"amp\", \"prob\", \"event_idx\"], axis=1, inplace=True, errors=\"ignore\")     if os.path.exists(events_file):         events = pd.read_csv(events_file)         events[\"time\"] = pd.to_datetime(events[\"time\"])         events[[\"x_km\", \"y_km\"]] = events.apply(             lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1         )         events[\"z_km\"] = events[\"depth_km\"] if \"depth_km\" in events.columns else 10.0     else:         events = None      ## read stations     # stations = pd.read_json(stations_file, orient=\"index\")     stations = pd.read_csv(stations_file, na_filter=False)     stations[\"station_id\"] = stations.apply(lambda x: f\"{x['network']}.{x['station']}.{x['location']}.{x['channel'][:-1]}\", axis=1)     stations = stations.groupby(\"station_id\").agg(lambda x: x.iloc[0] if len(set(x)) == 1 else sorted(list(x))).reset_index()     stations[\"depth_km\"] = -stations[\"elevation_m\"] / 1000     if \"station_term_time_p\" not in stations.columns:         stations[\"station_term_time_p\"] = 0.0     if \"station_term_time_s\" not in stations.columns:         stations[\"station_term_time_s\"] = 0.0     if \"station_term_amplitude\" not in stations.columns:         stations[\"station_term_amplitude\"] = 0.0     stations[[\"x_km\", \"y_km\"]] = stations.apply(         lambda x: pd.Series(proj(longitude=x.longitude, latitude=x.latitude)), axis=1     )     stations[\"z_km\"] = stations[\"elevation_m\"].apply(lambda x: -x / 1e3)      ## setting ADLoc configs     config[\"use_amplitude\"] = True      minlat, maxlat = config[\"latitude0\"] - config[\"maxradius_degree\"], config[\"latitude0\"] + config[\"maxradius_degree\"]     minlon, maxlon = config[\"longitude0\"] - config[\"maxradius_degree\"], config[\"longitude0\"] + config[\"maxradius_degree\"]     xmin, ymin = proj(minlon, minlat)     xmax, ymax = proj(maxlon, maxlat)     zmin, zmax = config[\"mindepth\"], config[\"maxdepth\"]     config[\"xlim_km\"] = (xmin, xmax)     config[\"ylim_km\"] = (ymin, ymax)     config[\"zlim_km\"] = (zmin, zmax)      ## Eikonal for 1D velocity model     zz = [0.0, 5.5, 5.5, 16.0, 16.0, 32.0, 32.0]     vp = [5.5, 5.5, 6.3, 6.3, 6.7, 6.7, 7.8]     vp_vs_ratio = 1.73     vs = [v / vp_vs_ratio for v in vp]     # Northern California (Gil7)     # zz = [0.0, 1.0, 3.0, 4.0, 5.0, 17.0, 25.0, 62.0]     # vp = [3.2, 3.2, 4.5, 4.8, 5.51, 6.21, 6.89, 7.83]     # vs = [1.5, 1.5, 2.4, 2.78, 3.18, 3.40, 3.98, 4.52]     h = 0.3     vel = {\"Z\": zz, \"P\": vp, \"S\": vs}     config[\"eikonal\"] = {         \"vel\": vel,         \"h\": h,         \"xlim_km\": config[\"xlim_km\"],         \"ylim_km\": config[\"ylim_km\"],         \"zlim_km\": config[\"zlim_km\"],     }     config[\"eikonal\"] = init_eikonal2d(config[\"eikonal\"])      # RASAC     config[\"min_picks\"] = 6     config[\"min_picks_ratio\"] = 0.5     config[\"max_residual_time\"] = 1.0     config[\"max_residual_amplitude\"] = 1.0     config[\"min_score\"] = 0.5     config[\"min_s_picks\"] = 1.5     config[\"min_p_picks\"] = 1.5      config[\"bfgs_bounds\"] = (         (config[\"xlim_km\"][0] - 1, config[\"xlim_km\"][1] + 1),  # x         (config[\"ylim_km\"][0] - 1, config[\"ylim_km\"][1] + 1),  # y         (0, config[\"zlim_km\"][1] + 1),         (None, None),  # t     )      # %%     mapping_phase_type_int = {\"P\": 0, \"S\": 1}     picks[\"phase_type\"] = picks[\"phase_type\"].map(mapping_phase_type_int)     if \"phase_amplitude\" in picks.columns:         picks[\"phase_amplitude\"] = picks[\"phase_amplitude\"].apply(lambda x: np.log10(x) + 2.0)  # convert to log10(cm/s)      # %%     stations[\"idx_sta\"] = np.arange(len(stations))     if events is None:         picks = picks.merge(stations[[\"station_id\", \"x_km\", \"y_km\", \"z_km\"]], on=\"station_id\")         events = picks.groupby(\"event_index\").agg({\"x_km\": \"mean\", \"y_km\": \"mean\", \"z_km\": \"mean\", \"phase_time\": \"min\"})         events[\"z_km\"] = 10.0  # km default depth         events.rename({\"phase_time\": \"time\"}, axis=1, inplace=True)         events[\"event_index\"] = events.index         events.reset_index(drop=True, inplace=True)         events[\"idx_eve\"] = np.arange(len(events))         picks.drop([\"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True)     else:         events[\"idx_eve\"] = np.arange(len(events))      picks = picks.merge(events[[\"event_index\", \"idx_eve\"]], on=\"event_index\")     picks = picks.merge(stations[[\"station_id\", \"idx_sta\"]], on=\"station_id\")       # for key, value in config.items():     #     print(f\"{key}: {value}\")      print(f\"Number of picks: {len(picks)}\")     print(f\"Number of events: {len(events)}\")      # %%     estimator = ADLoc(config, stations=stations[[\"x_km\", \"y_km\", \"z_km\"]].values, eikonal=config[\"eikonal\"])      # %%     MAX_SST_ITER = 8     events_init = events.copy()      for iter in range(MAX_SST_ITER):         picks, events = invert_location(picks, stations, config, estimator, events_init=events_init, iter=iter)          station_term_amp = (             picks[picks[\"mask\"] == 1.0].groupby(\"idx_sta\").agg({\"residual_amplitude\": \"median\"}).reset_index()         )         station_term_amp.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_amplitude\"] += stations[\"idx_sta\"].map(station_term_amp[\"residual_amplitude\"]).fillna(0)          station_term_time = (             picks[picks[\"mask\"] == 1.0].groupby([\"idx_sta\", \"phase_type\"]).agg({\"residual_time\": \"mean\"}).reset_index()         )         station_term_time.set_index(\"idx_sta\", inplace=True)         stations[\"station_term_time_p\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 0][\"residual_time\"]).fillna(0)         )         stations[\"station_term_time_s\"] += (             stations[\"idx_sta\"].map(station_term_time[station_term_time[\"phase_type\"] == 1][\"residual_time\"]).fillna(0)         )          if \"event_index\" not in events.columns:             events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]         events[[\"longitude\", \"latitude\"]] = events.apply(             lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1         )         events[\"depth_km\"] = events[\"z_km\"]          picks[\"adloc_mask\"] = picks[\"mask\"]         picks[\"adloc_residual_time\"] = picks[\"residual_time\"]         picks[\"adloc_residual_amplitude\"] = picks[\"residual_amplitude\"]          picks.to_csv(os.path.join(result_path, f\"adloc_picks_sst_{iter}.csv\"), index=False)         events.to_csv(os.path.join(result_path, f\"adloc_events_sst_{iter}.csv\"), index=False)         stations.to_csv(os.path.join(result_path, f\"adloc_stations_sst_{iter}.csv\"), index=False)      # %%     if \"event_index\" not in events.columns:         events[\"event_index\"] = events.merge(picks[[\"idx_eve\", \"event_index\"]], on=\"idx_eve\")[\"event_index\"]     events[[\"longitude\", \"latitude\"]] = events.apply(         lambda x: pd.Series(proj(x[\"x_km\"], x[\"y_km\"], inverse=True)), axis=1     )     events[\"depth_km\"] = events[\"z_km\"]     events.drop([\"idx_eve\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")     events.sort_values([\"time\"], inplace=True)      picks[\"phase_type\"] = picks[\"phase_type\"].map({0: \"P\", 1: \"S\"})     picks.drop(         [\"idx_eve\", \"idx_sta\", \"mask\", \"residual_time\", \"residual_amplitude\"], axis=1, inplace=True, errors=\"ignore\"     )     picks.sort_values([\"phase_time\"], inplace=True)      stations.drop([\"idx_sta\", \"x_km\", \"y_km\", \"z_km\"], axis=1, inplace=True, errors=\"ignore\")      picks.to_csv(os.path.join(result_path, \"adloc_picks.csv\"), index=False)     events.to_csv(os.path.join(result_path, \"adloc_events.csv\"), index=False)     stations.to_csv(os.path.join(result_path, \"adloc_stations.csv\"), index=False)      return events In\u00a0[17]: Copied! <pre>adloc_catalog = run_adloc(region=region, config=config)\n</pre> adloc_catalog = run_adloc(region=region, config=config) <pre>Eikonal Solver: \nIter 0, error = 999.945\nIter 1, error = 0.000\nTime: 1.726\nEikonal Solver: \nIter 0, error = 999.906\nIter 1, error = 0.000\nTime: 0.046\nNumber of picks: 460966\nNumber of events: 29792\n</pre> <pre>Iter 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29790/29792 [07:35&lt;00:00, 61.05it/s]</pre> <pre>ADLoc locates 20645 events outof 29792 events\nusing 382008 picks outof 460966 picks\n</pre> <pre>Iter 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [07:48&lt;00:00, 63.57it/s]\nIter 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29785/29792 [07:43&lt;00:00, 74.65it/s]</pre> <pre>ADLoc locates 19909 events outof 29792 events\nusing 373396 picks outof 460966 picks\n</pre> <pre>Iter 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [07:53&lt;00:00, 62.98it/s]\nIter 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29786/29792 [07:49&lt;00:00, 65.69it/s]</pre> <pre>ADLoc locates 19767 events outof 29792 events\nusing 371331 picks outof 460966 picks\n</pre> <pre>Iter 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:01&lt;00:00, 61.91it/s]\nIter 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29788/29792 [07:49&lt;00:00, 66.29it/s]</pre> <pre>ADLoc locates 19716 events outof 29792 events\nusing 370757 picks outof 460966 picks\n</pre> <pre>Iter 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:01&lt;00:00, 61.92it/s]\nIter 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29791/29792 [07:48&lt;00:00, 55.06it/s]</pre> <pre>ADLoc locates 19686 events outof 29792 events\nusing 370487 picks outof 460966 picks\n</pre> <pre>Iter 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:00&lt;00:00, 61.96it/s]\nIter 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29789/29792 [07:49&lt;00:00, 56.13it/s]</pre> <pre>ADLoc locates 19707 events outof 29792 events\nusing 370624 picks outof 460966 picks\n</pre> <pre>Iter 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [07:59&lt;00:00, 62.13it/s]\nIter 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:02&lt;00:00, 50.82it/s]</pre> <pre>ADLoc locates 19694 events outof 29792 events\nusing 370545 picks outof 460966 picks\n</pre> <pre>Iter 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:03&lt;00:00, 61.56it/s]\nIter 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29788/29792 [07:48&lt;00:00, 67.47it/s]</pre> <pre>ADLoc locates 19688 events outof 29792 events\nusing 370437 picks outof 460966 picks\n</pre> <pre>Iter 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29792/29792 [08:01&lt;00:00, 61.89it/s]\n</pre> In\u00a0[18]: Copied! <pre>plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config)\n</pre> plot_catalog(adloc_catalog, \"ADLoc\", region=region, config=config) <p>Credit: Felix Waldhauser</p> In\u00a0[19]: Copied! <pre>def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):\n\n    data_path = f\"{region}/adloc\"\n    result_path = f\"{region}/hypodd\"\n    if not os.path.exists(f\"{root_path}/{result_path}\"):\n        os.makedirs(f\"{root_path}/{result_path}\")\n\n    ## Station Format\n    stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")\n    stations.set_index(\"station_id\", inplace=True)\n\n    shift_topo = stations[\"elevation_m\"].max() / 1e3\n    converted_hypoinverse = []\n    converted_hypodd = {}\n\n    for sta, row in stations.iterrows():\n        network_code, station_code, comp_code, channel_code = sta.split(\".\")\n        station_weight = \" \"\n        lat_degree = int(row[\"latitude\"])\n        lat_minute = (row[\"latitude\"] - lat_degree) * 60\n        north = \"N\" if lat_degree &gt;= 0 else \"S\"\n        lng_degree = int(row[\"longitude\"])\n        lng_minute = (row[\"longitude\"] - lng_degree) * 60\n        west = \"W\" if lng_degree &lt;= 0 else \"E\"\n        elevation = row[\"elevation_m\"]\n        line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"\n        converted_hypoinverse.append(line_hypoinverse)\n\n        # tmp_code = f\"{station_code}{channel_code}\"\n        tmp_code = f\"{station_code}\"\n        converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"\n\n\n    with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:\n        for k, v in converted_hypodd.items():\n            f.write(v)\n\n\n    ## Picks Format\n    picks_csv = f\"{data_path}/adloc_picks.csv\"\n    events_csv = f\"{data_path}/adloc_events.csv\"\n\n    picks = pd.read_csv(f\"{root_path}/{picks_csv}\")\n    events = pd.read_csv(f\"{root_path}/{events_csv}\")\n    picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")\n    events[\"time\"] = pd.to_datetime(events[\"time\"])\n    # events[\"magnitude\"] = 1.0\n    events[\"sigma_time\"] = 1.0\n\n    # events.sort_values(\"time\", inplace=True)\n    picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]\n\n    lines = []\n    picks_by_event = picks.groupby(\"event_index\").groups\n    for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):\n        # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")\n        event_time = event[\"time\"]\n        lat = event[\"latitude\"]\n        lng = event[\"longitude\"]\n        # dep = event[\"depth(m)\"] / 1e3 + shift_topo\n        dep = event[\"depth_km\"] + shift_topo\n        mag = event[\"magnitude\"]\n        EH = 0\n        EZ = 0\n        RMS = event[\"sigma_time\"]\n\n        year, month, day, hour, min, sec = (\n            event_time.year,\n            event_time.month,\n            event_time.day,\n            event_time.hour,\n            event_time.minute,\n            float(event_time.strftime(\"%S.%f\")),\n        )\n        event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"\n\n        lines.append(event_line)\n\n        picks_idx = picks_by_event[event[\"event_index\"]]\n        for j in picks_idx:\n            # pick = picks.iloc[j]\n            pick = picks.loc[j]\n            network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")\n            phase_type = pick[\"phase_type\"].upper()\n            phase_score = pick[\"phase_score\"]\n            # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()\n            pick_time = (pick[\"phase_time\"] - event_time).total_seconds()\n            tmp_code = f\"{station_code}\"\n            pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"\n            lines.append(pick_line)\n\n    with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:\n        fp.writelines(lines)\n\n    ## Run Hypodd\n    print(f\"Running Hypodd:\")\n    os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")\n\n    ## Read  catalog\n    columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]\n    catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(\n        lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',\n        axis=1,\n    )\n    catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))\n    catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]\n    catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)\n    catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)\n    catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)\n\n    return catalog_ct_hypodd\n</pre> def run_hypodd(root_path: str = \"local\", region: str = \"demo\"):      data_path = f\"{region}/adloc\"     result_path = f\"{region}/hypodd\"     if not os.path.exists(f\"{root_path}/{result_path}\"):         os.makedirs(f\"{root_path}/{result_path}\")      ## Station Format     stations = pd.read_csv(f\"{root_path}/{data_path}/adloc_stations.csv\")     stations.set_index(\"station_id\", inplace=True)      shift_topo = stations[\"elevation_m\"].max() / 1e3     converted_hypoinverse = []     converted_hypodd = {}      for sta, row in stations.iterrows():         network_code, station_code, comp_code, channel_code = sta.split(\".\")         station_weight = \" \"         lat_degree = int(row[\"latitude\"])         lat_minute = (row[\"latitude\"] - lat_degree) * 60         north = \"N\" if lat_degree &gt;= 0 else \"S\"         lng_degree = int(row[\"longitude\"])         lng_minute = (row[\"longitude\"] - lng_degree) * 60         west = \"W\" if lng_degree &lt;= 0 else \"E\"         elevation = row[\"elevation_m\"]         line_hypoinverse = f\"{station_code:&lt;5} {network_code:&lt;2} {comp_code[:-1]:&lt;1}{channel_code:&lt;3} {station_weight}{abs(lat_degree):2.0f} {abs(lat_minute):7.4f}{north}{abs(lng_degree):3.0f} {abs(lng_minute):7.4f}{west}{elevation:4.0f}\\n\"         converted_hypoinverse.append(line_hypoinverse)          # tmp_code = f\"{station_code}{channel_code}\"         tmp_code = f\"{station_code}\"         converted_hypodd[tmp_code] = f\"{tmp_code:&lt;8s} {row['latitude']:.3f} {row['longitude']:.3f}\\n\"       with open(f\"{root_path}/{result_path}/stations.dat\", \"w\") as f:         for k, v in converted_hypodd.items():             f.write(v)       ## Picks Format     picks_csv = f\"{data_path}/adloc_picks.csv\"     events_csv = f\"{data_path}/adloc_events.csv\"      picks = pd.read_csv(f\"{root_path}/{picks_csv}\")     events = pd.read_csv(f\"{root_path}/{events_csv}\")     picks[\"phase_time\"] = pd.to_datetime(picks[\"phase_time\"], format=\"mixed\")     events[\"time\"] = pd.to_datetime(events[\"time\"])     # events[\"magnitude\"] = 1.0     events[\"sigma_time\"] = 1.0      # events.sort_values(\"time\", inplace=True)     picks = picks.loc[picks[\"event_index\"].isin(events[\"event_index\"])]      lines = []     picks_by_event = picks.groupby(\"event_index\").groups     for i, event in tqdm(events.iterrows(), desc=\"Convert catalog\", total=len(events)):         # event_time = datetime.strptime(event[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f\")         event_time = event[\"time\"]         lat = event[\"latitude\"]         lng = event[\"longitude\"]         # dep = event[\"depth(m)\"] / 1e3 + shift_topo         dep = event[\"depth_km\"] + shift_topo         mag = event[\"magnitude\"]         EH = 0         EZ = 0         RMS = event[\"sigma_time\"]          year, month, day, hour, min, sec = (             event_time.year,             event_time.month,             event_time.day,             event_time.hour,             event_time.minute,             float(event_time.strftime(\"%S.%f\")),         )         event_line = f\"# {year:4d} {month:2d} {day:2d} {hour:2d} {min:2d} {sec:5.2f}  {lat:7.4f} {lng:9.4f}   {dep:5.2f} {mag:5.2f} {EH:5.2f} {EZ:5.2f} {RMS:5.2f} {event['event_index']:9d}\\n\"          lines.append(event_line)          picks_idx = picks_by_event[event[\"event_index\"]]         for j in picks_idx:             # pick = picks.iloc[j]             pick = picks.loc[j]             network_code, station_code, comp_code, channel_code = pick[\"station_id\"].split(\".\")             phase_type = pick[\"phase_type\"].upper()             phase_score = pick[\"phase_score\"]             # pick_time = (datetime.strptime(pick[\"phase_time\"], \"%Y-%m-%dT%H:%M:%S.%f\") - event_time).total_seconds()             pick_time = (pick[\"phase_time\"] - event_time).total_seconds()             tmp_code = f\"{station_code}\"             pick_line = f\"{tmp_code:&lt;7s}   {pick_time:6.3f}   {phase_score:5.4f}   {phase_type}\\n\"             lines.append(pick_line)      with open(f\"{root_path}/{result_path}/phase.txt\", \"w\") as fp:         fp.writelines(lines)      ## Run Hypodd     print(f\"Running Hypodd:\")     os.system(f\"bash run_hypodd_ct.sh {root_path} {region}\")      ## Read  catalog     columns = [\"ID\", \"LAT\", \"LON\", \"DEPTH\", \"X\", \"Y\", \"Z\", \"EX\", \"EY\", \"EZ\", \"YR\", \"MO\", \"DY\", \"HR\", \"MI\", \"SC\", \"MAG\", \"NCCP\", \"NCCS\", \"NCTP\", \"NCTS\", \"RCC\", \"RCT\", \"CID\"]     catalog_ct_hypodd = pd.read_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.reloc\", sep=\"\\s+\", header=None, names=columns, dtype=float)     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd.apply(         lambda x: f'{x[\"YR\"]:04.0f}-{x[\"MO\"]:02.0f}-{x[\"DY\"]:02.0f}T{x[\"HR\"]:02.0f}:{x[\"MI\"]:02.0f}:{np.min([float(x[\"SC\"]), 59.999]):05.3f}',         axis=1,     )     catalog_ct_hypodd[\"time\"] = catalog_ct_hypodd[\"time\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f\"))     catalog_ct_hypodd = catalog_ct_hypodd[catalog_ct_hypodd[\"DEPTH\"] != \"*********\"]     catalog_ct_hypodd[\"DEPTH\"] = catalog_ct_hypodd[\"DEPTH\"].astype(float)     catalog_ct_hypodd.rename({\"ID\": \"event_index\", \"LAT\": \"latitude\", \"LON\": \"longitude\", \"DEPTH\": \"depth_km\", \"MAG\": \"magnitude\"}, axis=1, inplace=True)     catalog_ct_hypodd.to_csv(f\"{root_path}/{region}/hypodd/hypodd_ct.csv\", index=False)      return catalog_ct_hypodd In\u00a0[20]: Copied! <pre>hypodd_catalog = run_hypodd(region=region)\n</pre> hypodd_catalog = run_hypodd(region=region) <pre>Convert catalog: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19688/19688 [00:32&lt;00:00, 605.67it/s]\n+ WORKING_DIR=/workspaces/Earthquake_Catalog_Workshop/notebooks\n+ '[' 2 -eq 2 ']'\n+ root_path=local\n+ region=ridgecrest\n+ data_path=local/ridgecrest/hypodd\n+ '[' '!' -d local/ridgecrest/hypodd ']'\n+ cd local/ridgecrest/hypodd\n+ '[' '!' -d HypoDD ']'\n+ git clone https://github.com/zhuwq0/HypoDD.git\nCloning into 'HypoDD'...\n</pre> <pre>Running Hypodd:\n</pre> <pre>+ export PATH=/workspaces/Earthquake_Catalog_Workshop/.conda/quakeflow/bin:/opt/conda/condabin:/vscode/bin/linux-x64/4437686ffebaf200fa4a6e6e67f735f3edf24ada/bin/remote-cli:/home/codespace/.local/bin:/home/codespace/.dotnet:/home/codespace/nvm/current/bin:/home/codespace/.php/current/bin:/home/codespace/.python/current/bin:/home/codespace/java/current/bin:/home/codespace/.ruby/current/bin:/home/codespace/.local/bin:/usr/local/python/current/bin:/usr/local/py-utils/bin:/usr/local/jupyter:/usr/local/oryx:/usr/local/go/bin:/go/bin:/usr/local/sdkman/bin:/usr/local/sdkman/candidates/java/current/bin:/usr/local/sdkman/candidates/gradle/current/bin:/usr/local/sdkman/candidates/maven/current/bin:/usr/local/sdkman/candidates/ant/current/bin:/usr/local/rvm/gems/default/bin:/usr/local/rvm/gems/default@global/bin:/usr/local/rvm/rubies/default/bin:/usr/local/share/rbenv/bin:/usr/local/php/current/bin:/opt/conda/bin:/usr/local/nvs:/usr/local/share/nvm/current/bin:/usr/local/hugo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/share/dotnet:/home/codespace/.dotnet/tools:/usr/local/rvm/bin:/workspaces/Earthquake_Catalog_Workshop/notebooks/local/ridgecrest/hypodd/HypoDD\n+ PATH=/workspaces/Earthquake_Catalog_Workshop/.conda/quakeflow/bin:/opt/conda/condabin:/vscode/bin/linux-x64/4437686ffebaf200fa4a6e6e67f735f3edf24ada/bin/remote-cli:/home/codespace/.local/bin:/home/codespace/.dotnet:/home/codespace/nvm/current/bin:/home/codespace/.php/current/bin:/home/codespace/.python/current/bin:/home/codespace/java/current/bin:/home/codespace/.ruby/current/bin:/home/codespace/.local/bin:/usr/local/python/current/bin:/usr/local/py-utils/bin:/usr/local/jupyter:/usr/local/oryx:/usr/local/go/bin:/go/bin:/usr/local/sdkman/bin:/usr/local/sdkman/candidates/java/current/bin:/usr/local/sdkman/candidates/gradle/current/bin:/usr/local/sdkman/candidates/maven/current/bin:/usr/local/sdkman/candidates/ant/current/bin:/usr/local/rvm/gems/default/bin:/usr/local/rvm/gems/default@global/bin:/usr/local/rvm/rubies/default/bin:/usr/local/share/rbenv/bin:/usr/local/php/current/bin:/opt/conda/bin:/usr/local/nvs:/usr/local/share/nvm/current/bin:/usr/local/hugo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/share/dotnet:/home/codespace/.dotnet/tools:/usr/local/rvm/bin:/workspaces/Earthquake_Catalog_Workshop/notebooks/local/ridgecrest/hypodd/HypoDD\n+ make -C HypoDD/src/\n+ cat\n+ cat\n+ ./HypoDD/src/ph2dt/ph2dt ph2dt.inp\n</pre> <pre>starting ph2dt (v2.1b - 08/2012)...     Mon Apr  7 09:35:26 2025\u0000\n\nreading data ...\n &gt; stations =           15\n &gt; events total =        19688\n &gt; events selected =        19077\n &gt; phases =       377343\nforming dtimes...\n &gt; stations selected =           14\n &gt; P-phase pairs total =      6076169\n &gt; S-phase pairs total =      6442551\n &gt; outliers =       801928  (           6 %)\n &gt; phases at stations not in station list =            0\n &gt; phases at distances larger than MAXDIST =            0\n &gt; P-phase pairs selected =      4235138  (          69 %)\n &gt; S-phase pairs selected =      4611709  (          71 %)\n &gt; weakly linked events =          634  (           3 %)\n &gt; linked event pairs =       616150\n &gt; average links per pair =           14\n &gt; average offset (km) betw. linked events =    1.85464251    \n &gt; average offset (km) betw. strongly linked events =    1.85464251    \n &gt; maximum offset (km) betw. strongly linked events =    9.99988556    \n\nDone.  Mon Apr  7 09:36:57 2025\u0000\n\nOutput files: dt.ct; event.dat; event.sel; station.sel; ph2dt.log\nph2dt parameters were: \n(minwght,maxdist,maxsep,maxngh,minlnk,minobs,maxobs)\n 0.00  200.000   10.000  50   8   8 100\nstarting hypoDD (v2.1beta - 06/15/2016)...   Mon Apr  7 09:36:57 2025\u0003\nINPUT FILES:\ncross dtime data:  \ncatalog dtime data: dt.ct\nevents: event.sel\nstations: stations.dat\nOUTPUT FILES:\ninitial locations: hypodd_ct.loc\nrelocated events: hypodd_ct.reloc\nevent pair residuals: hypodd.res\nstation residuals: hypodd.sta\nsource parameters: hypodd.src\n Relocate all clusters\n Relocate all events\nUse local layered 1D model.\nReading data ...   Mon Apr  7 09:36:57 2025\u0000\n# events = 19077\n# stations &lt; maxdist =     15\n# stations w/ neg. elevation (set to 0) =    0\n</pre> <pre>+ ./HypoDD/src/hypoDD/hypoDD ct.inp\n</pre> <pre># catalog P dtimes = 4235138\n# catalog S dtimes = 4611709\n# dtimes total =  8846847\n# events after dtime match =      18945\n# stations =     14\n\nno clustering performed.\n\nRELOCATION OF CLUSTER: 1     Mon Apr  7 09:37:18 2025\u0003\n----------------------\nInitial trial sources = 18945\n1D ray tracing.\n\n  IT   EV  CT    RMSCT   RMSST   DX   DY   DZ   DT   OS  AQ  CND\n        %   %   ms     %    ms    m    m    m   ms    m \n 1    100  99  145 -13.8     0  388  405 1945   95    0 612 1809\n 2     97  96  141  -2.8     0  364  386 1454   88    0  13 1851\n 3     97  95  139  -1.8     0  364  387 1447   88    0   2 1875\n 4  1  97  95  138  -0.1   265  364  386 1444   88  271   0 1880\n 5     97  93  109 -21.6   265  360  234*****   51  271 196 2007\n 6     96  92  105  -2.9   265  349  229*****   50  271   8 1963\n 7     95  92  104  -0.8   265  349  230*****   50  271   2 1933\n 8  2  95  92  104  -0.1   210  349  230*****   50*****   0 1929\n 9     95  90   90 -13.3   210  248  169  690   35***** 129 2073\n10     95  89   89  -2.0   210  245  168  574   35*****   3 2078\n11     95  89   88  -0.6   210  245  168  574   35*****   2 2079\n12  3  95  89   88  -0.1   182  245  168  574   35 1044   0 2087\n13     95  88   81  -7.6   182  172  118  457   24 1044 112 2297\n14     94  87   80  -1.1   182  168  115  370   23 1044   3 2276\n15  4  94  87   80  -0.3   168  167  116  371   23 1291   0 2261\n16     91  75   66 -17.1   168  119  12187372   20 1291  91 1935\n17     91  74   63  -5.1   168  115  11777569   20 1291   3 1867\n18     91  74   63  -0.7   168  115  11776165   20 1291   1 1844\n19  5  91  74   63  -0.1   125  115  11775985   2074259   0 1847\n20     90  72   58  -6.8   125   77   75  227   1174259  54 1804\n21     90  71   57  -1.4   125   75   74  179   1174259   3 1766\n22  6  90  71   57  -0.2   113   75   74  178   11 1524   0 1784\n23     89  70   55  -3.5   113   57   58  184    7 1524  35 1695\n24  7  89  70   55  -0.8   108   56   56  135    7 1580   0 1651\n25     89  69   54  -2.1   108   45   45  261    5 1580  23 1593\n26  8  89  69   53  -0.5   105   45   44  117    5 1592   0 1587\n27     85  49   44 -18.3   105   56   58  180   10 1592  24 1023\n28  9  85  48   41  -7.2    67   55   57  168   10 1568   0 1019\n29     84  46   37  -8.7    67   34   35  123    6 1568  12 1002\n30     84  45   36  -2.3    67   34   36  110    6 1568   1 1004\n31 10  84  45   36  -0.6    61   34   35  110    6 1572   0 1004\n32     83  44   34  -4.4    61   25   26  100    4 1572  16  961\n33 11  83  44   34  -1.4    58   25   26   88    4 1562   0  960\n34     83  43   33  -3.1    58   19   2018928    3 1562   8  927\n35 12  83  43   32  -1.1    56   19   2018926    317300   0  924\n36     82  40   26 -18.5    56   21   21   72    417300  13  938\n37 13  82  39   25  -6.5    43   23   22   66    4 1560   0  922\n38     82  37   23  -7.7    43   18   17   56    3 1560   9  893\n39 14  82  37   22  -3.0    39   19   18   54    3 1551   0  889\n40     82  36   21  -5.5    39   16   15   52    3 1551   9  891\n41 15  82  35   20  -2.4    36   16   15   47    3 1541   0  877\n42     82  34   19  -4.5    36   13   12   40    2 1541   6  870\n43 16  82  34   19  -2.1    34   13   12   38    2 1539   0  869\n\nwriting out results ...\n   Program hypoDD finished. Mon Apr  7 10:30:20 2025\u0003\n</pre> <pre>+ cd /workspaces/Earthquake_Catalog_Workshop/notebooks\n</pre> In\u00a0[21]: Copied! <pre>plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)\n</pre> plot_catalog(hypodd_catalog, \"HypoDD\", region=region, config=config)"},{"location":"notebooks/quakeflow_ridgecrest/#earthquake-catalog-workshop","title":"Earthquake Catalog Workshop\u00b6","text":"<p>Contributors: Eric Beauce, Gabrielle Tepp, Clara Yoon, Ellen Yu, Weiqiang Zhu (alphabetical order)</p>"},{"location":"notebooks/quakeflow_ridgecrest/#machine-learning-part","title":"Machine Learning Part\u00b6","text":"<ol> <li><p>Download data using Obpsy and NCEDC/SCEDC AWS Public Dataset</p> <p>FDSN web service client for ObsPy</p> <p>NCEDC AWS Public Dataset</p> <p>SCEDC AWS Public Dataset</p> <p>Event Dataset (CEED)</p> <p>CEED paper</p> </li> <li><p>PhaseNet for P/S phase picking</p> <p>PhaseNet github page</p> <p>PhaseNet paper</p> </li> <li><p>GaMMA for phase association</p> <p>GaMMA github page</p> <p>GaMMA paper</p> </li> <li><p>ADLoc for earthquake location</p> <p>ADLoc github page</p> <p>ADLoc paper</p> </li> <li><p>HypoDD for earthquake relocation</p> <p>HypoDD github page</p> <p>HypoDD paper</p> </li> <li><p>QuakeFlow</p> <p>QuakeFlow github page</p> <p>QuakeFlow paper</p> </li> </ol>"},{"location":"notebooks/quakeflow_ridgecrest/#setup-environment","title":"Setup Environment\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#setup-configurations","title":"Setup configurations\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#download-the-standard-catalog-for-comparison","title":"Download the standard catalog for comparison\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#download-stations","title":"Download stations\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#download-waveform-data","title":"Download waveform data\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#run-phasenet-to-pick-ps-picks","title":"Run PhaseNet to pick P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#run-gamma-to-associate-ps-picks","title":"Run GaMMA to associate P/S picks\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#run-adloc-to-locate-absolute-earthquake-locations","title":"Run ADLoc to locate absolute earthquake locations\u00b6","text":""},{"location":"notebooks/quakeflow_ridgecrest/#run-hypodd-to-relocate-relative-earthquake-locations","title":"Run HypoDD to relocate relative earthquake locations\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/","title":"Notebook3","text":"In\u00a0[1]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[2]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[3]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\nstation_meta\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) station_meta Out[3]: network station location instrument component latitude longitude elevation_m depth_km provider station_id station_term_time_p station_term_time_s station_term_amplitude 0 CI CCC NaN BH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..BH 0.266086 0.500831 0.049399 1 CI CCC NaN HH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HH 0.295428 0.518465 0.191475 2 CI CCC NaN HN ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HN 0.296263 0.541148 0.064485 3 CI CLC NaN BH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..BH -0.231963 -0.415271 -0.331371 4 CI CLC NaN HH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HH -0.168743 -0.390045 -0.140313 5 CI CLC NaN HN ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HN -0.175671 -0.388116 -0.249066 6 CI DTP NaN BH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..BH -0.305881 -0.602459 -0.503411 7 CI DTP NaN HH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HH -0.263705 -0.564867 -0.437951 8 CI DTP NaN HN ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HN -0.244383 -0.538990 -0.500516 9 CI JRC2 NaN BH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..BH 0.011361 -0.080285 -0.039941 10 CI JRC2 NaN HH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HH 0.053539 -0.052748 0.068213 11 CI JRC2 NaN HN ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HN 0.059764 -0.045991 -0.007637 12 CI LRL NaN BH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..BH -0.295604 -0.540857 0.033788 13 CI LRL NaN HH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HH -0.268381 -0.513955 0.146876 14 CI LRL NaN HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HN -0.266329 -0.503088 0.045499 15 CI LRL 2C HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL.2C.HN 0.000000 0.000000 0.000000 16 CI MPM NaN BH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..BH -0.011825 -0.098089 -0.518793 17 CI MPM NaN HH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HH 0.009095 -0.081896 -0.459349 18 CI MPM NaN HN ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HN 0.011870 -0.052277 -0.532764 19 CI Q0072 01 HN ENZ 35.609617 -117.666721 695.0 -0.6950 SCEDC CI.Q0072.01.HN 0.000000 0.000000 0.000000 20 CI SLA NaN BH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..BH 0.066893 0.118500 -0.081634 21 CI SLA NaN HH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HH 0.089833 0.128589 -0.042928 22 CI SLA NaN HN ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HN 0.093526 0.168238 -0.150381 23 CI SRT NaN BH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..BH 0.148171 0.653411 -0.253527 24 CI SRT NaN HH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HH 0.183868 0.641707 -0.208859 25 CI SRT NaN HN ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HN 0.175475 0.674587 -0.295869 26 CI TOW2 NaN BH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..BH 0.268083 0.816279 0.028038 27 CI TOW2 NaN HH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HH 0.285970 0.831533 0.102681 28 CI TOW2 NaN HN ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HN 0.285113 0.843886 -0.006698 29 CI WBM NaN BH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..BH 0.136927 0.128744 -0.166279 30 CI WBM NaN HH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HH 0.152299 0.124250 -0.138614 31 CI WBM NaN HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HN 0.170719 0.179461 -0.101436 32 CI WBM 2C HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM.2C.HN 0.000000 0.000000 0.000000 33 CI WCS2 NaN BH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..BH 0.030349 -0.126935 0.023642 34 CI WCS2 NaN HH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HH 0.065321 -0.099447 0.146628 35 CI WCS2 NaN HN ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HN 0.061651 -0.096450 0.037563 36 CI WMF NaN BH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..BH 0.039416 -0.005761 -0.165183 37 CI WMF NaN HH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HH 0.075658 0.005327 -0.079900 38 CI WMF NaN HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HN 0.085427 0.025273 -0.240971 39 CI WMF 2C HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF.2C.HN 0.000000 0.000000 0.000000 40 CI WNM NaN EH Z 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..EH -0.026889 -0.070269 -0.332510 41 CI WNM NaN HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..HN -0.011659 -0.118223 -0.038719 42 CI WNM 2C HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM.2C.HN 0.000000 0.000000 0.000000 43 CI WRC2 NaN BH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..BH -0.023860 -0.043523 0.117833 44 CI WRC2 NaN HH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HH 0.010633 -0.029488 0.165234 45 CI WRC2 NaN HN ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HN 0.014658 -0.021367 0.103905 46 CI WRV2 NaN EH Z 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..EH -0.003461 -0.154572 -0.355199 47 CI WRV2 NaN HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..HN 0.017317 -0.137916 -0.273270 48 CI WRV2 2C HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2.2C.HN 0.000000 0.000000 0.000000 49 CI WVP2 NaN EH Z 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..EH 0.020325 0.008989 -0.341606 50 CI WVP2 NaN HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..HN 0.024742 -0.103024 -0.089014 51 CI WVP2 2C HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2.2C.HN 0.000000 0.000000 0.000000 <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[4]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[4]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[5]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\nevent_meta\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) event_meta Out[5]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 20898 2019-07-09 23:58:14.298499048 0.933013 0.097816 67 1.543257 0.131523 1907 -117.711811 35.926468 6.652020 20899 2019-07-09 23:58:47.701746285 0.882434 0.090185 73 1.087089 0.140159 2051 -117.604263 35.797450 6.617413 20900 2019-07-09 23:59:05.102247662 0.798047 0.435077 26 1.147040 0.170994 12567 -117.509729 35.692722 12.815041 20901 2019-07-09 23:59:40.257837813 0.971081 0.065523 35 1.161323 0.068586 7726 -117.846289 36.061435 5.224666 20902 2019-07-09 23:59:49.650466544 0.800524 0.023674 15 0.936622 0.095959 18566 -117.896100 36.095886 6.761860 <p>20903 rows \u00d7 10 columns</p> In\u00a0[6]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\npicks\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) picks Out[6]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 0 CI.WMF..BH 280874 2019-07-04 00:46:48.759 0.594 P 0.01 0.110 -3.213107 6720 0.955091 3.124152e-07 1 0.003919 0.012320 1 CI.WMF..HH 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.938 -3.077638 6720 0.948896 3.368390e-07 1 0.026491 0.063669 2 CI.WMF..HN 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.898 -3.128019 6720 1.598008 2.146277e-07 1 0.017580 0.173702 3 CI.WRV2..EH 280945 2019-07-04 00:46:49.450 0.977 P 0.01 -0.855 -3.464453 6720 1.000000 3.298200e-07 1 0.077194 0.244381 4 CI.WRV2..HN 280945 2019-07-04 00:46:49.450 0.941 P 0.01 -0.906 -3.585863 6720 1.147235 3.908305e-07 1 0.056694 0.041691 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 955700 CI.WRV2..HN 8639355 2019-07-09 23:59:53.550 0.688 S 0.01 -0.022 -3.384471 18566 0.644492 1.973834e-06 1 0.019093 0.028409 955701 CI.JRC2..HH 8639499 2019-07-09 23:59:54.998 0.402 S 0.01 -0.023 -3.446238 18566 0.647843 6.197256e-06 1 0.020364 -0.154404 955702 CI.JRC2..HN 8639499 2019-07-09 23:59:54.998 0.344 S 0.01 -0.016 -3.531800 18566 0.731796 4.075353e-06 1 0.012773 -0.164659 955703 CI.WVP2..EH 8639574 2019-07-09 23:59:55.740 0.314 S 0.01 -0.103 -3.488117 18566 0.636005 2.815148e-06 1 -0.092233 0.316712 955704 CI.WVP2..HN 8639576 2019-07-09 23:59:55.760 0.695 S 0.01 0.076 -3.218676 18566 0.825223 4.657352e-06 1 0.039319 0.332722 <p>955705 rows \u00d7 14 columns</p> In\u00a0[7]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n    \n</pre> def fetch_event_waveforms(     event_picks,     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream      In\u00a0[8]: Copied! <pre># explore event_meta to find a nice intermediate-size earthquake we could plot\nevent_meta.head(20)\n</pre> # explore event_meta to find a nice intermediate-size earthquake we could plot event_meta.head(20) Out[8]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 5 2019-07-04 03:20:28.674438914 0.755451 0.050436 44 0.869927 0.111885 6929 -117.673684 36.114308 5.894466 6 2019-07-04 04:03:01.619369274 0.897311 0.143631 32 0.386310 0.214381 7607 -117.805077 36.016063 0.396071 7 2019-07-04 05:16:47.223353119 0.835094 0.053522 22 0.575433 0.163675 13098 -117.879256 36.090409 4.268292 8 2019-07-04 06:57:32.758991812 0.922386 0.065846 23 0.256281 0.065212 15698 -117.867587 36.081665 5.939931 9 2019-07-04 11:51:07.805591259 0.692042 0.075023 48 0.818316 0.102087 4380 -117.671909 36.118573 6.085804 10 2019-07-04 15:36:04.228420696 0.652888 0.012137 9 -0.633904 0.167058 27370 -117.785299 36.005578 3.234981 11 2019-07-04 15:42:47.932558745 0.597856 0.072740 28 0.933088 0.151623 9740 -117.501116 35.707382 14.496446 12 2019-07-04 16:07:20.003321194 0.729246 0.093399 33 0.835854 0.133106 9365 -117.491503 35.711241 13.846898 13 2019-07-04 16:11:46.920083440 0.711579 0.516367 18 1.734816 0.117952 7098 -117.877675 35.196445 31.000000 14 2019-07-04 16:13:43.094792540 0.849673 0.070048 83 1.653859 0.111108 2305 -117.493716 35.710090 13.375377 15 2019-07-04 16:16:07.085486307 0.814365 0.040161 10 0.587583 0.099619 26520 -117.540198 35.689022 14.537368 16 2019-07-04 17:02:55.057058245 0.770696 0.079675 90 4.476724 0.143489 1101 -117.495094 35.711607 13.586411 17 2019-07-04 17:04:02.231614981 0.664676 0.064038 41 2.062226 0.186214 5379 -117.488446 35.711139 14.001705 18 2019-07-04 17:05:05.071421677 0.509088 0.089882 29 1.471434 0.157048 12115 -117.491307 35.710881 13.223374 19 2019-07-04 17:08:51.664841725 0.666790 0.046444 15 0.657653 0.175270 23044 -117.504506 35.706123 14.570921 In\u00a0[9]: Copied! <pre># feel free to play with the event index to plot different events\nEVENT_IDX = 1101\n\nevent_meta.set_index(\"event_index\").loc[EVENT_IDX]\n</pre> # feel free to play with the event index to plot different events EVENT_IDX = 1101  event_meta.set_index(\"event_index\").loc[EVENT_IDX] Out[9]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[10]: Copied! <pre># fetch the corresponding picks for this event\nevent_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nevent_picks\n</pre> # fetch the corresponding picks for this event event_picks = picks[picks[\"event_index\"] == EVENT_IDX] event_picks Out[10]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[11]: Copied! <pre># fetch the waveforms\nevent_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.)\nprint(event_waveforms.__str__(extended=True))\n</pre> # fetch the waveforms event_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.) print(event_waveforms.__str__(extended=True)) <pre>42 Trace(s) in Stream:\nCI.CLC..HHZ  | 2019-07-04T17:02:48.478300Z - 2019-07-04T17:03:18.478300Z | 25.0 Hz, 751 samples\nCI.SRT..HHZ  | 2019-07-04T17:02:49.918300Z - 2019-07-04T17:03:19.918300Z | 25.0 Hz, 751 samples\nCI.CCC..HHZ  | 2019-07-04T17:02:50.118300Z - 2019-07-04T17:03:20.118300Z | 25.0 Hz, 751 samples\nCI.SLA..HHZ  | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHZ | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.LRL..HHZ  | 2019-07-04T17:02:50.638300Z - 2019-07-04T17:03:20.638300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHZ | 2019-07-04T17:02:50.718300Z - 2019-07-04T17:03:20.718300Z | 25.0 Hz, 751 samples\nCI.CLC..HHN  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.CLC..HHE  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.MPM..HHZ  | 2019-07-04T17:02:52.038300Z - 2019-07-04T17:03:22.038300Z | 25.0 Hz, 751 samples\nCI.WBM..HHZ  | 2019-07-04T17:02:52.123100Z - 2019-07-04T17:03:22.123100Z | 25.0 Hz, 751 samples\nCI.WVP2..EHZ | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.WNM..EHZ  | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.JRC2..HHZ | 2019-07-04T17:02:52.558300Z - 2019-07-04T17:03:22.558300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHZ | 2019-07-04T17:02:52.598300Z - 2019-07-04T17:03:22.598300Z | 25.0 Hz, 751 samples\nCI.WRV2..EHZ | 2019-07-04T17:02:53.600000Z - 2019-07-04T17:03:23.600000Z | 25.0 Hz, 751 samples\nCI.SRT..HHN  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.SRT..HHE  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.CCC..HHN  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.CCC..HHE  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.SLA..HHE  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.SLA..HHN  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.LRL..HHN  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.LRL..HHE  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.WMF..HHZ  | 2019-07-04T17:02:54.798300Z - 2019-07-04T17:03:24.798300Z | 25.0 Hz, 751 samples\nCI.DTP..HHZ  | 2019-07-04T17:02:54.958300Z - 2019-07-04T17:03:24.958300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WBM..HHE  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.WBM..HHN  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.MPM..HHE  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.MPM..HHN  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHN | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHE | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHE | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHN | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WMF..HHN  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.WMF..HHE  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.DTP..HHN  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\nCI.DTP..HHE  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\n</pre> In\u00a0[12]: Copied! <pre># plot them!\nfig = event_waveforms.select(component=\"Z\").plot(equal_scale=False)\n</pre> # plot them! fig = event_waveforms.select(component=\"Z\").plot(equal_scale=False) In\u00a0[13]: Copied! <pre>selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX]\nselected_event_meta\n</pre> selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX] selected_event_meta Out[13]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[14]: Copied! <pre>selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nselected_event_picks\n</pre> selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX] selected_event_picks Out[14]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[15]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[16]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[17]: Copied! <pre>GAP_START_SEC = 3. * 60. * 60. \nGAP_END_SEC = 14. * 60. * 60.\nstation_list = list(set([st.stats.station for st in continuous_seismograms]))\nSTATIONS_W_GAP = np.random.choice(\n    station_list, size=int(3. / 4. * len(station_list)), replace=False\n)\nfor sta in STATIONS_W_GAP:\n    for tr in continuous_seismograms.select(station=sta):\n        gap_start_samp = int(GAP_START_SEC * tr.stats.sampling_rate)\n        gap_end_samp = int(GAP_END_SEC * tr.stats.sampling_rate)\n        tr.data[gap_start_samp:gap_end_samp] = 0.\n</pre> GAP_START_SEC = 3. * 60. * 60.  GAP_END_SEC = 14. * 60. * 60. station_list = list(set([st.stats.station for st in continuous_seismograms])) STATIONS_W_GAP = np.random.choice(     station_list, size=int(3. / 4. * len(station_list)), replace=False ) for sta in STATIONS_W_GAP:     for tr in continuous_seismograms.select(station=sta):         gap_start_samp = int(GAP_START_SEC * tr.stats.sampling_rate)         gap_end_samp = int(GAP_END_SEC * tr.stats.sampling_rate)         tr.data[gap_start_samp:gap_end_samp] = 0. In\u00a0[18]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[19]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[19]: <pre>array([[[-9.8226795e-11,  5.7554624e-11,  3.4072033e-11, ...,\n         -1.8828223e-09,  8.1136459e-10, -1.7122942e-10],\n        [-1.7923078e-10,  9.7317022e-11,  3.3442499e-10, ...,\n          7.6900075e-10,  2.7582496e-09, -1.1529546e-09],\n        [-1.6065722e-10, -2.5988860e-11, -8.8432366e-11, ...,\n          6.2743988e-10,  3.0330730e-10, -1.6875973e-10]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 1.8492177e-11,  7.3153331e-11,  1.3191939e-10, ...,\n          4.4108817e-10, -3.0806005e-10, -4.0270010e-10]],\n\n       [[-1.6893502e-11,  8.4201196e-12,  2.7799072e-11, ...,\n         -1.7474208e-10, -4.5014548e-11, -2.6048433e-10],\n        [ 1.4196522e-11,  3.8935088e-12, -2.4043532e-12, ...,\n         -4.9603099e-11, -9.2666923e-11,  2.2229388e-10],\n        [-7.8133723e-12,  1.1610854e-11,  1.2903780e-11, ...,\n         -8.2966682e-11, -9.3903114e-11, -8.1583212e-12]],\n\n       ...,\n\n       [[ 5.4725401e-11,  7.0578925e-11, -6.2566233e-12, ...,\n         -3.6589140e-10, -7.8155921e-10, -6.3879835e-10],\n        [-1.3925512e-11,  2.0290504e-11,  1.0709553e-11, ...,\n          1.0249724e-09,  7.7783147e-10, -4.5876902e-10],\n        [ 1.9426023e-10, -3.3486833e-10, -1.7510682e-10, ...,\n          9.0171864e-10, -9.0758560e-11, -1.7541314e-10]],\n\n       [[-6.0105490e-11, -1.9033368e-09, -2.2194966e-09, ...,\n          9.9305908e-10, -2.5555899e-10, -1.2649487e-09],\n        [ 3.3998053e-09, -9.0392399e-10, -4.9158380e-09, ...,\n         -5.5282345e-10, -3.0974114e-09, -1.6373071e-09],\n        [-6.3921446e-10, -1.2163219e-09, -1.2187444e-09, ...,\n         -1.0883094e-09,  2.2945404e-10,  4.6940152e-10]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [-2.3801865e-11,  4.4190979e-12, -4.3637774e-11, ...,\n         -4.7749610e-11, -1.3966529e-10, -1.5340888e-10]]], dtype=float32)</pre> In\u00a0[20]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  In\u00a0[21]: Copied! <pre># add station_code columns to `selected_event_picks`\nselected_event_picks.set_index(\"station_id\", inplace=True)\nfor staid in selected_event_picks.index:\n    station_code = staid.split(\".\")[1]\n    selected_event_picks.loc[staid, \"station_code\"] = station_code\nselected_event_picks\n</pre> # add station_code columns to `selected_event_picks` selected_event_picks.set_index(\"station_id\", inplace=True) for staid in selected_event_picks.index:     station_code = staid.split(\".\")[1]     selected_event_picks.loc[staid, \"station_code\"] = station_code selected_event_picks <pre>/tmp/ipykernel_2902838/1881751651.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  selected_event_picks.loc[staid, \"station_code\"] = station_code\n</pre> Out[21]: phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude station_code station_id CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 CLC CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 CLC CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 CLC CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 SRT CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 SRT ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 WMF CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 WMF CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 DTP CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 DTP CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 DTP <p>90 rows \u00d7 14 columns</p> <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[22]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\ntau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        phase_type = PHASE_ON_COMP[cp]\n        picks_s_c = selected_event_picks[\n            (\n                (selected_event_picks[\"station_code\"] == sta)\n                &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n            )\n        ]\n        if len(picks_s_c) == 0:\n            # no pick for this station/component: set to -999\n            tau_s_c_sec[s, c] = -999\n        elif len(picks_s_c) == 1:\n            # express pick relative to beginning of day (midnight)\n            _pick = pd.Timestamp(picks_s_c[\"phase_time\"])\n            _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n        else:\n            # there were several picks from different channels: average them\n            _relative_pick_sec = 0.\n            for _pick in picks_s_c[\"phase_time\"].values:\n                _pick = pd.Timestamp(_pick)\n                _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n# now, we convert these relative times into samples \n# and express them relative to the earliest time\n# we also store in memory the minimum time offset `tau_min_samp` for the next step\nmoveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\ntau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0])\nmoveouts_samp_arr = moveouts_samp_arr - tau_min_samp\nmoveouts_samp_arr\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         phase_type = PHASE_ON_COMP[cp]         picks_s_c = selected_event_picks[             (                 (selected_event_picks[\"station_code\"] == sta)                 &amp; (selected_event_picks[\"phase_type\"] == phase_type)             )         ]         if len(picks_s_c) == 0:             # no pick for this station/component: set to -999             tau_s_c_sec[s, c] = -999         elif len(picks_s_c) == 1:             # express pick relative to beginning of day (midnight)             _pick = pd.Timestamp(picks_s_c[\"phase_time\"])             _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]         else:             # there were several picks from different channels: average them             _relative_pick_sec = 0.             for _pick in picks_s_c[\"phase_time\"].values:                 _pick = pd.Timestamp(_pick)                 _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type] # now, we convert these relative times into samples  # and express them relative to the earliest time # we also store in memory the minimum time offset `tau_min_samp` for the next step moveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64) tau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0]) moveouts_samp_arr = moveouts_samp_arr - tau_min_samp moveouts_samp_arr Out[22]: <pre>array([[      94,       94,       66],\n       [     160,      160,      105],\n       [-1559399, -1559399, -1559399],\n       [     159,      159,      102],\n       [     272,      272,      171],\n       [     179,      179,      116],\n       [     101,      101,       68],\n       [-1559399, -1559399, -1559399],\n       [      78,       78,       53],\n       [     277,      277,      174],\n       [     225,      225,      140],\n       [       0,        0,       13],\n       [-1559399, -1559399, -1559399],\n       [     102,      102,       64],\n       [-1559399, -1559399, -1559399],\n       [      72,       72,       48],\n       [-1559399, -1559399, -1559399],\n       [     175,      175,      115],\n       [     156,      156,      104],\n       [      91,       91,       62],\n       [     165,      165,      105]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[23]: Copied! <pre>template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_stations, num_channels), dtype=np.float32)\n\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        if moveouts_samp_arr[s, c] &lt; 0:\n            # no picks were found on this station\n            weights_arr[s, c] = 0.\n            continue\n        starttime = tau_min_samp + moveouts_samp_arr[s, c]\n        endtime = starttime + TEMPLATE_DURATION_SAMP\n        template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n        if template_waveforms_arr[s, c, :].sum() == 0.:\n            # no data was available on this channel\n            weights_arr[s, c] = 0.\n        \ntemplate_waveforms_arr\n</pre> template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_stations, num_channels), dtype=np.float32)  for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         if moveouts_samp_arr[s, c] &lt; 0:             # no picks were found on this station             weights_arr[s, c] = 0.             continue         starttime = tau_min_samp + moveouts_samp_arr[s, c]         endtime = starttime + TEMPLATE_DURATION_SAMP         template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]         if template_waveforms_arr[s, c, :].sum() == 0.:             # no data was available on this channel             weights_arr[s, c] = 0.          template_waveforms_arr Out[23]: <pre>array([[[-7.95800133e-06,  8.55314720e-05,  2.11978477e-04, ...,\n         -3.52683623e-04,  9.79480028e-05,  1.74769040e-04],\n        [-1.22021929e-05,  4.51964515e-05,  1.12187183e-04, ...,\n         -4.29799955e-04, -1.41114127e-04,  4.94585664e-04],\n        [ 2.51332494e-07,  1.18968394e-07, -9.30824982e-08, ...,\n         -8.89477014e-05,  8.27156691e-05,  1.22664089e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 1.27826226e-07,  1.54385773e-07,  1.52002073e-07, ...,\n          1.35540016e-04, -3.16935242e-04, -1.91862506e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       ...,\n\n       [[-1.15102615e-04, -2.08503159e-04, -8.57295672e-05, ...,\n         -3.02342505e-05,  1.90505059e-04,  2.32697013e-04],\n        [-6.95939161e-06, -1.14591476e-05, -4.32817069e-05, ...,\n          1.95618981e-04, -6.08503397e-05, -1.64061450e-04],\n        [-3.15401110e-08,  6.19310697e-07,  1.24642884e-06, ...,\n         -6.75416959e-05,  9.95227219e-06,  9.39235106e-05]],\n\n       [[ 1.31645243e-06,  1.41594983e-05,  1.18550970e-05, ...,\n         -1.61218868e-05,  1.20281387e-04,  7.25206701e-05],\n        [ 1.01139749e-05,  3.06667994e-06,  6.11646101e-07, ...,\n          5.91952994e-04,  5.78140549e-04,  2.96404498e-04],\n        [ 5.31832107e-08,  8.85903688e-08,  1.39000178e-07, ...,\n          3.86358661e-05,  4.55262576e-04,  2.83642818e-04]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.11326456e-08,  1.84645728e-07,  2.68150814e-07, ...,\n         -1.40672455e-05,  9.12318046e-06,  1.10272435e-06]]],\n      dtype=float32)</pre> In\u00a0[24]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[25]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" <p>The following cell computes the time series of correlation coefficients between the template waveforms, $T_{s,c}$, and the continuous seismograms, $u_{s,c}$: $$ CC(t) = \\sum_{s,c} w_{s,c} \\sum_{i=1}^N \\dfrac{T^*_{s,c}(n \\Delta t) u^*_{s,c}(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}{\\sqrt{\\sum_{i=1}^N {T^*_{s,c}}^2(n \\Delta t) \\sum_{i=1}^N {u^*_{s,c}}^2(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}}, $$ with:</p> <ul> <li>$T^*_{s,c} = T_{s,c} - \\frac{1}{N} \\sum_{i=1}^N T_{s,c}(n \\Delta t)$,</li> <li>$u^*_{s,c}(t) = u_{s,c}(t) - \\frac{1}{N} \\sum_{i=1}^N u_{s,c}(t + n \\Delta t)$.</li> </ul> <p>Note that because the seismograms were filtered below periods that are shorter than the template window ($N \\Delta t$) we have $T^*_{s,c} \\approx T_{s,c}$ and $u^*_{s,c} \\approx u_{s,c}$, which spares us the computation of the mean in each sliding window.</p> In\u00a0[26]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[27]: Copied! <pre># FMF is programmed to handle multiple templates at once. Here, we only used\n# a single template, hence the size of the outermost axis of \"1\"\ncc.shape\n</pre> # FMF is programmed to handle multiple templates at once. Here, we only used # a single template, hence the size of the outermost axis of \"1\" cc.shape Out[27]: <pre>(1, 2159801)</pre> In\u00a0[28]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms _cc = cc[0, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[29]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[30]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) _cc = cc[0, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[30]: <pre>&lt;matplotlib.legend.Legend at 0x7fae4e49bb20&gt;</pre> <p>The time variation of the standard deviations of $CC(t)$ caused by gaps in some of the stations may lower the detection threshold and may thus trigger many false detections. In general, it's better to use a time-dependent detection threshold to adapt to possible gaps in the data. When using template matching on smaller seismic networks than in this example, a gap in a single station may strongly affect your time series $CC(t)$!</p> <p>Interested in a bullet-proof time-dependent threshold? Check out the link below: https://ebeauce.github.io/Seismic_BPMF/usage/api/similarity_search.html#BPMF.similarity_search.time_dependent_threshold</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#demonstration-of-a-common-issue-encountered-in-template-matching","title":"Demonstration of a common issue encountered in template matching\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#data-requirements","title":"Data requirements\u00b6","text":"<p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p> <p>Download the PhaseNet earthquake catalog with the following three commands:</p> <ul> <li>curl -o adloc_events.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_events.csv</li> <li>curl -o adloc_picks.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_picks.csv</li> <li>curl -o adloc_stations.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_stations.csv</li> </ul>"},{"location":"notebooks/tm_issues_w_detection_threshold/#installing-fast_matched_filter","title":"Installing <code>fast_matched_filter</code>\u00b6","text":"<p>This example uses <code>fast_matched_filter</code>, a Python wrapper for C and CUDA-C routines. The C and CUDA-C libraries have to be compiled. Read the instructions at https://ebeauce.github.io/FMF_documentation/introduction.html.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_issues_w_detection_threshold/#pick-one-event","title":"Pick one event\u00b6","text":"<p>Let's use the PhaseNet catalog to read the waveforms of an event.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#run-template-matching","title":"Run template matching\u00b6","text":"<p>We will now use one of the events from the PhaseNet catalog as a template event to detect events with template matching.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#read-data-from-same-day","title":"Read data from same day\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#introduce-gaps-in-some-stations","title":"Introduce gaps in some stations\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#build-template","title":"Build template\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#-background-","title":"------------------ Background ------------------\u00b6","text":"<p>A template is a collection of waveforms at different channels, $T_{s,c}(t)$, which are clips taken from the continuous seismograms, $u_{s,c}$. These clips are taken at times defined by: $$ u_{s,c}(t)\\ |\\ t \\in \\lbrace \\tau_{s,c}; \\tau_{s,c} + D \\rbrace, $$ where $\\tau_{s,c}$ is the start time of the template window and $D$ is the template duration.</p> <p>$\\tau_{s,c}$ is given by some prior information on the event: picks or modeled arrival times. The moveouts, $\\tilde{\\tau}_{s,c}$, are the collection of delay times relative to the earliest $\\tau_{s,c}$: $$ \\tilde{\\tau}_{s,c} = \\tau_{s,c} - \\underset{s,c}{\\min} \\lbrace \\tau_{s,c} \\rbrace .$$</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#-on-the-necessity-to-clip-template-waveforms-out-of-numpyndarray-instead-of-obspystream-","title":"------------------ On the necessity to clip template waveforms out of <code>numpy.ndarray</code> instead of <code>obspy.Stream</code> ------------------\u00b6","text":"<p>Looking carefully at the output of <code>print(continuous_seismograms.__str__(extended=True))</code>, a few cells before, we see that start times are generally not exactly at midnight. This is a consequence of the discrete nature of the continuous seismograms (here, sampled at 25 samples per second). Thus, in general, the $\\tau_{s,c}$ computed from picks or modeled arrival times fall in between two samples of the seismograms.</p> <p>When running a matched-filter search, we need to make sure the moveouts, $\\tilde{\\tau}_{s,c}$, ultimately expressed in samples, match exactly the times that were used when clipping the template waveforms out of $u_{s,c}$. One way to ensure this is to first cast the $\\tau_{s,c}$ to times in samples and then operate exclusively on the <code>numpy.ndarray</code>: <code>continuous_seismograms_arr</code>:</p> <p>$$ T_{s,c}[t_n] = u_{s,c}[\\tau_{s,c} + n \\Delta t],$$ where $\\Delta t$ is the sampling time.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#clip-out-waveforms-and-moveout-and-station-weight-arrays","title":"Clip out waveforms and moveout and station-weight arrays\u00b6","text":""},{"location":"notebooks/tm_issues_w_detection_threshold/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template event.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_issues_w_detection_threshold/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_multiple_templates/","title":"Notebook2","text":"In\u00a0[3]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[4]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[5]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[6]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[6]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[7]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\nevent_meta[\"time\"] = pd.to_datetime(event_meta[\"time\"])\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) event_meta[\"time\"] = pd.to_datetime(event_meta[\"time\"]) In\u00a0[8]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) In\u00a0[9]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[10]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[11]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[12]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[12]: <pre>array([[[-1.0042699e-09, -6.3175076e-10,  1.7377442e-09, ...,\n          7.1045619e-10,  4.5922328e-09, -6.6689909e-10],\n        [-2.0188387e-09,  3.7616676e-10,  2.5867664e-09, ...,\n          3.9825059e-09, -3.5006458e-09, -7.1298150e-09],\n        [-2.5482180e-10, -3.1979287e-11,  6.2106553e-10, ...,\n         -1.3381429e-09, -4.1556095e-10,  8.5630869e-10]],\n\n       [[-2.8151682e-11, -1.6209164e-11, -3.0759197e-11, ...,\n         -1.9396069e-09, -1.6679076e-10, -2.0801223e-10],\n        [-1.9637462e-11, -2.0128644e-11, -1.9461724e-11, ...,\n          5.5315131e-11, -1.1718609e-09,  3.0951329e-12],\n        [ 4.6178204e-11,  5.0274535e-11,  5.4542742e-11, ...,\n         -7.2025702e-10,  1.2868202e-10,  4.8955934e-10]],\n\n       [[ 5.4725401e-11,  7.0578925e-11, -6.2566233e-12, ...,\n         -3.6589140e-10, -7.8155921e-10, -6.3879835e-10],\n        [-1.3925512e-11,  2.0290504e-11,  1.0709553e-11, ...,\n          1.0249724e-09,  7.7783147e-10, -4.5876902e-10],\n        [ 1.9426023e-10, -3.3486833e-10, -1.7510682e-10, ...,\n          9.0171864e-10, -9.0758560e-11, -1.7541314e-10]],\n\n       ...,\n\n       [[-1.6893502e-11,  8.4201196e-12,  2.7799072e-11, ...,\n         -1.7474208e-10, -4.5014548e-11, -2.6048433e-10],\n        [ 1.4196522e-11,  3.8935088e-12, -2.4043532e-12, ...,\n         -4.9603099e-11, -9.2666923e-11,  2.2229388e-10],\n        [-7.8133723e-12,  1.1610854e-11,  1.2903780e-11, ...,\n         -8.2966682e-11, -9.3903114e-11, -8.1583212e-12]],\n\n       [[-2.1787634e-10,  6.8596184e-10,  7.1788792e-10, ...,\n          1.7016477e-10, -1.1784057e-10,  7.7036755e-10],\n        [-4.9789445e-10,  1.0552786e-10,  2.8873520e-10, ...,\n          1.2388037e-09,  5.1907750e-10,  1.0279966e-09],\n        [-3.4713493e-10,  3.7769615e-10,  2.5999658e-10, ...,\n         -1.2521584e-09, -6.6525463e-10,  1.0722102e-09]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 1.2958606e-10, -8.5401089e-11, -1.5452843e-10, ...,\n         -1.5294584e-10, -4.1159623e-10, -2.5090546e-10]]], dtype=float32)</pre> In\u00a0[13]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    folder=\"preprocessed_2_12\",\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    folder : str, optional\n        Folder name where the preprocessed waveforms are stored, by default \"preprocessed_2_12\".\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    date = pd.Timestamp(event_picks.iloc[0][\"phase_time\"]).strftime(\"%Y-%m-%d\")\n    # full path to waveform directory for this given day\n    dir_data = os.path.join(dir_waveforms, date.replace(\"-\", \"\"), folder)\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_data, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_data, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n\ndef two_point_distance(lon_1, lat_1, depth_1, lon_2, lat_2, depth_2):\n    \"\"\"Compute the distance between two points.\n\n\n    Parameters\n    -----------\n    lon_1: scalar, float\n        Longitude of Point 1.\n    lat_1: scalar, float\n        Latitude of Point 1.\n    depth_1: scalar, float\n        Depth of Point 1 (in km).\n    lon_2: scalar, float\n        Longitude of Point 2.\n    lat_2: scalar, float\n        Latitude of Point 2.\n    depth_2: scalar, float\n        Depth of Point 2 (in km).\n\n    Returns\n    ---------\n    dist: scalar, float\n        Distance between Point 1 and Point 2 in kilometers.\n    \"\"\"\n\n    from obspy.geodetics.base import calc_vincenty_inverse\n\n    dist, az, baz = calc_vincenty_inverse(lat_1, lon_1, lat_2, lon_2)\n    dist /= 1000.0  # from m to km\n    dist = np.sqrt(dist**2 + (depth_1 - depth_2) ** 2)\n    return dist\n    \n</pre> def fetch_event_waveforms(     event_picks,     folder=\"preprocessed_2_12\",     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     folder : str, optional         Folder name where the preprocessed waveforms are stored, by default \"preprocessed_2_12\".     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     date = pd.Timestamp(event_picks.iloc[0][\"phase_time\"]).strftime(\"%Y-%m-%d\")     # full path to waveform directory for this given day     dir_data = os.path.join(dir_waveforms, date.replace(\"-\", \"\"), folder)     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_data, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_data, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream  def two_point_distance(lon_1, lat_1, depth_1, lon_2, lat_2, depth_2):     \"\"\"Compute the distance between two points.       Parameters     -----------     lon_1: scalar, float         Longitude of Point 1.     lat_1: scalar, float         Latitude of Point 1.     depth_1: scalar, float         Depth of Point 1 (in km).     lon_2: scalar, float         Longitude of Point 2.     lat_2: scalar, float         Latitude of Point 2.     depth_2: scalar, float         Depth of Point 2 (in km).      Returns     ---------     dist: scalar, float         Distance between Point 1 and Point 2 in kilometers.     \"\"\"      from obspy.geodetics.base import calc_vincenty_inverse      dist, az, baz = calc_vincenty_inverse(lat_1, lon_1, lat_2, lon_2)     dist /= 1000.0  # from m to km     dist = np.sqrt(dist**2 + (depth_1 - depth_2) ** 2)     return dist      In\u00a0[14]: Copied! <pre># select events based on magnitude and origin time\n\n# comprehensive run with lots of templates:\nselected_events_meta = event_meta[\n    (\n        (event_meta[\"time\"] &gt;= \"2019-07-04\")\n        &amp; (event_meta[\"time\"] &lt; \"2019-07-04T15:00:00\")\n    )\n    | (\n        (event_meta[\"magnitude\"] &gt; 1.0)\n        &amp; (event_meta[\"magnitude\"] &lt; 5.0)\n        &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\")\n        &amp; (event_meta[\"time\"] &lt; \"2019-07-04T23:50:00\")\n    )\n    ]\nnum_templates = len(selected_events_meta)\n\n# quick test:\n# selected_events_meta = event_meta[\n#     (event_meta[\"magnitude\"] &gt; 3.0)\n#     &amp; (event_meta[\"magnitude\"] &lt; 5.0)\n#     &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\")\n#     &amp; (event_meta[\"time\"] &lt; \"2019-07-04T23:50:00\")\n#     ]\n# num_templates = len(selected_events_meta)\n\nselected_events_meta\n</pre> # select events based on magnitude and origin time  # comprehensive run with lots of templates: selected_events_meta = event_meta[     (         (event_meta[\"time\"] &gt;= \"2019-07-04\")         &amp; (event_meta[\"time\"] &lt; \"2019-07-04T15:00:00\")     )     | (         (event_meta[\"magnitude\"] &gt; 1.0)         &amp; (event_meta[\"magnitude\"] &lt; 5.0)         &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\")         &amp; (event_meta[\"time\"] &lt; \"2019-07-04T23:50:00\")     )     ] num_templates = len(selected_events_meta)  # quick test: # selected_events_meta = event_meta[ #     (event_meta[\"magnitude\"] &gt; 3.0) #     &amp; (event_meta[\"magnitude\"] &lt; 5.0) #     &amp; (event_meta[\"time\"] &gt;= \"2019-07-04\") #     &amp; (event_meta[\"time\"] &lt; \"2019-07-04T23:50:00\") #     ] # num_templates = len(selected_events_meta)  selected_events_meta Out[14]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 957 2019-07-04 23:47:59.571653225 0.664112 0.297966 34 2.029195 0.247851 9083 -117.553617 35.754379 0.807458 958 2019-07-04 23:48:34.629115323 0.914139 0.045003 31 1.202878 0.136497 13305 -117.550146 35.735471 8.289523 960 2019-07-04 23:48:58.396267689 0.923662 0.039034 47 1.401906 0.081853 6293 -117.539234 35.741503 12.039196 961 2019-07-04 23:49:08.344870010 0.552696 0.035169 29 1.136455 0.128575 10909 -117.481268 35.692472 11.125272 962 2019-07-04 23:49:29.970256959 0.697343 0.129847 46 1.269805 0.103881 6747 -117.511154 35.684019 9.534288 <p>937 rows \u00d7 10 columns</p> <p>In general, when handling a database of template events, it is convenient to keep track of a unique template id for each template, which may be anything. However, template indexes in <code>numpy.ndarray</code> will go from 0 to <code>num_templates</code> - 1.</p> In\u00a0[15]: Copied! <pre>template_ids = pd.Series(selected_events_meta[\"event_index\"].values, name=\"template_id\")\ntemplate_ids\n</pre> template_ids = pd.Series(selected_events_meta[\"event_index\"].values, name=\"template_id\") template_ids Out[15]: <pre>0       6720\n1      17122\n2       5411\n3      17868\n4      25037\n       ...  \n932     9083\n933    13305\n934     6293\n935    10909\n936     6747\nName: template_id, Length: 937, dtype: int64</pre> In\u00a0[16]: Copied! <pre># for example, the id of the template indexed by 3 is:\ntemplate_ids.iloc[3]\n</pre> # for example, the id of the template indexed by 3 is: template_ids.iloc[3] Out[16]: <pre>17868</pre> In\u00a0[17]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[18]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\nmoveouts_samp_arr = np.zeros((num_templates, num_stations, num_channels), dtype=np.int64)\ntau_min_samp_arr = np.zeros(num_templates, dtype=np.int64)\nfor t, tid in enumerate(template_ids):\n    # add station_code columns to `selected_event_picks`\n    selected_event_picks = picks[picks[\"event_index\"] == tid].copy()\n    selected_event_picks.set_index(\"station_id\", inplace=True)\n    for staid in selected_event_picks.index:\n        station_code = staid.split(\".\")[1]\n        selected_event_picks.loc[staid, \"station_code\"] = station_code\n    tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\n    template_metadata = selected_events_meta.set_index(\"event_index\").loc[tid]\n    for s, sta in enumerate(station_codes):\n        for c, cp in enumerate(component_codes):\n            phase_type = PHASE_ON_COMP[cp]\n            picks_s_c = selected_event_picks[\n                (\n                    (selected_event_picks[\"station_code\"] == sta)\n                    &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n                )\n            ]\n            if len(picks_s_c) == 0:\n                # no pick for this station/component:\n                # make a VERY crude approximation of the travel time\n                if sta not in station_meta[\"station\"].values:\n                    tau_s_c_sec[s, c] = -999\n                    continue\n                rec_meta = (station_meta.set_index(\"station\").loc[sta]).iloc[0]\n                src_rec_distance = two_point_distance(\n                    template_metadata[\"longitude\"],\n                    template_metadata[\"latitude\"],\n                    template_metadata[\"depth_km\"],\n                    rec_meta[\"longitude\"],\n                    rec_meta[\"latitude\"],\n                    rec_meta[\"depth_km\"]\n                )\n                if phase_type.lower() == \"p\":\n                    synthetic_tt = src_rec_distance / (3. * 1.72)\n                elif phase_type.lower() == \"s\":\n                    synthetic_tt = src_rec_distance / 3.\n                synthetic_pick = (\n                    pd.Timestamp(template_metadata[\"time\"])\n                    + pd.Timedelta(seconds=synthetic_tt)\n                )\n                tau_s_c_sec[s, c] = (synthetic_pick - pd.Timestamp(synthetic_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            elif len(picks_s_c) == 1:\n                # express pick relative to beginning of day (midnight)\n                _pick = pd.Timestamp(picks_s_c[\"phase_time\"].iloc[0])\n                _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n                tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n            else:\n                # there were several picks from different channels: average them\n                _relative_pick_sec = 0.\n                for _pick in picks_s_c[\"phase_time\"].values:\n                    _pick = pd.Timestamp(_pick)\n                    _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n                _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n                tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n    # now, we convert these relative times into samples \n    # and express them relative to the earliest time\n    # we also store in memory the minimum time offset `tau_min_samp` for the next step\n    moveouts_samp_arr[t, ...] = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\n    tau_min_samp_arr[t] = np.min(moveouts_samp_arr[t, moveouts_samp_arr[t, ...] &gt; 0])\n    moveouts_samp_arr[t, ...] = moveouts_samp_arr[t, ...] - tau_min_samp_arr[t]\nmoveouts_samp_arr[1, ...]\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component moveouts_samp_arr = np.zeros((num_templates, num_stations, num_channels), dtype=np.int64) tau_min_samp_arr = np.zeros(num_templates, dtype=np.int64) for t, tid in enumerate(template_ids):     # add station_code columns to `selected_event_picks`     selected_event_picks = picks[picks[\"event_index\"] == tid].copy()     selected_event_picks.set_index(\"station_id\", inplace=True)     for staid in selected_event_picks.index:         station_code = staid.split(\".\")[1]         selected_event_picks.loc[staid, \"station_code\"] = station_code     tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)     template_metadata = selected_events_meta.set_index(\"event_index\").loc[tid]     for s, sta in enumerate(station_codes):         for c, cp in enumerate(component_codes):             phase_type = PHASE_ON_COMP[cp]             picks_s_c = selected_event_picks[                 (                     (selected_event_picks[\"station_code\"] == sta)                     &amp; (selected_event_picks[\"phase_type\"] == phase_type)                 )             ]             if len(picks_s_c) == 0:                 # no pick for this station/component:                 # make a VERY crude approximation of the travel time                 if sta not in station_meta[\"station\"].values:                     tau_s_c_sec[s, c] = -999                     continue                 rec_meta = (station_meta.set_index(\"station\").loc[sta]).iloc[0]                 src_rec_distance = two_point_distance(                     template_metadata[\"longitude\"],                     template_metadata[\"latitude\"],                     template_metadata[\"depth_km\"],                     rec_meta[\"longitude\"],                     rec_meta[\"latitude\"],                     rec_meta[\"depth_km\"]                 )                 if phase_type.lower() == \"p\":                     synthetic_tt = src_rec_distance / (3. * 1.72)                 elif phase_type.lower() == \"s\":                     synthetic_tt = src_rec_distance / 3.                 synthetic_pick = (                     pd.Timestamp(template_metadata[\"time\"])                     + pd.Timedelta(seconds=synthetic_tt)                 )                 tau_s_c_sec[s, c] = (synthetic_pick - pd.Timestamp(synthetic_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             elif len(picks_s_c) == 1:                 # express pick relative to beginning of day (midnight)                 _pick = pd.Timestamp(picks_s_c[\"phase_time\"].iloc[0])                 _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()                 tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]             else:                 # there were several picks from different channels: average them                 _relative_pick_sec = 0.                 for _pick in picks_s_c[\"phase_time\"].values:                     _pick = pd.Timestamp(_pick)                     _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()                 _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))                 tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]     # now, we convert these relative times into samples      # and express them relative to the earliest time     # we also store in memory the minimum time offset `tau_min_samp` for the next step     moveouts_samp_arr[t, ...] = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)     tau_min_samp_arr[t] = np.min(moveouts_samp_arr[t, moveouts_samp_arr[t, ...] &gt; 0])     moveouts_samp_arr[t, ...] = moveouts_samp_arr[t, ...] - tau_min_samp_arr[t] moveouts_samp_arr[1, ...] Out[18]: <pre>array([[    589,     589,     335],\n       [    550,     550,     312],\n       [    227,     227,     104],\n       [      0,       0,      27],\n       [    267,     267,     189],\n       [    290,     290,     161],\n       [    675,     675,     385],\n       [-108309, -108309, -108309],\n       [-108309, -108309, -108309],\n       [    520,     520,     295],\n       [    425,     425,     240],\n       [-108309, -108309, -108309],\n       [     20,      20,      40],\n       [-108309, -108309, -108309],\n       [    604,     604,     343],\n       [    370,     370,     300],\n       [    392,     392,     220],\n       [    661,     661,     377],\n       [-108309, -108309, -108309],\n       [    351,     351,     196],\n       [    577,     577,     328]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[25]: Copied! <pre># when searching for small earthquakes, the closest seismometers are the likeliest to detect them\n# thus, we limit the computation of the correlation coefficients to the closest NUM_STATIONS_PER_TEMPLATE stations\nNUM_STATIONS_PER_TEMPLATE = 10\n\ntemplate_waveforms_arr = np.zeros((num_templates, num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_templates, num_stations, num_channels), dtype=np.float32)\n\nfor t in range(num_templates):\n    distance_ordered_sta_index = np.argsort(moveouts_samp_arr[t, :, 0])\n    no_data_sta_index = np.where(moveouts_samp_arr[t, :, 0] &lt; 0)[0]\n    selected_sta_index = np.setdiff1d(distance_ordered_sta_index, no_data_sta_index, assume_unique=True)[:NUM_STATIONS_PER_TEMPLATE]\n    for s, sta in enumerate(station_codes):\n        if s not in selected_sta_index:\n            weights_arr[t, s, :] = 0.\n            continue\n        for c, cp in enumerate(component_codes):\n            starttime = tau_min_samp_arr[t] + moveouts_samp_arr[t, s, c]\n            endtime = starttime + TEMPLATE_DURATION_SAMP\n            template_waveforms_arr[t, s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n            if template_waveforms_arr[t, s, c, :].sum() == 0.:\n                # no data was available on this channel\n                weights_arr[t, s, c] = 0.\n            \ntemplate_waveforms_arr[0, ...]\n</pre> # when searching for small earthquakes, the closest seismometers are the likeliest to detect them # thus, we limit the computation of the correlation coefficients to the closest NUM_STATIONS_PER_TEMPLATE stations NUM_STATIONS_PER_TEMPLATE = 10  template_waveforms_arr = np.zeros((num_templates, num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_templates, num_stations, num_channels), dtype=np.float32)  for t in range(num_templates):     distance_ordered_sta_index = np.argsort(moveouts_samp_arr[t, :, 0])     no_data_sta_index = np.where(moveouts_samp_arr[t, :, 0] &lt; 0)[0]     selected_sta_index = np.setdiff1d(distance_ordered_sta_index, no_data_sta_index, assume_unique=True)[:NUM_STATIONS_PER_TEMPLATE]     for s, sta in enumerate(station_codes):         if s not in selected_sta_index:             weights_arr[t, s, :] = 0.             continue         for c, cp in enumerate(component_codes):             starttime = tau_min_samp_arr[t] + moveouts_samp_arr[t, s, c]             endtime = starttime + TEMPLATE_DURATION_SAMP             template_waveforms_arr[t, s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]             if template_waveforms_arr[t, s, c, :].sum() == 0.:                 # no data was available on this channel                 weights_arr[t, s, c] = 0.              template_waveforms_arr[0, ...] Out[25]: <pre>array([[[-1.7712456e-04, -3.3437931e-03,  5.2173040e-03, ...,\n          7.2149672e-03, -6.7763682e-03, -4.4316081e-03],\n        [ 4.6852408e-03,  2.5464378e-03, -1.1661325e-03, ...,\n         -2.0354675e-04, -3.5304744e-03,  2.8006290e-03],\n        [ 2.7528582e-03,  1.4680702e-03, -6.3902762e-04, ...,\n         -7.5245270e-04, -4.0219571e-03, -1.4727380e-03]],\n\n       [[ 5.5182213e-04,  2.0843970e-04, -2.1052101e-05, ...,\n          4.0525541e-04,  4.4130632e-03,  2.4935650e-03],\n        [ 1.7849941e-04,  3.3065138e-04, -1.3959112e-04, ...,\n         -3.7473389e-03,  4.4838581e-03,  1.4016192e-03],\n        [-2.6668483e-04,  1.0491205e-04,  5.2927178e-05, ...,\n         -7.8176003e-04, -3.8597337e-04,  4.0891889e-04]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n\n       ...,\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n\n       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n        [-5.4853724e-04, -5.2027177e-04,  3.8197983e-04, ...,\n          4.3100165e-04, -4.4469696e-05,  7.7649130e-04]]], dtype=float32)</pre> In\u00a0[26]: Copied! <pre>template_waveforms_arr.shape\n</pre> template_waveforms_arr.shape Out[26]: <pre>(937, 21, 3, 200)</pre> In\u00a0[27]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr, axis=(1, 2), keepdims=True)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr, axis=(1, 2), keepdims=True)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[28]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" <p>The following cell computes the time series of correlation coefficients between the template waveforms, $T_{s,c}$, and the continuous seismograms, $u_{s,c}$: $$ CC(t) = \\sum_{s,c} w_{s,c} \\sum_{i=1}^N \\dfrac{T^*_{s,c}(n \\Delta t) u^*_{s,c}(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}{\\sqrt{\\sum_{i=1}^N {T^*_{s,c}}^2(n \\Delta t) \\sum_{i=1}^N {u^*_{s,c}}^2(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}}, $$ with:</p> <ul> <li>$T^*_{s,c} = T_{s,c} - \\frac{1}{N} \\sum_{i=1}^N T_{s,c}(n \\Delta t)$,</li> <li>$u^*_{s,c}(t) = u_{s,c}(t) - \\frac{1}{N} \\sum_{i=1}^N u_{s,c}(t + n \\Delta t)$.</li> </ul> <p>Note that because the seismograms were filtered below periods that are shorter than the template window ($N \\Delta t$) we have $T^*_{s,c} \\approx T_{s,c}$ and $u^*_{s,c} \\approx u_{s,c}$, which spares us the computation of the mean in each sliding window.</p> In\u00a0[29]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[30]: Copied! <pre># unlike in the previous notebook, we now have multiple time series of correlation coefficients,\n# one for each template\ncc.shape\n</pre> # unlike in the previous notebook, we now have multiple time series of correlation coefficients, # one for each template cc.shape Out[30]: <pre>(937, 2159801)</pre> In\u00a0[32]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\ntemplate_idx = 3\n_cc = cc[template_idx, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(f\"Time series of template {template_idx:d}/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms template_idx = 3 _cc = cc[template_idx, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(f\"Time series of template {template_idx:d}/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[33]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[34]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\ntemplate_idx = 2\n_cc = cc[template_idx, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) template_idx = 2 _cc = cc[template_idx, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[34]: <pre>&lt;matplotlib.legend.Legend at 0x7f3b7a18b850&gt;</pre> In\u00a0[35]: Copied! <pre>NUM_RMS = 8.\ndate = pd.Timestamp(\n    (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,\n    unit=\"s\"\n).strftime(\"%Y-%m-%d\")\n\ncatalog = {\n    \"detection_time\": [],\n    \"cc\": [],\n    \"normalized_cc\": [],\n    \"tid\": [],\n    \"longitude\": [],\n    \"latitude\": [],\n    \"depth\": []\n}\n\nfor t, tid in enumerate(template_ids):\n    detection_threshold = NUM_RMS * np.std(cc[t, :])\n    event_cc_indexes = select_cc_indexes(cc[t, :], detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n    for i in range(len(event_cc_indexes)):\n        # --------------------------------------\n        catalog[\"detection_time\"].append(pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\"))\n        catalog[\"cc\"].append(cc[t, event_cc_indexes[i]])\n        catalog[\"normalized_cc\"].append(cc[t, event_cc_indexes[i]] / detection_threshold)\n        catalog[\"tid\"].append(tid)\n        template_hypocenter = selected_events_meta[\n            selected_events_meta[\"event_index\"] == tid\n        ].iloc[0]\n        catalog[\"longitude\"].append(template_hypocenter[\"longitude\"])\n        catalog[\"latitude\"].append(template_hypocenter[\"latitude\"])\n        catalog[\"depth\"].append(template_hypocenter[\"depth_km\"])\ncatalog = pd.DataFrame(catalog)\ncatalog.sort_values(\"detection_time\", inplace=True)\ncatalog\n</pre> NUM_RMS = 8. date = pd.Timestamp(     (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,     unit=\"s\" ).strftime(\"%Y-%m-%d\")  catalog = {     \"detection_time\": [],     \"cc\": [],     \"normalized_cc\": [],     \"tid\": [],     \"longitude\": [],     \"latitude\": [],     \"depth\": [] }  for t, tid in enumerate(template_ids):     detection_threshold = NUM_RMS * np.std(cc[t, :])     event_cc_indexes = select_cc_indexes(cc[t, :], detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)     for i in range(len(event_cc_indexes)):         # --------------------------------------         catalog[\"detection_time\"].append(pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\"))         catalog[\"cc\"].append(cc[t, event_cc_indexes[i]])         catalog[\"normalized_cc\"].append(cc[t, event_cc_indexes[i]] / detection_threshold)         catalog[\"tid\"].append(tid)         template_hypocenter = selected_events_meta[             selected_events_meta[\"event_index\"] == tid         ].iloc[0]         catalog[\"longitude\"].append(template_hypocenter[\"longitude\"])         catalog[\"latitude\"].append(template_hypocenter[\"latitude\"])         catalog[\"depth\"].append(template_hypocenter[\"depth_km\"]) catalog = pd.DataFrame(catalog) catalog.sort_values(\"detection_time\", inplace=True) catalog Out[35]: detection_time cc normalized_cc tid longitude latitude depth 0 2019-07-04 00:46:45.640 1.000000 5.208948 6720 -117.882570 36.091088 4.643969 41 2019-07-04 00:46:45.640 0.629913 3.102449 13098 -117.879256 36.090409 4.268292 19 2019-07-04 00:46:45.640 0.339081 1.628305 17868 -117.866468 36.093520 4.981447 10 2019-07-04 00:46:45.640 0.716178 4.297031 5411 -117.880902 36.091986 4.854336 26 2019-07-04 00:46:45.680 0.276461 1.259136 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... 2958 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 7529 2019-07-04 23:59:22.160 0.191812 1.316138 381 -117.501536 35.715034 11.589107 7192 2019-07-04 23:59:31.640 0.193527 1.305970 127 -117.515593 35.703031 10.744590 8293 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 7160 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 <p>9114 rows \u00d7 7 columns</p> <p>When building a catalog, it is always necessary to visualize some of the detected event waveforms to get a sense of the ratio of true-to-false detection rate.</p> <p>In the following, we plot the waveforms of each detected event and we also compare the stack of all the waveforms to the original template waveform. Since all events share similar waveforms, the stack is similar to the template waveform. Moreover, since noise across all these waveforms sums up incoherently, stacking acts as a denoiser which may help you produce a cleaner version of the template waveform, for example on remote stations.</p> In\u00a0[36]: Copied! <pre>STATION_NAME = \"CLC\"\nCOMPONENT_NAME = \"Z\"\n\nsta_idx = station_codes.index(STATION_NAME)\ncp_idx = component_codes.index(COMPONENT_NAME)\ntp_idx = 223\n\nsubcatalog = catalog[catalog[\"tid\"] == template_ids.iloc[tp_idx]]\n\ndetected_event_waveforms = []\nfor i in range(len(subcatalog)):\n    detection_time_samp = (\n        subcatalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)\n    ).total_seconds() * SAMPLING_RATE_HZ\n    idx_start = int(detection_time_samp) + moveouts_samp_arr[tp_idx, sta_idx, cp_idx]\n    idx_end = idx_start + TEMPLATE_DURATION_SAMP\n    detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])\n\ndetected_event_waveforms = np.asarray(detected_event_waveforms)\ndetected_event_waveforms.shape\n</pre> STATION_NAME = \"CLC\" COMPONENT_NAME = \"Z\"  sta_idx = station_codes.index(STATION_NAME) cp_idx = component_codes.index(COMPONENT_NAME) tp_idx = 223  subcatalog = catalog[catalog[\"tid\"] == template_ids.iloc[tp_idx]]  detected_event_waveforms = [] for i in range(len(subcatalog)):     detection_time_samp = (         subcatalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)     ).total_seconds() * SAMPLING_RATE_HZ     idx_start = int(detection_time_samp) + moveouts_samp_arr[tp_idx, sta_idx, cp_idx]     idx_end = idx_start + TEMPLATE_DURATION_SAMP     detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])  detected_event_waveforms = np.asarray(detected_event_waveforms) detected_event_waveforms.shape Out[36]: <pre>(17, 200)</pre> In\u00a0[37]: Copied! <pre>fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15))\ngs = fig.add_gridspec(nrows=4)\n\nax1 = fig.add_subplot(gs[:3])\n\n_time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ\n\nstack = np.zeros(detected_event_waveforms.shape[1])\nfor i in range(detected_event_waveforms.shape[0]):\n    norm = np.abs(detected_event_waveforms[i, :]).max()\n    if subcatalog[\"cc\"].iloc[i] &gt; 0.999:\n        color = \"r\"\n        template_wav = detected_event_waveforms[i, :] / norm\n    else:\n        color = \"k\"\n    time_of_day = subcatalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")\n    ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)\n    ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")\n    stack += detected_event_waveforms[i, :] / norm\nstack /= np.abs(stack).max()\nax1.set_xlabel(\"Time (s)\")\nax1.set_xlim(_time_wav.min(), _time_wav.max())\nax1.set_ylabel(\"Normalized offset amplitude\")\nax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")\n\nax2 = fig.add_subplot(gs[3], sharex=ax1)\nax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\")\nax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\")\nax2.legend(loc=\"upper left\")\nax2.set_xlabel(\"Time (s)\")\nax2.set_ylabel(\"Normalized amplitude\")\n</pre> fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15)) gs = fig.add_gridspec(nrows=4)  ax1 = fig.add_subplot(gs[:3])  _time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ  stack = np.zeros(detected_event_waveforms.shape[1]) for i in range(detected_event_waveforms.shape[0]):     norm = np.abs(detected_event_waveforms[i, :]).max()     if subcatalog[\"cc\"].iloc[i] &gt; 0.999:         color = \"r\"         template_wav = detected_event_waveforms[i, :] / norm     else:         color = \"k\"     time_of_day = subcatalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")     ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)     ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")     stack += detected_event_waveforms[i, :] / norm stack /= np.abs(stack).max() ax1.set_xlabel(\"Time (s)\") ax1.set_xlim(_time_wav.min(), _time_wav.max()) ax1.set_ylabel(\"Normalized offset amplitude\") ax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")  ax2 = fig.add_subplot(gs[3], sharex=ax1) ax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\") ax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\") ax2.legend(loc=\"upper left\") ax2.set_xlabel(\"Time (s)\") ax2.set_ylabel(\"Normalized amplitude\") Out[37]: <pre>Text(0, 0.5, 'Normalized amplitude')</pre> In\u00a0[38]: Copied! <pre>catalog[\"interevent_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds()\nfor tid in template_ids:\n    subcatalog = catalog[catalog[\"tid\"] == tid]\n    catalog.loc[subcatalog.index, \"return_time_s\"] = subcatalog[\"detection_time\"].diff().dt.total_seconds()\ncatalog\n</pre> catalog[\"interevent_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds() for tid in template_ids:     subcatalog = catalog[catalog[\"tid\"] == tid]     catalog.loc[subcatalog.index, \"return_time_s\"] = subcatalog[\"detection_time\"].diff().dt.total_seconds() catalog Out[38]: detection_time cc normalized_cc tid longitude latitude depth interevent_time_s return_time_s 0 2019-07-04 00:46:45.640 1.000000 5.208948 6720 -117.882570 36.091088 4.643969 NaN NaN 41 2019-07-04 00:46:45.640 0.629913 3.102449 13098 -117.879256 36.090409 4.268292 0.00 NaN 19 2019-07-04 00:46:45.640 0.339081 1.628305 17868 -117.866468 36.093520 4.981447 0.00 NaN 10 2019-07-04 00:46:45.640 0.716178 4.297031 5411 -117.880902 36.091986 4.854336 0.00 NaN 26 2019-07-04 00:46:45.680 0.276461 1.259136 25037 -117.846320 36.100386 5.943363 0.04 NaN ... ... ... ... ... ... ... ... ... ... 2958 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 4.72 3658.12 7529 2019-07-04 23:59:22.160 0.191812 1.316138 381 -117.501536 35.715034 11.589107 5.28 4977.76 7192 2019-07-04 23:59:31.640 0.193527 1.305970 127 -117.515593 35.703031 10.744590 9.48 3191.20 8293 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 0.08 231.72 7160 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 2.44 243.84 <p>9114 rows \u00d7 9 columns</p> In\u00a0[39]: Copied! <pre>fig, axes = plt.subplots(num=\"interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))\n\naxes[0].scatter(\n    catalog[\"detection_time\"], catalog[\"interevent_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[0].set_xlabel(\"Detection time\")\naxes[0].set_ylabel(\"Interevent time (s)\")\naxes[0].set_title(\"Interevent time vs detection time\")\n\naxes[1].scatter(\n    catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[1].set_xlabel(\"Detection time\")\naxes[1].set_ylabel(\"Return time (s)\")\naxes[1].set_title(\"Return time vs detection time\")\n\nfor ax in axes:\n    ax.grid()\n    cbar = plt.colorbar(ax.collections[0], ax=ax)\n    cbar.set_label(\"CC\")\n    ax.set_yscale(\"log\")\n</pre> fig, axes = plt.subplots(num=\"interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))  axes[0].scatter(     catalog[\"detection_time\"], catalog[\"interevent_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[0].set_xlabel(\"Detection time\") axes[0].set_ylabel(\"Interevent time (s)\") axes[0].set_title(\"Interevent time vs detection time\")  axes[1].scatter(     catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[1].set_xlabel(\"Detection time\") axes[1].set_ylabel(\"Return time (s)\") axes[1].set_title(\"Return time vs detection time\")  for ax in axes:     ax.grid()     cbar = plt.colorbar(ax.collections[0], ax=ax)     cbar.set_label(\"CC\")     ax.set_yscale(\"log\") In\u00a0[40]: Copied! <pre>def compute_distances(\n    source_longitudes,\n    source_latitudes,\n    source_depths,\n    receiver_longitudes,\n    receiver_latitudes,\n    receiver_depths,\n    backend=\"obspy\"\n):\n    \"\"\"\n    Fast distance computation between all source points and all receivers.\n\n    This function uses `cartopy.geodesic.Geodesic` to compute pair-wise distances\n    between source points and receivers. It computes both hypocentral distances\n    and, if specified, epicentral distances.\n\n    Parameters\n    ----------\n    source_longitudes : numpy.ndarray or list\n        Longitudes, in decimal degrees, of the source points.\n    source_latitudes : numpy.ndarray or list\n        Latitudes, in decimal degrees, of the source points.\n    source_depths : numpy.ndarray or list\n        Depths, in kilometers, of the source points.\n    receiver_longitudes : numpy.ndarray or list\n        Longitudes, in decimal degrees, of the receivers.\n    receiver_latitudes : numpy.ndarray or list\n        Latitudes, in decimal degrees, of the receivers.\n    receiver_depths : numpy.ndarray or list\n        Depths, in kilometers, of the receivers. Negative depths indicate\n        receivers located at the surface.\n\n    Returns\n    -------\n    hypocentral_distances : numpy.ndarray\n        Array of hypocentral distances between source points and receivers.\n        The shape of the array is (n_sources, n_receivers).\n    \"\"\"\n    if backend == \"cartopy\":\n        from cartopy.geodesic import Geodesic\n    elif backend == \"obspy\":\n        from obspy.geodetics.base import calc_vincenty_inverse\n        \n    source_longitudes = np.atleast_1d(source_longitudes)\n    source_latitudes = np.atleast_1d(source_latitudes)\n    source_depths = np.atleast_1d(source_depths)\n    receiver_longitudes = np.atleast_1d(receiver_longitudes)\n    receiver_latitudes = np.atleast_1d(receiver_latitudes)\n    receiver_depths = np.atleast_1d(receiver_depths)\n\n    # initialize distance array\n    hypocentral_distances = np.zeros(\n        (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32\n    )\n    epicentral_distances = np.zeros(\n        (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32\n    )\n    if backend == \"cartopy\":\n        # initialize the Geodesic instance\n        G = Geodesic()\n        for s in range(len(receiver_latitudes)):\n            epi_distances = G.inverse(\n                np.array([[receiver_longitudes[s], receiver_latitudes[s]]]),\n                np.hstack(\n                    (source_longitudes[:, np.newaxis], source_latitudes[:, np.newaxis])\n                ),\n            )\n            epicentral_distances[:, s] = np.asarray(epi_distances)[:, 0].squeeze() / 1000.0\n            hypocentral_distances[:, s] = np.sqrt(\n                epicentral_distances[:, s] ** 2 + (source_depths - receiver_depths[s]) ** 2\n            )\n    elif backend == \"obspy\":\n        for s1 in range(len(source_latitudes)):\n            for s2 in range(len(receiver_latitudes)):\n                dist, _, _ = calc_vincenty_inverse(\n                    source_latitudes[s1],\n                    source_longitudes[s1],\n                    receiver_latitudes[s2],\n                    receiver_longitudes[s2],\n                )\n                epicentral_distances[s1, s2] = dist / 1000.0\n                hypocentral_distances[s1, s2] = np.sqrt(\n                    epicentral_distances[s1, s2] ** 2 + (source_depths[s1] - receiver_depths[s2]) ** 2\n                )\n    return hypocentral_distances\n</pre> def compute_distances(     source_longitudes,     source_latitudes,     source_depths,     receiver_longitudes,     receiver_latitudes,     receiver_depths,     backend=\"obspy\" ):     \"\"\"     Fast distance computation between all source points and all receivers.      This function uses `cartopy.geodesic.Geodesic` to compute pair-wise distances     between source points and receivers. It computes both hypocentral distances     and, if specified, epicentral distances.      Parameters     ----------     source_longitudes : numpy.ndarray or list         Longitudes, in decimal degrees, of the source points.     source_latitudes : numpy.ndarray or list         Latitudes, in decimal degrees, of the source points.     source_depths : numpy.ndarray or list         Depths, in kilometers, of the source points.     receiver_longitudes : numpy.ndarray or list         Longitudes, in decimal degrees, of the receivers.     receiver_latitudes : numpy.ndarray or list         Latitudes, in decimal degrees, of the receivers.     receiver_depths : numpy.ndarray or list         Depths, in kilometers, of the receivers. Negative depths indicate         receivers located at the surface.      Returns     -------     hypocentral_distances : numpy.ndarray         Array of hypocentral distances between source points and receivers.         The shape of the array is (n_sources, n_receivers).     \"\"\"     if backend == \"cartopy\":         from cartopy.geodesic import Geodesic     elif backend == \"obspy\":         from obspy.geodetics.base import calc_vincenty_inverse              source_longitudes = np.atleast_1d(source_longitudes)     source_latitudes = np.atleast_1d(source_latitudes)     source_depths = np.atleast_1d(source_depths)     receiver_longitudes = np.atleast_1d(receiver_longitudes)     receiver_latitudes = np.atleast_1d(receiver_latitudes)     receiver_depths = np.atleast_1d(receiver_depths)      # initialize distance array     hypocentral_distances = np.zeros(         (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32     )     epicentral_distances = np.zeros(         (len(source_latitudes), len(receiver_latitudes)), dtype=np.float32     )     if backend == \"cartopy\":         # initialize the Geodesic instance         G = Geodesic()         for s in range(len(receiver_latitudes)):             epi_distances = G.inverse(                 np.array([[receiver_longitudes[s], receiver_latitudes[s]]]),                 np.hstack(                     (source_longitudes[:, np.newaxis], source_latitudes[:, np.newaxis])                 ),             )             epicentral_distances[:, s] = np.asarray(epi_distances)[:, 0].squeeze() / 1000.0             hypocentral_distances[:, s] = np.sqrt(                 epicentral_distances[:, s] ** 2 + (source_depths - receiver_depths[s]) ** 2             )     elif backend == \"obspy\":         for s1 in range(len(source_latitudes)):             for s2 in range(len(receiver_latitudes)):                 dist, _, _ = calc_vincenty_inverse(                     source_latitudes[s1],                     source_longitudes[s1],                     receiver_latitudes[s2],                     receiver_longitudes[s2],                 )                 epicentral_distances[s1, s2] = dist / 1000.0                 hypocentral_distances[s1, s2] = np.sqrt(                     epicentral_distances[s1, s2] ** 2 + (source_depths[s1] - receiver_depths[s2]) ** 2                 )     return hypocentral_distances In\u00a0[41]: Copied! <pre>intertemplate_distances = compute_distances(\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values,\n    selected_events_meta[\"depth_km\"].values,\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values, \n    selected_events_meta[\"depth_km\"].values,\n    backend=\"obspy\"\n)\n\nintertemplate_distances = pd.DataFrame(\n        index=template_ids.values, columns=template_ids.values, data=intertemplate_distances\n    )\n\nintertemplate_distances\n</pre> intertemplate_distances = compute_distances(     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,     selected_events_meta[\"depth_km\"].values,     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,      selected_events_meta[\"depth_km\"].values,     backend=\"obspy\" )  intertemplate_distances = pd.DataFrame(         index=template_ids.values, columns=template_ids.values, data=intertemplate_distances     )  intertemplate_distances Out[41]: 6720 17122 5411 17868 25037 6929 7607 13098 15698 4380 ... 3733 11897 910 341 5934 9083 13305 6293 10909 6747 6720 0.000000 79.720917 0.277062 1.513100 3.661949 19.026287 11.666162 0.485678 2.143338 19.266460 ... 28.748285 61.275852 52.038624 59.792736 58.538982 47.874050 49.704773 50.195679 57.541214 56.467831 17122 79.720917 0.000000 79.789001 79.838013 80.411301 82.627167 71.581108 79.649605 78.469719 83.105278 ... 56.714146 31.158018 45.438164 34.562592 33.451321 48.411022 45.710072 46.708466 45.250542 42.855175 5411 0.277062 79.789001 0.000000 1.317072 3.428295 18.851551 11.727011 0.629307 1.981982 19.088093 ... 28.712433 61.289993 52.007412 59.782089 58.536476 47.875999 49.678215 50.149368 57.499794 56.440426 17868 1.513100 79.838013 1.317072 0.000000 2.190433 17.535700 11.202190 1.397858 1.630720 17.772236 ... 28.115082 60.881992 51.348293 59.303776 58.096458 47.229881 49.035213 49.474678 56.810215 55.807220 25037 3.661949 80.411301 3.428295 2.190433 0.000000 15.621014 11.494502 3.581764 2.825583 15.833551 ... 27.726255 60.813461 50.848087 59.064812 57.931202 46.883896 48.574974 48.897926 56.227928 55.343788 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9083 47.874050 48.411022 47.875999 47.229881 46.883896 41.692081 36.861530 47.601456 46.348061 42.128132 ... 20.998363 19.081427 9.226923 18.280758 17.826046 0.000000 7.776943 11.396736 14.017366 12.323303 13305 49.704773 45.710072 49.678215 49.035213 48.574974 43.554199 39.517002 49.493969 47.980606 43.960644 ... 21.178249 15.883133 2.486704 13.015112 12.991101 7.776943 0.000000 3.934742 8.345553 6.825695 6293 50.195679 46.708466 50.149368 49.474678 48.897926 43.544212 40.496021 50.010254 48.375713 43.923611 ... 21.549055 17.343016 3.761866 13.325891 13.700439 11.396736 3.934742 0.000000 7.611817 7.308194 10909 57.541214 45.250542 57.499794 56.810215 56.227928 50.198162 47.537022 57.339027 55.753185 50.565315 ... 28.959011 14.759328 5.861502 10.763541 12.184548 14.017366 8.345553 7.611817 0.000000 3.275540 6747 56.467831 42.855175 56.440426 55.807220 55.343788 50.079926 46.323112 56.264225 54.730919 50.471375 ... 27.840965 12.077148 4.614255 8.590357 9.616536 12.323303 6.825695 7.308194 3.275540 0.000000 <p>937 rows \u00d7 937 columns</p> In\u00a0[42]: Copied! <pre>intertemplate_distances = compute_distances(\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values,\n    selected_events_meta[\"depth_km\"].values,\n    selected_events_meta[\"longitude\"].values,\n    selected_events_meta[\"latitude\"].values, \n    selected_events_meta[\"depth_km\"].values,\n    backend=\"cartopy\"\n)\n\nintertemplate_distances = pd.DataFrame(\n        index=template_ids.values, columns=template_ids.values, data=intertemplate_distances\n    )\n\nintertemplate_distances\n</pre> intertemplate_distances = compute_distances(     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,     selected_events_meta[\"depth_km\"].values,     selected_events_meta[\"longitude\"].values,     selected_events_meta[\"latitude\"].values,      selected_events_meta[\"depth_km\"].values,     backend=\"cartopy\" )  intertemplate_distances = pd.DataFrame(         index=template_ids.values, columns=template_ids.values, data=intertemplate_distances     )  intertemplate_distances Out[42]: 6720 17122 5411 17868 25037 6929 7607 13098 15698 4380 ... 3733 11897 910 341 5934 9083 13305 6293 10909 6747 6720 0.000000 79.720917 0.277062 1.513100 3.661949 19.026285 11.666162 0.485678 2.143338 19.266460 ... 28.748285 61.275852 52.038624 59.792736 58.538982 47.874050 49.704773 50.195683 57.541214 56.467831 17122 79.720917 0.000000 79.789001 79.838013 80.411301 82.627167 71.581108 79.649605 78.469719 83.105278 ... 56.714146 31.158018 45.438164 34.562592 33.451321 48.411022 45.710072 46.708466 45.250542 42.855175 5411 0.277062 79.789001 0.000000 1.317072 3.428295 18.851551 11.727011 0.629307 1.981982 19.088093 ... 28.712433 61.289993 52.007412 59.782089 58.536476 47.875999 49.678215 50.149368 57.499794 56.440426 17868 1.513100 79.838013 1.317072 0.000000 2.190433 17.535700 11.202190 1.397858 1.630720 17.772236 ... 28.115082 60.881992 51.348293 59.303776 58.096458 47.229881 49.035213 49.474678 56.810215 55.807220 25037 3.661949 80.411301 3.428295 2.190433 0.000000 15.621014 11.494502 3.581764 2.825583 15.833551 ... 27.726255 60.813461 50.848087 59.064812 57.931202 46.883900 48.574970 48.897926 56.227928 55.343788 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9083 47.874050 48.411022 47.875999 47.229881 46.883900 41.692085 36.861530 47.601456 46.348061 42.128132 ... 20.998363 19.081427 9.226923 18.280758 17.826046 0.000000 7.776943 11.396736 14.017366 12.323303 13305 49.704773 45.710072 49.678215 49.035213 48.574970 43.554199 39.517002 49.493969 47.980606 43.960644 ... 21.178249 15.883133 2.486704 13.015112 12.991101 7.776943 0.000000 3.934742 8.345553 6.825695 6293 50.195683 46.708466 50.149368 49.474678 48.897926 43.544212 40.496021 50.010254 48.375717 43.923611 ... 21.549055 17.343016 3.761866 13.325892 13.700439 11.396736 3.934742 0.000000 7.611817 7.308194 10909 57.541214 45.250542 57.499794 56.810215 56.227928 50.198162 47.537022 57.339027 55.753185 50.565315 ... 28.959011 14.759328 5.861502 10.763542 12.184548 14.017366 8.345553 7.611817 0.000000 3.275540 6747 56.467831 42.855175 56.440426 55.807220 55.343788 50.079926 46.323112 56.264225 54.730919 50.471375 ... 27.840967 12.077148 4.614255 8.590357 9.616536 12.323303 6.825695 7.308194 3.275540 0.000000 <p>937 rows \u00d7 937 columns</p> In\u00a0[43]: Copied! <pre>def compute_intertemplate_cc(\n    template_waveforms_arr,\n    max_lag=5,\n):\n    \"\"\"\n    Compute the pairwise template cross-correlations (CCs).\n\n    Parameters\n    ----------\n    max_lag : int, default to 5\n        The maximum lag, in samples, allowed when searching for the maximum\n        CC on each channel. This parameter accounts for small discrepancies\n        in windowing that could occur for two templates highly similar but\n        associated with slightly different locations.\n        reading a potentially large file.\n\n    Returns\n    -------\n    intertemplate_cc : numpy.ndarray\n        The computed inter-template correlation coefficients.\n    \"\"\"\n    # format arrays for FMF\n    data_arr = template_waveforms_arr.copy()\n    template_arr = template_waveforms_arr[..., max_lag:-max_lag]\n    moveouts_arr = np.zeros(template_waveforms_arr.shape[:-1], dtype=np.int32)\n    num_templates = template_waveforms_arr.shape[0]\n    intertp_cc = np.zeros(\n        (num_templates, num_templates), dtype=np.float32\n    )\n    # use FMF on one template at a time against all others\n    for t in range(num_templates):\n        weights = np.ones(template_arr.shape[:-1], dtype=np.float32)\n        for s in range(template_arr.shape[1]):\n            for c in range(template_arr.shape[2]):\n                if np.sum(template_arr[t, s, c, :]) == 0.:\n                    weights[t, s, c] = 0.\n        weights /= np.sum(weights, axis=(1, 2), keepdims=True)\n        keep = np.sum(weights != 0.0, axis=(1, 2)) &gt; 0\n        cc = fmf.matched_filter(\n            template_arr[keep, ...],\n            moveouts_arr[keep, ...],\n            weights[keep, ...],\n            data_arr[t, ...],\n            1,\n            arch=\"cpu\",\n            network_sum=False,\n            check_zeros=False,\n        )\n        intertp_cc[t, keep] = np.sum(\n            weights[keep, ...] * np.max(cc, axis=1), axis=(-1, -2)\n        )\n    # make the CC matrix symmetric by averaging the lower\n    # and upper triangles\n    intertemplate_cc = (intertp_cc + intertp_cc.T) / 2.0\n    return intertemplate_cc\n</pre> def compute_intertemplate_cc(     template_waveforms_arr,     max_lag=5, ):     \"\"\"     Compute the pairwise template cross-correlations (CCs).      Parameters     ----------     max_lag : int, default to 5         The maximum lag, in samples, allowed when searching for the maximum         CC on each channel. This parameter accounts for small discrepancies         in windowing that could occur for two templates highly similar but         associated with slightly different locations.         reading a potentially large file.      Returns     -------     intertemplate_cc : numpy.ndarray         The computed inter-template correlation coefficients.     \"\"\"     # format arrays for FMF     data_arr = template_waveforms_arr.copy()     template_arr = template_waveforms_arr[..., max_lag:-max_lag]     moveouts_arr = np.zeros(template_waveforms_arr.shape[:-1], dtype=np.int32)     num_templates = template_waveforms_arr.shape[0]     intertp_cc = np.zeros(         (num_templates, num_templates), dtype=np.float32     )     # use FMF on one template at a time against all others     for t in range(num_templates):         weights = np.ones(template_arr.shape[:-1], dtype=np.float32)         for s in range(template_arr.shape[1]):             for c in range(template_arr.shape[2]):                 if np.sum(template_arr[t, s, c, :]) == 0.:                     weights[t, s, c] = 0.         weights /= np.sum(weights, axis=(1, 2), keepdims=True)         keep = np.sum(weights != 0.0, axis=(1, 2)) &gt; 0         cc = fmf.matched_filter(             template_arr[keep, ...],             moveouts_arr[keep, ...],             weights[keep, ...],             data_arr[t, ...],             1,             arch=\"cpu\",             network_sum=False,             check_zeros=False,         )         intertp_cc[t, keep] = np.sum(             weights[keep, ...] * np.max(cc, axis=1), axis=(-1, -2)         )     # make the CC matrix symmetric by averaging the lower     # and upper triangles     intertemplate_cc = (intertp_cc + intertp_cc.T) / 2.0     return intertemplate_cc In\u00a0[44]: Copied! <pre>intertemplate_cc = compute_intertemplate_cc(\n    template_waveforms_arr.astype(np.float32),\n)\n\nintertemplate_cc = pd.DataFrame(\n        index=template_ids.values, columns=template_ids.values, data=intertemplate_cc\n    )\n\nintertemplate_cc\n</pre> intertemplate_cc = compute_intertemplate_cc(     template_waveforms_arr.astype(np.float32), )  intertemplate_cc = pd.DataFrame(         index=template_ids.values, columns=template_ids.values, data=intertemplate_cc     )  intertemplate_cc Out[44]: 6720 17122 5411 17868 25037 6929 7607 13098 15698 4380 ... 3733 11897 910 341 5934 9083 13305 6293 10909 6747 6720 1.000000 0.027428 0.248301 0.128720 0.119031 0.068705 0.075539 0.196440 0.097394 0.066785 ... 0.064462 0.033454 0.030623 0.028775 0.030853 0.029237 0.033768 0.050524 0.047637 0.041777 17122 0.027428 1.000000 0.026761 0.026829 0.032013 0.024978 0.027899 0.025736 0.033046 0.024862 ... 0.034811 0.065602 0.056293 0.059984 0.072702 0.045179 0.052081 0.050459 0.045123 0.051460 5411 0.248301 0.026761 1.000000 0.139320 0.110125 0.075083 0.072549 0.193988 0.095849 0.074843 ... 0.070623 0.039743 0.033977 0.025233 0.034048 0.033736 0.033273 0.052135 0.052709 0.045557 17868 0.128720 0.026829 0.139320 1.000000 0.130679 0.056800 0.071021 0.148250 0.092264 0.071087 ... 0.055197 0.030743 0.028529 0.020380 0.032241 0.030409 0.034576 0.043859 0.038458 0.029215 25037 0.119031 0.032013 0.110125 0.130679 1.000000 0.062907 0.069270 0.133696 0.102896 0.064461 ... 0.053312 0.028509 0.036324 0.026229 0.027577 0.032606 0.033975 0.047996 0.037807 0.040282 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9083 0.029237 0.045179 0.033736 0.030409 0.032606 0.037664 0.034719 0.033916 0.026739 0.049771 ... 0.052088 0.074603 0.081570 0.078450 0.060359 1.000000 0.062585 0.057824 0.065711 0.062075 13305 0.033768 0.052081 0.033273 0.034576 0.033975 0.037316 0.027540 0.031308 0.034490 0.037897 ... 0.053978 0.085699 0.073243 0.062671 0.073228 0.062585 1.000000 0.069554 0.082031 0.085256 6293 0.050524 0.050459 0.052135 0.043859 0.047996 0.051520 0.043395 0.048264 0.045893 0.059161 ... 0.067654 0.086148 0.064724 0.076438 0.078336 0.057824 0.069554 1.000000 0.079080 0.071196 10909 0.047637 0.045123 0.052709 0.038458 0.037807 0.055493 0.049102 0.044462 0.046792 0.054465 ... 0.072801 0.083891 0.083470 0.063200 0.072974 0.065711 0.082031 0.079080 1.000000 0.096372 6747 0.041777 0.051460 0.045557 0.029215 0.040282 0.047280 0.034376 0.042467 0.036025 0.045457 ... 0.048389 0.093624 0.081760 0.079670 0.080193 0.062075 0.085256 0.071196 0.096372 1.000000 <p>937 rows \u00d7 937 columns</p> In\u00a0[45]: Copied! <pre>def flag_multiples(\n    catalog,\n    intertemplate_distances,\n    intertemplate_cc,\n    dt_criterion=4.0,\n    distance_criterion=15.,\n    similarity_criterion=-1.0,\n):\n    \"\"\"\n    Search for events detected by multiple templates and flag them.\n\n    Parameters\n    ----------\n    catalog : pd.DataFrame\n    dt_criterion : float, optional\n        The time interval, in seconds, under which two events are examined for redundancy.\n    distance_criterion : float, optional\n        The inter-event distance, in kilometers, under which two events are examined for redundancy.\n    similarity_criterion : float, optional\n        The template similarity threshold, in terms of average cc, over which two events\n        are examined for redundancy. The default value of -1 means that similarity is not\n        taken into account.\n    intertemplate_cc : pd.DataFrame\n    progress : bool, optional\n        If True, print progress bar with `tqdm`.\n    **kwargs\n        Additional keyword arguments.\n\n    Returns\n    -------\n    unique_event : numpy.ndarray\n        A (`len(detection_times)`,) `numpy.ndarray` of booleans with `unique_event[i]=True`\n        if event number `i` is a unique detection of a single event.\n    \"\"\"\n    detection_time_sec = catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.\n    interevent_time = np.hstack(\n        ([1.10 * dt_criterion], detection_time_sec[1:] - detection_time_sec[:-1])\n    )\n    # -----------------------------------\n    n_events = len(catalog)\n    unique_event = np.ones(n_events, dtype=bool)\n    for n1 in range(n_events):\n        if not unique_event[n1]:\n            # was already flagged as a multiple\n            continue\n        tid1 = catalog[\"tid\"].iloc[n1]\n        # apply the time criterion\n        n2 = n1 + 1\n        if n2 &lt; n_events:\n            dt_n1n2 = interevent_time[n2]\n        else:\n            continue\n        temporal_neighbors = [n1]\n        while dt_n1n2 &lt; dt_criterion:\n            temporal_neighbors.append(n2)\n            n2 += 1\n            if n2 &gt;= n_events:\n                break\n            dt_n1n2 += interevent_time[n2]\n        temporal_neighbors = np.array(temporal_neighbors).astype(\"int64\")\n        if len(temporal_neighbors) == 1:\n            # did not find any temporal neighbors\n            continue\n        # remove events that were already flagged as non unique\n        temporal_neighbors = temporal_neighbors[unique_event[temporal_neighbors]]\n        candidates = temporal_neighbors\n        if len(candidates) == 1:\n            continue\n        tids_candidates = catalog[\"tid\"].values[temporal_neighbors]\n        \n        similarities = intertemplate_cc.loc[tid1, tids_candidates].values\n        \n        distances = intertemplate_distances.loc[tid1, tids_candidates].values\n        \n        multiples = candidates[\n            np.where(\n                (similarities &gt;= similarity_criterion)\n                &amp; (distances &lt;= distance_criterion)\n                )[0]\n        ]\n        unique_event[multiples] = False\n        # find best CC and keep it\n        ccs = catalog[\"cc\"].values[multiples]\n        best_cc = multiples[ccs.argmax()]\n        unique_event[best_cc] = True\n    # -------------------------------------------\n    return unique_event\n</pre> def flag_multiples(     catalog,     intertemplate_distances,     intertemplate_cc,     dt_criterion=4.0,     distance_criterion=15.,     similarity_criterion=-1.0, ):     \"\"\"     Search for events detected by multiple templates and flag them.      Parameters     ----------     catalog : pd.DataFrame     dt_criterion : float, optional         The time interval, in seconds, under which two events are examined for redundancy.     distance_criterion : float, optional         The inter-event distance, in kilometers, under which two events are examined for redundancy.     similarity_criterion : float, optional         The template similarity threshold, in terms of average cc, over which two events         are examined for redundancy. The default value of -1 means that similarity is not         taken into account.     intertemplate_cc : pd.DataFrame     progress : bool, optional         If True, print progress bar with `tqdm`.     **kwargs         Additional keyword arguments.      Returns     -------     unique_event : numpy.ndarray         A (`len(detection_times)`,) `numpy.ndarray` of booleans with `unique_event[i]=True`         if event number `i` is a unique detection of a single event.     \"\"\"     detection_time_sec = catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.     interevent_time = np.hstack(         ([1.10 * dt_criterion], detection_time_sec[1:] - detection_time_sec[:-1])     )     # -----------------------------------     n_events = len(catalog)     unique_event = np.ones(n_events, dtype=bool)     for n1 in range(n_events):         if not unique_event[n1]:             # was already flagged as a multiple             continue         tid1 = catalog[\"tid\"].iloc[n1]         # apply the time criterion         n2 = n1 + 1         if n2 &lt; n_events:             dt_n1n2 = interevent_time[n2]         else:             continue         temporal_neighbors = [n1]         while dt_n1n2 &lt; dt_criterion:             temporal_neighbors.append(n2)             n2 += 1             if n2 &gt;= n_events:                 break             dt_n1n2 += interevent_time[n2]         temporal_neighbors = np.array(temporal_neighbors).astype(\"int64\")         if len(temporal_neighbors) == 1:             # did not find any temporal neighbors             continue         # remove events that were already flagged as non unique         temporal_neighbors = temporal_neighbors[unique_event[temporal_neighbors]]         candidates = temporal_neighbors         if len(candidates) == 1:             continue         tids_candidates = catalog[\"tid\"].values[temporal_neighbors]                  similarities = intertemplate_cc.loc[tid1, tids_candidates].values                  distances = intertemplate_distances.loc[tid1, tids_candidates].values                  multiples = candidates[             np.where(                 (similarities &gt;= similarity_criterion)                 &amp; (distances &lt;= distance_criterion)                 )[0]         ]         unique_event[multiples] = False         # find best CC and keep it         ccs = catalog[\"cc\"].values[multiples]         best_cc = multiples[ccs.argmax()]         unique_event[best_cc] = True     # -------------------------------------------     return unique_event In\u00a0[46]: Copied! <pre>SIMILARITY_CRITERION = 0.1\nDT_CRITERION = INTEREVENT_TIME_RESOLUTION_SEC # re-use same time resolution as before\nDISTANCE_CRITERION = 15.0\n\n# if the inter-template cc value is below this value, and there are two or more detections that are close in time to each other, these will NOT be flagged as multiples. \n# So they *might* be discrete events... We need to do some tests to see how sensitive the catalog looks to this parameter ?  \nunique_event = flag_multiples(\n    catalog,\n    intertemplate_distances,\n    intertemplate_cc,\n    dt_criterion=DT_CRITERION,\n    similarity_criterion=SIMILARITY_CRITERION,\n    distance_criterion=DISTANCE_CRITERION,\n)\n\ncatalog[\"unique_event\"] = unique_event\n\ncatalog\n</pre> SIMILARITY_CRITERION = 0.1 DT_CRITERION = INTEREVENT_TIME_RESOLUTION_SEC # re-use same time resolution as before DISTANCE_CRITERION = 15.0  # if the inter-template cc value is below this value, and there are two or more detections that are close in time to each other, these will NOT be flagged as multiples.  # So they *might* be discrete events... We need to do some tests to see how sensitive the catalog looks to this parameter ?   unique_event = flag_multiples(     catalog,     intertemplate_distances,     intertemplate_cc,     dt_criterion=DT_CRITERION,     similarity_criterion=SIMILARITY_CRITERION,     distance_criterion=DISTANCE_CRITERION, )  catalog[\"unique_event\"] = unique_event  catalog Out[46]: detection_time cc normalized_cc tid longitude latitude depth interevent_time_s return_time_s unique_event 0 2019-07-04 00:46:45.640 1.000000 5.208948 6720 -117.882570 36.091088 4.643969 NaN NaN True 41 2019-07-04 00:46:45.640 0.629913 3.102449 13098 -117.879256 36.090409 4.268292 0.00 NaN False 19 2019-07-04 00:46:45.640 0.339081 1.628305 17868 -117.866468 36.093520 4.981447 0.00 NaN False 10 2019-07-04 00:46:45.640 0.716178 4.297031 5411 -117.880902 36.091986 4.854336 0.00 NaN False 26 2019-07-04 00:46:45.680 0.276461 1.259136 25037 -117.846320 36.100386 5.943363 0.04 NaN False ... ... ... ... ... ... ... ... ... ... ... 2958 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 4.72 3658.12 True 7529 2019-07-04 23:59:22.160 0.191812 1.316138 381 -117.501536 35.715034 11.589107 5.28 4977.76 True 7192 2019-07-04 23:59:31.640 0.193527 1.305970 127 -117.515593 35.703031 10.744590 9.48 3191.20 False 8293 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 0.08 231.72 True 7160 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 2.44 243.84 True <p>9114 rows \u00d7 10 columns</p> In\u00a0[47]: Copied! <pre>catalog[catalog[\"unique_event\"]]\n</pre> catalog[catalog[\"unique_event\"]] Out[47]: detection_time cc normalized_cc tid longitude latitude depth interevent_time_s return_time_s unique_event 0 2019-07-04 00:46:45.640 1.000000 5.208948 6720 -117.882570 36.091088 4.643969 NaN NaN True 11 2019-07-04 00:47:26.920 0.320361 1.922152 5411 -117.880902 36.091986 4.854336 0.00 41.28 True 9 2019-07-04 00:55:33.360 1.000000 6.145690 17122 -117.799226 35.378160 11.078458 486.40 NaN True 12 2019-07-04 00:56:35.560 1.000000 5.999952 5411 -117.880902 36.091986 4.854336 62.20 548.64 True 22 2019-07-04 02:00:37.440 1.000000 4.802108 17868 -117.866468 36.093520 4.981447 0.00 3841.88 True ... ... ... ... ... ... ... ... ... ... ... 1608 2019-07-04 23:59:12.160 0.167771 1.122184 8143 -117.536542 35.732510 10.353954 0.04 19694.52 True 2958 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 4.72 3658.12 True 7529 2019-07-04 23:59:22.160 0.191812 1.316138 381 -117.501536 35.715034 11.589107 5.28 4977.76 True 8293 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 0.08 231.72 True 7160 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 2.44 243.84 True <p>3193 rows \u00d7 10 columns</p> In\u00a0[48]: Copied! <pre>cat_delumped = catalog[catalog[\"unique_event\"]]\ninterevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()\n\nfig, ax = plt.subplots(num=\"interevent_time_vs_detection_time\", figsize=(18, 7))\n\nax.scatter(cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50)\nax.axhline(1. / SAMPLING_RATE_HZ, ls=\"--\", color=\"k\", label=\"1 sample\")\nax.set_xlabel(\"Detection time\")\nax.set_ylabel(\"Interevent time (s)\")\nax.set_title(\"Interevent time after 'delumping' vs detection time\")\n\nax.grid()\ncbar = plt.colorbar(ax.collections[0], ax=ax)\ncbar.set_label(\"Normalized CC\")\nax.set_yscale(\"log\")\nax.legend(loc=\"upper right\")\n</pre> cat_delumped = catalog[catalog[\"unique_event\"]] interevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()  fig, ax = plt.subplots(num=\"interevent_time_vs_detection_time\", figsize=(18, 7))  ax.scatter(cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50) ax.axhline(1. / SAMPLING_RATE_HZ, ls=\"--\", color=\"k\", label=\"1 sample\") ax.set_xlabel(\"Detection time\") ax.set_ylabel(\"Interevent time (s)\") ax.set_title(\"Interevent time after 'delumping' vs detection time\")  ax.grid() cbar = plt.colorbar(ax.collections[0], ax=ax) cbar.set_label(\"Normalized CC\") ax.set_yscale(\"log\") ax.legend(loc=\"upper right\") Out[48]: <pre>&lt;matplotlib.legend.Legend at 0x7f3b4c1cdbd0&gt;</pre> In\u00a0[49]: Copied! <pre>cat_delumped = catalog[catalog[\"unique_event\"]]\ninterevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()\n\nfig, axes = plt.subplots(num=\"delumped_interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))\n\naxes[0].scatter(\n    cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[0].set_xlabel(\"Detection time\")\naxes[0].set_ylabel(\"Interevent time (s)\")\naxes[0].set_title(\"'Delumped' interevent time vs detection time\")\n\naxes[1].scatter(\n    catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],\n    linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2\n    )\naxes[1].set_xlabel(\"Detection time\")\naxes[1].set_ylabel(\"Return time (s)\")\naxes[1].set_title(\"Return time vs detection time\")\n\nfor ax in axes:\n    ax.grid()\n    cbar = plt.colorbar(ax.collections[0], ax=ax)\n    cbar.set_label(\"CC\")\n    ax.set_yscale(\"log\")\n</pre> cat_delumped = catalog[catalog[\"unique_event\"]] interevent_times_delumped_cat = cat_delumped[\"detection_time\"].diff().dt.total_seconds()  fig, axes = plt.subplots(num=\"delumped_interevent_time_vs_detection_time\", nrows=2, figsize=(18, 14))  axes[0].scatter(     cat_delumped[\"detection_time\"], interevent_times_delumped_cat, c=cat_delumped[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[0].set_xlabel(\"Detection time\") axes[0].set_ylabel(\"Interevent time (s)\") axes[0].set_title(\"'Delumped' interevent time vs detection time\")  axes[1].scatter(     catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"],     linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50, zorder=2     ) axes[1].set_xlabel(\"Detection time\") axes[1].set_ylabel(\"Return time (s)\") axes[1].set_title(\"Return time vs detection time\")  for ax in axes:     ax.grid()     cbar = plt.colorbar(ax.collections[0], ax=ax)     cbar.set_label(\"CC\")     ax.set_yscale(\"log\") <p>Note how the plots of inter-event times and return times differ even after \"delumping\" the catalog. In the process of delumping, we make arbitrary choices (see the criteria above) and we end up discarding some real events and keeping some repeats of the same event. Thus, some valuable information may be lost in the process. The pre-delumping catalog should always be kept because some studies may be well-suited for a template-by-template analysis, or may not be negatively affected by the presence of redundant detections, thus making optimal use of the template matching catalog.</p> In\u00a0[44]: Copied! <pre># first, extract clips from the continuous seismograms\ndetected_event_waveforms_all_channels = np.zeros(\n    (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32\n)\nfor i in range(len(catalog)):\n    tid = catalog[\"tid\"].iloc[i]\n    t =template_ids.tolist().index(tid)\n    for s in range(num_stations):\n        for c in range(num_channels):\n            if moveouts_samp_arr[t, s, c] &lt; 0:\n                continue\n            detection_time_samp = (\n                catalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)\n            ).total_seconds() * SAMPLING_RATE_HZ\n            idx_start = int(detection_time_samp) * FMF_STEP_SAMP + moveouts_samp_arr[t, s, c]\n            idx_end = idx_start + TEMPLATE_DURATION_SAMP\n            detected_event_waveforms_all_channels[i, s, c, :] = (\n                continuous_seismograms_arr[s, c, idx_start:idx_end]\n                )\n</pre> # first, extract clips from the continuous seismograms detected_event_waveforms_all_channels = np.zeros(     (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32 ) for i in range(len(catalog)):     tid = catalog[\"tid\"].iloc[i]     t =template_ids.tolist().index(tid)     for s in range(num_stations):         for c in range(num_channels):             if moveouts_samp_arr[t, s, c] &lt; 0:                 continue             detection_time_samp = (                 catalog[\"detection_time\"].iloc[i] - pd.Timestamp(date)             ).total_seconds() * SAMPLING_RATE_HZ             idx_start = int(detection_time_samp) * FMF_STEP_SAMP + moveouts_samp_arr[t, s, c]             idx_end = idx_start + TEMPLATE_DURATION_SAMP             detected_event_waveforms_all_channels[i, s, c, :] = (                 continuous_seismograms_arr[s, c, idx_start:idx_end]                 )  In\u00a0[45]: Copied! <pre>m_rel = np.zeros(len(catalog), dtype=np.float32)\n\n# then, measure the peak amplitudes and apply the above formula to get the relative magnitudes\npeak_amplitudes = np.max(np.abs(detected_event_waveforms_all_channels), axis=-1)\n\nfor tid in catalog[\"tid\"].unique():\n    t = selected_events_meta[\"event_index\"].tolist().index(tid)\n    m_ref = selected_events_meta[\"magnitude\"].iloc[t]\n    ref_event_index = np.where(\n        (catalog[\"tid\"] == tid) &amp; (catalog[\"cc\"]&gt; 0.99)\n    )[0][0]\n    peak_amplitudes_ref = peak_amplitudes[ref_event_index, ...]\n    template_subcat = catalog[catalog[\"tid\"] == tid]\n    \n    amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n    invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))\n    amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)\n    \n    catalog.loc[template_subcat.index, \"m_rel\"] = (\n        m_ref + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))\n    )\ncatalog\n</pre> m_rel = np.zeros(len(catalog), dtype=np.float32)  # then, measure the peak amplitudes and apply the above formula to get the relative magnitudes peak_amplitudes = np.max(np.abs(detected_event_waveforms_all_channels), axis=-1)  for tid in catalog[\"tid\"].unique():     t = selected_events_meta[\"event_index\"].tolist().index(tid)     m_ref = selected_events_meta[\"magnitude\"].iloc[t]     ref_event_index = np.where(         (catalog[\"tid\"] == tid) &amp; (catalog[\"cc\"]&gt; 0.99)     )[0][0]     peak_amplitudes_ref = peak_amplitudes[ref_event_index, ...]     template_subcat = catalog[catalog[\"tid\"] == tid]          amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref     invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))     amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)          catalog.loc[template_subcat.index, \"m_rel\"] = (         m_ref + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))     ) catalog <pre>/tmp/ipykernel_1041927/1462564979.py:15: RuntimeWarning: invalid value encountered in true_divide\n  amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n/tmp/ipykernel_1041927/1462564979.py:15: RuntimeWarning: invalid value encountered in true_divide\n  amplitude_ratios = peak_amplitudes[catalog[\"tid\"] == tid, ...] / peak_amplitudes_ref\n</pre> Out[45]: detection_time cc normalized_cc tid longitude latitude depth interevent_time_s return_time_s unique_event m_rel 0 2019-07-04 00:46:45.640 1.000000 5.208948 6720 -117.882570 36.091088 4.643969 NaN NaN True 0.595368 19 2019-07-04 00:46:45.640 0.339081 1.628305 17868 -117.866468 36.093520 4.981447 0.00 NaN False 0.302823 10 2019-07-04 00:46:45.640 0.716178 4.297031 5411 -117.880902 36.091986 4.854336 0.00 NaN False 0.674579 41 2019-07-04 00:46:45.640 0.629913 3.102449 13098 -117.879256 36.090409 4.268292 0.00 NaN False 0.632657 26 2019-07-04 00:46:45.680 0.276461 1.259136 25037 -117.846320 36.100386 5.943363 0.04 NaN False 0.349690 ... ... ... ... ... ... ... ... ... ... ... ... 2960 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 4.72 3658.12 True -0.915777 7534 2019-07-04 23:59:22.160 0.175081 1.200479 381 -117.501536 35.715034 11.589107 5.28 1092.96 True -0.774148 7197 2019-07-04 23:59:31.640 0.193527 1.305970 127 -117.515593 35.703031 10.744590 9.48 3191.20 False -1.156079 8298 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 0.08 231.72 True -1.250341 7165 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 2.44 243.84 True -1.526509 <p>9118 rows \u00d7 11 columns</p> <p>Next, we plot the distribution of earthquake magnitudes. The cumulative distribution usually follows the so-called Gutenberg-Richter law: $$ \\log N(m \\geq M) = a - b M$$</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8))\naxb = ax.twinx()\nax.set_title(\"Distribution of earthquake magnitudes\")\n\n# cut off the last events of the day because those are in the tapered part of the continuous seismograms\n_cat = catalog[(catalog[\"detection_time\"] &lt; \"2019-07-04T23:45:00\") &amp; (catalog[\"unique_event\"])]\n\ncount, m_bins, _ = ax.hist(_cat[\"m_rel\"], bins=15, color=\"k\", alpha=0.5)\n\n# calculate the b-value\nm_midbins = 0.5 * (m_bins[:-1] + m_bins[1:])\n# estimate the magnitude of completeness with the maximum curvature method (Mc is taken as the mode of the magnitude distribution)\nm_c = m_midbins[count.argmax()]\nm_above_Mc = _cat[\"m_rel\"][_cat[\"m_rel\"] &gt; m_c].values\nm_c_w_buffer = m_c + 0.1\n# estimate the b-value with the maximum likelihood method\nbvalue = 1.0 / (np.log(10) * np.mean(m_above_Mc - m_c_w_buffer))\n\naxb.plot(np.sort(_cat[\"m_rel\"]), np.arange(len(_cat))[::-1], color=\"k\", lw=2, label=f\"MLE b-value: {bvalue:.2f}\")\n\naxb.axvline(m_c, label=r\"$M_c=$\"f\"{m_c:.2f}\", color=\"k\", ls=\"--\")\n\nax.set_xlabel(\"Magnitude\")\nax.set_ylabel(\"Event Count\")\naxb.set_ylabel(\"Cumulative Event Count\")\naxb.legend(loc=\"lower left\")\n\nfor ax in [ax, axb]:\n    ax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8)) axb = ax.twinx() ax.set_title(\"Distribution of earthquake magnitudes\")  # cut off the last events of the day because those are in the tapered part of the continuous seismograms _cat = catalog[(catalog[\"detection_time\"] &lt; \"2019-07-04T23:45:00\") &amp; (catalog[\"unique_event\"])]  count, m_bins, _ = ax.hist(_cat[\"m_rel\"], bins=15, color=\"k\", alpha=0.5)  # calculate the b-value m_midbins = 0.5 * (m_bins[:-1] + m_bins[1:]) # estimate the magnitude of completeness with the maximum curvature method (Mc is taken as the mode of the magnitude distribution) m_c = m_midbins[count.argmax()] m_above_Mc = _cat[\"m_rel\"][_cat[\"m_rel\"] &gt; m_c].values m_c_w_buffer = m_c + 0.1 # estimate the b-value with the maximum likelihood method bvalue = 1.0 / (np.log(10) * np.mean(m_above_Mc - m_c_w_buffer))  axb.plot(np.sort(_cat[\"m_rel\"]), np.arange(len(_cat))[::-1], color=\"k\", lw=2, label=f\"MLE b-value: {bvalue:.2f}\")  axb.axvline(m_c, label=r\"$M_c=$\"f\"{m_c:.2f}\", color=\"k\", ls=\"--\")  ax.set_xlabel(\"Magnitude\") ax.set_ylabel(\"Event Count\") axb.set_ylabel(\"Cumulative Event Count\") axb.legend(loc=\"lower left\")  for ax in [ax, axb]:     ax.set_yscale(\"log\") <p>Note: the physical meaning of the b-value is better understood when the Gutenberg-Richter law is recast in terms of a power-law of the seismic moment, $M_0$. Assuming we have the following scaling relationship between magnitude and seismic moment: $M \\sim c \\log M_0$, then: $$ \\log N \\sim b c \\log M_0 \\Leftrightarrow N \\sim M_0^{bc}.$$ The value of the exponent $bc$ is fixed by the physical properties of the system. Thus, it is clear that the b-value depends on the choice of the magnitude scale.</p> <p>Moreover, one must keep in mind that such a scaling relationship, $M \\sim c \\log M_0$, may not hold over the entire magnitude range. Moment magnitudes, $M_w$, were introduced to make sure that magnitudes and seismic moments were related through a unique scaling relationship for any magnitudes, but the relative magnitudes used here hardly ensure such a consistent scaling. Relative magnitudes, therefore, should not be used for thorough analyses of the b-value (and things like time variations of the b-value).</p> In\u00a0[47]: Copied! <pre>deep_learning_cat = event_meta[\n    (event_meta[\"time\"] &gt; \"2019-07-04\")\n    &amp; (event_meta[\"time\"] &lt; \"2019-07-05\")\n].copy()\ndeep_learning_cat[\"time\"] = pd.to_datetime(deep_learning_cat[\"time\"])\ndeep_learning_cat\n</pre> deep_learning_cat = event_meta[     (event_meta[\"time\"] &gt; \"2019-07-04\")     &amp; (event_meta[\"time\"] &lt; \"2019-07-05\") ].copy() deep_learning_cat[\"time\"] = pd.to_datetime(deep_learning_cat[\"time\"]) deep_learning_cat Out[47]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 978 2019-07-04 23:57:57.991578467 0.860503 0.046175 18 0.884022 0.134415 13748 -117.593034 35.768763 8.755313 979 2019-07-04 23:58:33.755557349 0.841734 0.253947 43 1.213073 0.123163 8383 -117.549675 35.741730 2.446929 980 2019-07-04 23:58:48.057018725 0.878082 0.039899 65 1.410004 0.082498 2445 -117.509182 35.716748 10.672514 981 2019-07-04 23:59:20.859908653 0.675912 0.025560 27 1.293314 0.124839 21156 -117.504724 35.713536 11.309137 982 2019-07-04 23:59:44.676697534 0.852219 0.048873 43 1.277199 0.110752 12104 -117.557233 35.746969 9.001408 <p>983 rows \u00d7 10 columns</p> In\u00a0[48]: Copied! <pre>deep_learning_cat[\"time_sec\"] = (\n    deep_learning_cat[\"time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.\n)\ncatalog[\"detection_time_sec\"] = (\n    catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000.\n)\n</pre> deep_learning_cat[\"time_sec\"] = (     deep_learning_cat[\"time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000. ) catalog[\"detection_time_sec\"] = (     catalog[\"detection_time\"].values.astype(\"datetime64[ms]\").astype(\"float64\") / 1000. ) In\u00a0[49]: Copied! <pre># find SCSN events in the BPMF catalog\nMAX_ORIGIN_TIME_DIFFERENCE_SEC = 5.\nMAX_DISTANCE_KM = 15.\nVELOCITY_MS = 3500.\n\ncat_u = catalog[catalog[\"unique_event\"]]\n\nmatching_event_id = np.zeros(len(deep_learning_cat), dtype=object)\nunmatched = np.ones(len(cat_u), dtype=bool)\nfor i in range(len(deep_learning_cat)):\n    delta_t = np.abs(deep_learning_cat.iloc[i][\"time_sec\"] - cat_u[\"detection_time_sec\"].values)\n    time_neighbors = (delta_t &lt;= MAX_ORIGIN_TIME_DIFFERENCE_SEC) &amp; unmatched\n    if np.sum(time_neighbors) == 0:\n        continue\n    time_neighbors = np.where(time_neighbors)[0]\n    distance = np.zeros(len(time_neighbors), dtype=np.float32)\n    for j in range(len(time_neighbors)):\n        distance[j] = compute_distances(\n            deep_learning_cat.iloc[i][\"longitude\"],\n            deep_learning_cat.iloc[i][\"latitude\"],\n            0.,\n            cat_u.iloc[time_neighbors[j]][\"longitude\"],\n            cat_u.iloc[time_neighbors[j]][\"latitude\"],\n            0.,\n        )\n    space_neighbors = distance &lt;= MAX_DISTANCE_KM\n    if np.sum(space_neighbors) == 0:\n        continue\n    # restrict distances to subset of space and time neighbors\n    distance = distance[space_neighbors]\n    # get indexes of BPMF catalog corresponding to space-time neighbors\n    space_neighbors = time_neighbors[space_neighbors]\n    if len(space_neighbors) == 1:\n        matching_event_id[i] = cat_u.index[space_neighbors[0]]\n        unmatched[space_neighbors[0]] = False\n    else:\n        space_time_distance = np.zeros(len(space_neighbors), dtype=np.float32)\n        for k in range(len(space_neighbors)):\n            space_time_distance[k] = (\n                delta_t[space_neighbors[k]] + distance[k] / VELOCITY_MS\n            )\n        matching_event_id[i] = cat_u.index[space_neighbors[space_time_distance.argmin()]]\n        unmatched[space_neighbors[space_time_distance.argmin()]] = False\nnum_matched_events = np.sum(matching_event_id != 0.)\nnum_unmatched_events = len(deep_learning_cat) - num_matched_events\nmatched_pct = 100. * (float(num_matched_events) / len(deep_learning_cat))\nprint(f\"{num_matched_events:d} events ({matched_pct:.2f}%) of the deep learning catalog were matched with the template matching catalog.\")\n    \n</pre> # find SCSN events in the BPMF catalog MAX_ORIGIN_TIME_DIFFERENCE_SEC = 5. MAX_DISTANCE_KM = 15. VELOCITY_MS = 3500.  cat_u = catalog[catalog[\"unique_event\"]]  matching_event_id = np.zeros(len(deep_learning_cat), dtype=object) unmatched = np.ones(len(cat_u), dtype=bool) for i in range(len(deep_learning_cat)):     delta_t = np.abs(deep_learning_cat.iloc[i][\"time_sec\"] - cat_u[\"detection_time_sec\"].values)     time_neighbors = (delta_t &lt;= MAX_ORIGIN_TIME_DIFFERENCE_SEC) &amp; unmatched     if np.sum(time_neighbors) == 0:         continue     time_neighbors = np.where(time_neighbors)[0]     distance = np.zeros(len(time_neighbors), dtype=np.float32)     for j in range(len(time_neighbors)):         distance[j] = compute_distances(             deep_learning_cat.iloc[i][\"longitude\"],             deep_learning_cat.iloc[i][\"latitude\"],             0.,             cat_u.iloc[time_neighbors[j]][\"longitude\"],             cat_u.iloc[time_neighbors[j]][\"latitude\"],             0.,         )     space_neighbors = distance &lt;= MAX_DISTANCE_KM     if np.sum(space_neighbors) == 0:         continue     # restrict distances to subset of space and time neighbors     distance = distance[space_neighbors]     # get indexes of BPMF catalog corresponding to space-time neighbors     space_neighbors = time_neighbors[space_neighbors]     if len(space_neighbors) == 1:         matching_event_id[i] = cat_u.index[space_neighbors[0]]         unmatched[space_neighbors[0]] = False     else:         space_time_distance = np.zeros(len(space_neighbors), dtype=np.float32)         for k in range(len(space_neighbors)):             space_time_distance[k] = (                 delta_t[space_neighbors[k]] + distance[k] / VELOCITY_MS             )         matching_event_id[i] = cat_u.index[space_neighbors[space_time_distance.argmin()]]         unmatched[space_neighbors[space_time_distance.argmin()]] = False num_matched_events = np.sum(matching_event_id != 0.) num_unmatched_events = len(deep_learning_cat) - num_matched_events matched_pct = 100. * (float(num_matched_events) / len(deep_learning_cat)) print(f\"{num_matched_events:d} events ({matched_pct:.2f}%) of the deep learning catalog were matched with the template matching catalog.\")      <pre>972 events (98.88%) of the deep learning catalog were matched with the template matching catalog.\n</pre> In\u00a0[50]: Copied! <pre>M64_FORESHOCK_TIME = pd.Timestamp(\"2019-07-04T17:34:00\")\n\nfig, axes = plt.subplots(num=\"catalog_comparison\", nrows=2, figsize=(16, 14), sharex=True)\n\naxes[0].plot(deep_learning_cat[\"time\"], np.arange(len(deep_learning_cat)), color=\"k\", ls=\"--\", lw=2, label=\"Deep learning catalog\")\naxes[0].plot(cat_u[\"detection_time\"], np.arange(len(cat_u)), color=\"r\", lw=2, label=\"Template matching catalog\")\naxes[0].set_yscale(\"log\")\naxes[0].set_ylabel(\"Cumulative number of events\")\naxes[0].set_title(\"Comparison of the deep learning and deep learning+template matching catalogs\")\n\n\nin_both_cats = np.isin(cat_u.index, matching_event_id)\naxes[1].scatter(\n    cat_u.loc[~in_both_cats, \"detection_time\"], cat_u.loc[~in_both_cats, \"m_rel\"], color=\"r\", s=5, marker=\"o\", rasterized=True, label=\"Template matching catalog\"\n)\naxes[1].scatter(\n    cat_u.loc[in_both_cats, \"detection_time\"], cat_u.loc[in_both_cats, \"m_rel\"], color=\"k\", s=10, marker=\"o\", rasterized=True,  label=\"Deep learning catalog\"\n)\naxes[1].set_ylabel(\"Magnitudes\")\n\nfor ax in axes:\n    ax.axvline(\n        M64_FORESHOCK_TIME, color=\"magenta\", ls=\"-.\",\n        label=\"M6.4 Foreshock\"\n    )\n    ax.grid()\n    ax.set_xlabel(\"Time\")\n    ax.legend(loc=\"upper left\")\n</pre> M64_FORESHOCK_TIME = pd.Timestamp(\"2019-07-04T17:34:00\")  fig, axes = plt.subplots(num=\"catalog_comparison\", nrows=2, figsize=(16, 14), sharex=True)  axes[0].plot(deep_learning_cat[\"time\"], np.arange(len(deep_learning_cat)), color=\"k\", ls=\"--\", lw=2, label=\"Deep learning catalog\") axes[0].plot(cat_u[\"detection_time\"], np.arange(len(cat_u)), color=\"r\", lw=2, label=\"Template matching catalog\") axes[0].set_yscale(\"log\") axes[0].set_ylabel(\"Cumulative number of events\") axes[0].set_title(\"Comparison of the deep learning and deep learning+template matching catalogs\")   in_both_cats = np.isin(cat_u.index, matching_event_id) axes[1].scatter(     cat_u.loc[~in_both_cats, \"detection_time\"], cat_u.loc[~in_both_cats, \"m_rel\"], color=\"r\", s=5, marker=\"o\", rasterized=True, label=\"Template matching catalog\" ) axes[1].scatter(     cat_u.loc[in_both_cats, \"detection_time\"], cat_u.loc[in_both_cats, \"m_rel\"], color=\"k\", s=10, marker=\"o\", rasterized=True,  label=\"Deep learning catalog\" ) axes[1].set_ylabel(\"Magnitudes\")  for ax in axes:     ax.axvline(         M64_FORESHOCK_TIME, color=\"magenta\", ls=\"-.\",         label=\"M6.4 Foreshock\"     )     ax.grid()     ax.set_xlabel(\"Time\")     ax.legend(loc=\"upper left\") <p>NOTE: The magnitudes computed after the M6.4 foreshock are likely to be highly affected by the overall elevated noise level and the coda waves of previous events. The comparison mostly shows that the deep learning+template matching catalog adds events that were too small to be detected by deep learning, but template matching also does better at times of intense seismicity when phase association is difficult.</p> In\u00a0[51]: Copied! <pre>def fetch_detected_event_waveforms(detecton_time, moveouts):\n    event_waveforms = np.zeros(\n        (num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32\n    )\n    for s in range(num_stations):\n        for c in range(num_channels):\n            detection_time_samp = (\n                detecton_time - pd.Timestamp(date)\n            ).total_seconds() * SAMPLING_RATE_HZ\n            idx_start = int(detection_time_samp) + max(0, moveouts[s, c])\n            idx_end = idx_start + TEMPLATE_DURATION_SAMP\n            event_waveforms[s, c, :] = (\n                continuous_seismograms_arr[s, c, idx_start:idx_end]\n                )\n    return event_waveforms\n</pre> def fetch_detected_event_waveforms(detecton_time, moveouts):     event_waveforms = np.zeros(         (num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32     )     for s in range(num_stations):         for c in range(num_channels):             detection_time_samp = (                 detecton_time - pd.Timestamp(date)             ).total_seconds() * SAMPLING_RATE_HZ             idx_start = int(detection_time_samp) + max(0, moveouts[s, c])             idx_end = idx_start + TEMPLATE_DURATION_SAMP             event_waveforms[s, c, :] = (                 continuous_seismograms_arr[s, c, idx_start:idx_end]                 )     return event_waveforms In\u00a0[52]: Copied! <pre>new_events_cat = cat_u[~in_both_cats].copy()\nnew_events_cat\n</pre> new_events_cat = cat_u[~in_both_cats].copy() new_events_cat Out[52]: detection_time cc normalized_cc tid longitude latitude depth interevent_time_s return_time_s unique_event m_rel detection_time_sec 11 2019-07-04 00:47:26.920 0.320361 1.922152 5411 -117.880902 36.091986 4.854336 0.00 41.28 True 0.346957 1.562201e+09 33 2019-07-04 03:07:48.640 0.172434 1.150105 6929 -117.673684 36.114308 5.894466 139.20 NaN True 0.217015 1.562210e+09 34 2019-07-04 03:07:56.240 0.419868 2.800440 6929 -117.673684 36.114308 5.894466 7.60 7.60 True 0.295035 1.562210e+09 46 2019-07-04 05:16:24.960 0.273424 1.346671 13098 -117.879256 36.090409 4.268292 0.00 7855.56 True 0.376857 1.562217e+09 51 2019-07-04 05:20:58.200 0.349437 2.165418 4380 -117.671909 36.118573 6.085804 0.20 7228.80 True 0.487145 1.562218e+09 ... ... ... ... ... ... ... ... ... ... ... ... ... 1521 2019-07-04 23:59:12.120 0.157782 1.072621 19614 -117.559906 35.713991 8.982811 1.64 4049.80 True -0.776425 1.562285e+09 1609 2019-07-04 23:59:12.160 0.167771 1.122184 8143 -117.536542 35.732510 10.353954 0.04 19694.52 True -0.810293 1.562285e+09 2960 2019-07-04 23:59:16.880 0.273072 1.869326 783 -117.471812 35.711704 8.944759 4.72 3658.12 True -0.915777 1.562285e+09 8298 2019-07-04 23:59:31.720 0.232111 1.578736 12050 -117.517267 35.702664 11.127555 0.08 231.72 True -1.250341 1.562285e+09 7165 2019-07-04 23:59:34.160 0.226216 1.500470 5090 -117.567490 35.768518 9.428084 2.44 243.84 True -1.526509 1.562285e+09 <p>2228 rows \u00d7 12 columns</p> In\u00a0[53]: Copied! <pre>evidx = 456\ntid = new_events_cat[\"tid\"].iloc[evidx]\ntidx = template_ids.tolist().index(tid)\n\nevent_waveforms = fetch_detected_event_waveforms(\n    new_events_cat[\"detection_time\"].iloc[evidx],\n    moveouts_samp_arr[tidx]\n)\n\ndef _max_norm(x):\n    norm = np.abs(x).max()\n    if norm == 0.:\n        return x\n    return x / norm\n\nfig, axes = plt.subplots(num=\"event_waveforms\", nrows=num_stations, ncols=num_channels, figsize=(16, 20), sharex=True)\nfig.suptitle(f\"Event detected at {new_events_cat['detection_time'].iloc[evidx]} (TID: {tid})\")\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        time_sec = (max(0., moveouts_samp_arr[tidx, s, c]) + np.arange(TEMPLATE_DURATION_SAMP)) / SAMPLING_RATE_HZ\n        axes[s, c].plot(\n            time_sec,\n            _max_norm(event_waveforms[s, c, :]),\n            color=\"k\",\n            lw=0.75\n        )\n        axes[s, c].plot(\n            time_sec,\n            _max_norm(template_waveforms_arr[tidx, s, c, :]),\n            color=\"r\",\n            lw=0.75,\n            ls=\"--\",\n        )\n        axes[s, c].text(0.02, 0.96, sta + \" \" + cp, ha=\"left\", va=\"top\", transform=axes[s, c].transAxes)\n        # axes[s, c].set_title(sta + \" \" + cp)\n        if s == num_stations - 1:\n            axes[s, c].set_xlabel(\"Time (s)\")\n        if c == 0:\n            axes[s, c].set_ylabel(\"Amp.\")\n        axes[s, c].grid()\n# plt.tight_layout()\n</pre> evidx = 456 tid = new_events_cat[\"tid\"].iloc[evidx] tidx = template_ids.tolist().index(tid)  event_waveforms = fetch_detected_event_waveforms(     new_events_cat[\"detection_time\"].iloc[evidx],     moveouts_samp_arr[tidx] )  def _max_norm(x):     norm = np.abs(x).max()     if norm == 0.:         return x     return x / norm  fig, axes = plt.subplots(num=\"event_waveforms\", nrows=num_stations, ncols=num_channels, figsize=(16, 20), sharex=True) fig.suptitle(f\"Event detected at {new_events_cat['detection_time'].iloc[evidx]} (TID: {tid})\") for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         time_sec = (max(0., moveouts_samp_arr[tidx, s, c]) + np.arange(TEMPLATE_DURATION_SAMP)) / SAMPLING_RATE_HZ         axes[s, c].plot(             time_sec,             _max_norm(event_waveforms[s, c, :]),             color=\"k\",             lw=0.75         )         axes[s, c].plot(             time_sec,             _max_norm(template_waveforms_arr[tidx, s, c, :]),             color=\"r\",             lw=0.75,             ls=\"--\",         )         axes[s, c].text(0.02, 0.96, sta + \" \" + cp, ha=\"left\", va=\"top\", transform=axes[s, c].transAxes)         # axes[s, c].set_title(sta + \" \" + cp)         if s == num_stations - 1:             axes[s, c].set_xlabel(\"Time (s)\")         if c == 0:             axes[s, c].set_ylabel(\"Amp.\")         axes[s, c].grid() # plt.tight_layout()"},{"location":"notebooks/tm_multiple_templates/#template-matching-with-a-multiple-templates","title":"Template matching with a multiple templates\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p> <p>This notebook is the continuation of the first notebook, <code>tm_one_template.ipynb</code>.</p>"},{"location":"notebooks/tm_multiple_templates/#data-requirements","title":"Data requirements\u00b6","text":"<p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p> <p>Download the PhaseNet earthquake catalog with the following three commands:</p> <ul> <li>curl -o adloc_events.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_events.csv</li> <li>curl -o adloc_picks.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_picks.csv</li> <li>curl -o adloc_stations.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_stations.csv</li> </ul>"},{"location":"notebooks/tm_multiple_templates/#installing-fast_matched_filter","title":"Installing <code>fast_matched_filter</code>\u00b6","text":"<p>This example uses <code>fast_matched_filter</code>, a Python wrapper for C and CUDA-C routines. The C and CUDA-C libraries have to be compiled. Read the instructions at https://ebeauce.github.io/FMF_documentation/introduction.html.</p>"},{"location":"notebooks/tm_multiple_templates/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_multiple_templates/#read-continuous-seismograms-from-2019-07-04","title":"Read continuous seismograms from 2019-07-04\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#build-template-database","title":"Build template database\u00b6","text":"<p>Using the PhaseNet catalog, we will select all events with magnitudes between 3 and 5 as template events (totally arbitrary choice!).</p>"},{"location":"notebooks/tm_multiple_templates/#build-the-template-matching-catalog","title":"Build the template matching catalog\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template events.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_multiple_templates/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_multiple_templates/#assemble-all-detections-to-build-the-template-matching-catalog","title":"Assemble all detections to build the template matching catalog\u00b6","text":"<p>Use the trigger times to build the earthquake catalog and extract event waveforms on a given station/component.</p>"},{"location":"notebooks/tm_multiple_templates/#plot-some-waveforms","title":"Plot some waveforms\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#plot-inter-event-time-and-template-return-time-vs-detection-time","title":"Plot inter-event time and template return time vs detection time\u00b6","text":"<p>Having multiple templates, we can define two types of inter-event times:</p> <ul> <li>inter-event time: this is the time between two consecutive events in the catalog, disregarding their locations,</li> <li>return time: this is the time between two consecutive events detected by a same template, that is, two consecutive co-located events.</li> </ul> <p>We naively assembled all the detected events from all the templates, but closely located templates can detect the same events, leading to multiple detections in the catalog. These multiples are visible with horizontal lines in the inter-event time vs detection time plot (see below).</p>"},{"location":"notebooks/tm_multiple_templates/#de-lumping-the-catalog","title":"\"De-lumping\" the catalog\u00b6","text":"<p>Templates with similar waveforms and similar moveouts detect similar events. Thus, naively assembling the detected events from all templates results in \"lumped\" detections that represent multiple detections of the same event. (note: we avoid using \"clustered\" detections because the term \"cluster\" has lots of meaning in statistical seismology).</p> <p>In the following, we:</p> <ul> <li>compute inter-template distances,</li> <li>compute inter-template waveform similarity (correlation coefficient),</li> <li>flag the \"lumped\" detections based on three criteria on inter-event time, inter-event distance and inter-event waveform similarity.</li> </ul>"},{"location":"notebooks/tm_multiple_templates/#inter-template-distances","title":"Inter-template distances\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#inter-template-waveform-similarity","title":"Inter-template waveform similarity\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#flag-multiples","title":"Flag multiples\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#relative-magnitudes","title":"Relative magnitudes\u00b6","text":""},{"location":"notebooks/tm_multiple_templates/#compare-with-deep-learning-catalog","title":"Compare with deep learning catalog\u00b6","text":""},{"location":"notebooks/tm_one_template/","title":"Notebook1","text":"In\u00a0[3]: Copied! <pre>import os\nimport fast_matched_filter as fmf\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport obspy as obs\nimport pandas as pd\n</pre> import os import fast_matched_filter as fmf import glob import numpy as np import matplotlib.pyplot as plt import obspy as obs import pandas as pd  In\u00a0[4]: Copied! <pre># path variables and file names\nDIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA\nDIR_CATALOG = \"../picks_phasenet/\"\n\nSTATION_FILE = \"adloc_stations.csv\"\nEVENT_FILE = \"adloc_events.csv\"\nPICK_FILE = \"adloc_picks.csv\"\n</pre> # path variables and file names DIR_WAVEFORMS = \"/home/ebeauce/SSA_EQ_DETECTION_WORKSHOP/data\" # REPLACE THIS PATH WITH WHEREVER YOU DOWNLOADED THE DATA DIR_CATALOG = \"../picks_phasenet/\"  STATION_FILE = \"adloc_stations.csv\" EVENT_FILE = \"adloc_events.csv\" PICK_FILE = \"adloc_picks.csv\" In\u00a0[5]: Copied! <pre>station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE))\nstation_meta\n</pre> station_meta = pd.read_csv(os.path.join(DIR_CATALOG, STATION_FILE)) station_meta Out[5]: network station location instrument component latitude longitude elevation_m depth_km provider station_id station_term_time_p station_term_time_s station_term_amplitude 0 CI CCC NaN BH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..BH 0.266086 0.500831 0.049399 1 CI CCC NaN HH ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HH 0.295428 0.518465 0.191475 2 CI CCC NaN HN ENZ 35.524950 -117.364530 670.0 -0.6700 SCEDC CI.CCC..HN 0.296263 0.541148 0.064485 3 CI CLC NaN BH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..BH -0.231963 -0.415271 -0.331371 4 CI CLC NaN HH ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HH -0.168743 -0.390045 -0.140313 5 CI CLC NaN HN ENZ 35.815740 -117.597510 775.0 -0.7750 SCEDC CI.CLC..HN -0.175671 -0.388116 -0.249066 6 CI DTP NaN BH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..BH -0.305881 -0.602459 -0.503411 7 CI DTP NaN HH ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HH -0.263705 -0.564867 -0.437951 8 CI DTP NaN HN ENZ 35.267420 -117.845810 908.0 -0.9080 SCEDC CI.DTP..HN -0.244383 -0.538990 -0.500516 9 CI JRC2 NaN BH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..BH 0.011361 -0.080285 -0.039941 10 CI JRC2 NaN HH ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HH 0.053539 -0.052748 0.068213 11 CI JRC2 NaN HN ENZ 35.982490 -117.808850 1469.0 -1.4690 SCEDC CI.JRC2..HN 0.059764 -0.045991 -0.007637 12 CI LRL NaN BH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..BH -0.295604 -0.540857 0.033788 13 CI LRL NaN HH ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HH -0.268381 -0.513955 0.146876 14 CI LRL NaN HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL..HN -0.266329 -0.503088 0.045499 15 CI LRL 2C HN ENZ 35.479540 -117.682120 1340.0 -1.3400 SCEDC CI.LRL.2C.HN 0.000000 0.000000 0.000000 16 CI MPM NaN BH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..BH -0.011825 -0.098089 -0.518793 17 CI MPM NaN HH ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HH 0.009095 -0.081896 -0.459349 18 CI MPM NaN HN ENZ 36.057990 -117.489010 1839.0 -1.8390 SCEDC CI.MPM..HN 0.011870 -0.052277 -0.532764 19 CI Q0072 01 HN ENZ 35.609617 -117.666721 695.0 -0.6950 SCEDC CI.Q0072.01.HN 0.000000 0.000000 0.000000 20 CI SLA NaN BH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..BH 0.066893 0.118500 -0.081634 21 CI SLA NaN HH ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HH 0.089833 0.128589 -0.042928 22 CI SLA NaN HN ENZ 35.890950 -117.283320 1174.0 -1.1740 SCEDC CI.SLA..HN 0.093526 0.168238 -0.150381 23 CI SRT NaN BH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..BH 0.148171 0.653411 -0.253527 24 CI SRT NaN HH ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HH 0.183868 0.641707 -0.208859 25 CI SRT NaN HN ENZ 35.692350 -117.750510 667.0 -0.6670 SCEDC CI.SRT..HN 0.175475 0.674587 -0.295869 26 CI TOW2 NaN BH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..BH 0.268083 0.816279 0.028038 27 CI TOW2 NaN HH ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HH 0.285970 0.831533 0.102681 28 CI TOW2 NaN HN ENZ 35.808560 -117.764880 685.0 -0.6850 SCEDC CI.TOW2..HN 0.285113 0.843886 -0.006698 29 CI WBM NaN BH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..BH 0.136927 0.128744 -0.166279 30 CI WBM NaN HH ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HH 0.152299 0.124250 -0.138614 31 CI WBM NaN HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM..HN 0.170719 0.179461 -0.101436 32 CI WBM 2C HN ENZ 35.608390 -117.890490 892.0 -0.8920 SCEDC CI.WBM.2C.HN 0.000000 0.000000 0.000000 33 CI WCS2 NaN BH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..BH 0.030349 -0.126935 0.023642 34 CI WCS2 NaN HH ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HH 0.065321 -0.099447 0.146628 35 CI WCS2 NaN HN ENZ 36.025210 -117.765260 1143.0 -1.1430 SCEDC CI.WCS2..HN 0.061651 -0.096450 0.037563 36 CI WMF NaN BH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..BH 0.039416 -0.005761 -0.165183 37 CI WMF NaN HH ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HH 0.075658 0.005327 -0.079900 38 CI WMF NaN HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF..HN 0.085427 0.025273 -0.240971 39 CI WMF 2C HN ENZ 36.117580 -117.854860 1537.4 -1.5374 SCEDC CI.WMF.2C.HN 0.000000 0.000000 0.000000 40 CI WNM NaN EH Z 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..EH -0.026889 -0.070269 -0.332510 41 CI WNM NaN HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM..HN -0.011659 -0.118223 -0.038719 42 CI WNM 2C HN ENZ 35.842200 -117.906160 974.3 -0.9743 SCEDC CI.WNM.2C.HN 0.000000 0.000000 0.000000 43 CI WRC2 NaN BH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..BH -0.023860 -0.043523 0.117833 44 CI WRC2 NaN HH ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HH 0.010633 -0.029488 0.165234 45 CI WRC2 NaN HN ENZ 35.947900 -117.650380 943.0 -0.9430 SCEDC CI.WRC2..HN 0.014658 -0.021367 0.103905 46 CI WRV2 NaN EH Z 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..EH -0.003461 -0.154572 -0.355199 47 CI WRV2 NaN HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2..HN 0.017317 -0.137916 -0.273270 48 CI WRV2 2C HN ENZ 36.007740 -117.890400 1070.0 -1.0700 SCEDC CI.WRV2.2C.HN 0.000000 0.000000 0.000000 49 CI WVP2 NaN EH Z 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..EH 0.020325 0.008989 -0.341606 50 CI WVP2 NaN HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2..HN 0.024742 -0.103024 -0.089014 51 CI WVP2 2C HN ENZ 35.949390 -117.817690 1465.0 -1.4650 SCEDC CI.WVP2.2C.HN 0.000000 0.000000 0.000000 <p>The following shows a very rudimentary map of the station network. Look into the <code>cartopy</code> package for more sophisticated maps.</p> In\u00a0[6]: Copied! <pre>_station_meta = station_meta.drop_duplicates(\"station\")\n\nfig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10))\nax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\")\nfor idx, row in _station_meta.iterrows():\n    ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\")\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.grid()\nax.set_title(\"Stations used to build the PhaseNet catalog\")\n</pre> _station_meta = station_meta.drop_duplicates(\"station\")  fig, ax = plt.subplots(num=\"station_network\", figsize=(10, 10)) ax.scatter(_station_meta[\"longitude\"], _station_meta[\"latitude\"], marker=\"v\", color=\"k\") for idx, row in _station_meta.iterrows():     ax.text(row.longitude + 0.01, row.latitude + 0.01, row.station, va=\"bottom\", ha=\"left\") ax.set_xlabel(\"Longitude\") ax.set_ylabel(\"Latitude\") ax.grid() ax.set_title(\"Stations used to build the PhaseNet catalog\") Out[6]: <pre>Text(0.5, 1.0, 'Stations used to build the PhaseNet catalog')</pre> In\u00a0[7]: Copied! <pre>event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE))\nevent_meta\n</pre> event_meta = pd.read_csv(os.path.join(DIR_CATALOG, EVENT_FILE)) event_meta Out[7]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 ... ... ... ... ... ... ... ... ... ... ... 20898 2019-07-09 23:58:14.298499048 0.933013 0.097816 67 1.543257 0.131523 1907 -117.711811 35.926468 6.652020 20899 2019-07-09 23:58:47.701746285 0.882434 0.090185 73 1.087089 0.140159 2051 -117.604263 35.797450 6.617413 20900 2019-07-09 23:59:05.102247662 0.798047 0.435077 26 1.147040 0.170994 12567 -117.509729 35.692722 12.815041 20901 2019-07-09 23:59:40.257837813 0.971081 0.065523 35 1.161323 0.068586 7726 -117.846289 36.061435 5.224666 20902 2019-07-09 23:59:49.650466544 0.800524 0.023674 15 0.936622 0.095959 18566 -117.896100 36.095886 6.761860 <p>20903 rows \u00d7 10 columns</p> In\u00a0[8]: Copied! <pre>picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE))\npicks\n</pre> picks = pd.read_csv(os.path.join(DIR_CATALOG, PICK_FILE)) picks Out[8]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 0 CI.WMF..BH 280874 2019-07-04 00:46:48.759 0.594 P 0.01 0.110 -3.213107 6720 0.955091 3.124152e-07 1 0.003919 0.012320 1 CI.WMF..HH 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.938 -3.077638 6720 0.948896 3.368390e-07 1 0.026491 0.063669 2 CI.WMF..HN 280881 2019-07-04 00:46:48.818 0.973 P 0.01 0.898 -3.128019 6720 1.598008 2.146277e-07 1 0.017580 0.173702 3 CI.WRV2..EH 280945 2019-07-04 00:46:49.450 0.977 P 0.01 -0.855 -3.464453 6720 1.000000 3.298200e-07 1 0.077194 0.244381 4 CI.WRV2..HN 280945 2019-07-04 00:46:49.450 0.941 P 0.01 -0.906 -3.585863 6720 1.147235 3.908305e-07 1 0.056694 0.041691 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 955700 CI.WRV2..HN 8639355 2019-07-09 23:59:53.550 0.688 S 0.01 -0.022 -3.384471 18566 0.644492 1.973834e-06 1 0.019093 0.028409 955701 CI.JRC2..HH 8639499 2019-07-09 23:59:54.998 0.402 S 0.01 -0.023 -3.446238 18566 0.647843 6.197256e-06 1 0.020364 -0.154404 955702 CI.JRC2..HN 8639499 2019-07-09 23:59:54.998 0.344 S 0.01 -0.016 -3.531800 18566 0.731796 4.075353e-06 1 0.012773 -0.164659 955703 CI.WVP2..EH 8639574 2019-07-09 23:59:55.740 0.314 S 0.01 -0.103 -3.488117 18566 0.636005 2.815148e-06 1 -0.092233 0.316712 955704 CI.WVP2..HN 8639576 2019-07-09 23:59:55.760 0.695 S 0.01 0.076 -3.218676 18566 0.825223 4.657352e-06 1 0.039319 0.332722 <p>955705 rows \u00d7 14 columns</p> In\u00a0[9]: Copied! <pre>def fetch_event_waveforms(\n    event_picks,\n    dir_waveforms=DIR_WAVEFORMS,\n    time_before_phase_onset_sec=2.0,\n    duration_sec=10.0\n    ):\n    \"\"\"\n    Fetches the waveforms for a given event based on the picks.\n\n    Parameters\n    ----------\n    event_picks : pandas.DataFrame\n        DataFrame containing the picks for the event.\n    dir_waveforms : str, optional\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n    time_before_phase_onset_sec : float, optional\n        Time in seconds to start the waveform before the phase onset, by default 2.0.\n    duration_sec : float, optional\n        Duration in seconds of the waveform to fetch, by default 10.0.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched waveforms.\n    \"\"\"\n    stream = obs.Stream()\n    for _, pick in event_picks.iterrows():\n        # check whether we have a miniseed file for this waveform\n        if pick.phase_type == \"P\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))\n        elif pick.phase_type == \"S\":\n            files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))\n        starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec\n        endtime = starttime + duration_sec\n        for _file in files:\n            stream += obs.read(\n                _file,\n                starttime=starttime,\n                endtime=endtime\n            )\n    return stream\n    \n</pre> def fetch_event_waveforms(     event_picks,     dir_waveforms=DIR_WAVEFORMS,     time_before_phase_onset_sec=2.0,     duration_sec=10.0     ):     \"\"\"     Fetches the waveforms for a given event based on the picks.      Parameters     ----------     event_picks : pandas.DataFrame         DataFrame containing the picks for the event.     dir_waveforms : str, optional         Directory where the waveform data is stored, by default DIR_WAVEFORMS.     time_before_phase_onset_sec : float, optional         Time in seconds to start the waveform before the phase onset, by default 2.0.     duration_sec : float, optional         Duration in seconds of the waveform to fetch, by default 10.0.      Returns     -------     obspy.Stream         Stream object containing the fetched waveforms.     \"\"\"     stream = obs.Stream()     for _, pick in event_picks.iterrows():         # check whether we have a miniseed file for this waveform         if pick.phase_type == \"P\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"Z*mseed\"))         elif pick.phase_type == \"S\":             files = glob.glob(os.path.join(dir_waveforms, pick.station_id + \"[N,E]*mseed\"))         starttime = obs.UTCDateTime(pick.phase_time) - time_before_phase_onset_sec         endtime = starttime + duration_sec         for _file in files:             stream += obs.read(                 _file,                 starttime=starttime,                 endtime=endtime             )     return stream      In\u00a0[10]: Copied! <pre># explore event_meta to find a nice intermediate-size earthquake we could plot\nevent_meta.head(20)\n</pre> # explore event_meta to find a nice intermediate-size earthquake we could plot event_meta.head(20) Out[10]: time adloc_score adloc_residual_time num_picks magnitude adloc_residual_amplitude event_index longitude latitude depth_km 0 2019-07-04 00:46:47.342963596 0.866596 0.057355 35 0.595368 0.165480 6720 -117.882570 36.091088 4.643969 1 2019-07-04 00:55:32.648412579 0.781116 0.203567 16 1.142510 0.170509 17122 -117.799226 35.378160 11.078458 2 2019-07-04 00:56:37.232733104 0.908073 0.086183 42 0.912494 0.166681 5411 -117.880902 36.091986 4.854336 3 2019-07-04 02:00:39.149363202 0.814322 0.036164 15 0.209530 0.092534 17868 -117.866468 36.093520 4.981447 4 2019-07-04 03:05:31.018885833 0.799281 0.080708 11 0.104050 0.156833 25037 -117.846320 36.100386 5.943363 5 2019-07-04 03:20:28.674438914 0.755451 0.050436 44 0.869927 0.111885 6929 -117.673684 36.114308 5.894466 6 2019-07-04 04:03:01.619369274 0.897311 0.143631 32 0.386310 0.214381 7607 -117.805077 36.016063 0.396071 7 2019-07-04 05:16:47.223353119 0.835094 0.053522 22 0.575433 0.163675 13098 -117.879256 36.090409 4.268292 8 2019-07-04 06:57:32.758991812 0.922386 0.065846 23 0.256281 0.065212 15698 -117.867587 36.081665 5.939931 9 2019-07-04 11:51:07.805591259 0.692042 0.075023 48 0.818316 0.102087 4380 -117.671909 36.118573 6.085804 10 2019-07-04 15:36:04.228420696 0.652888 0.012137 9 -0.633904 0.167058 27370 -117.785299 36.005578 3.234981 11 2019-07-04 15:42:47.932558745 0.597856 0.072740 28 0.933088 0.151623 9740 -117.501116 35.707382 14.496446 12 2019-07-04 16:07:20.003321194 0.729246 0.093399 33 0.835854 0.133106 9365 -117.491503 35.711241 13.846898 13 2019-07-04 16:11:46.920083440 0.711579 0.516367 18 1.734816 0.117952 7098 -117.877675 35.196445 31.000000 14 2019-07-04 16:13:43.094792540 0.849673 0.070048 83 1.653859 0.111108 2305 -117.493716 35.710090 13.375377 15 2019-07-04 16:16:07.085486307 0.814365 0.040161 10 0.587583 0.099619 26520 -117.540198 35.689022 14.537368 16 2019-07-04 17:02:55.057058245 0.770696 0.079675 90 4.476724 0.143489 1101 -117.495094 35.711607 13.586411 17 2019-07-04 17:04:02.231614981 0.664676 0.064038 41 2.062226 0.186214 5379 -117.488446 35.711139 14.001705 18 2019-07-04 17:05:05.071421677 0.509088 0.089882 29 1.471434 0.157048 12115 -117.491307 35.710881 13.223374 19 2019-07-04 17:08:51.664841725 0.666790 0.046444 15 0.657653 0.175270 23044 -117.504506 35.706123 14.570921 In\u00a0[11]: Copied! <pre># feel free to play with the event index to plot different events\nEVENT_IDX = 1101\n\nevent_meta.set_index(\"event_index\").loc[EVENT_IDX]\n</pre> # feel free to play with the event index to plot different events EVENT_IDX = 1101  event_meta.set_index(\"event_index\").loc[EVENT_IDX] Out[11]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[12]: Copied! <pre># fetch the corresponding picks for this event\nevent_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nevent_picks\n</pre> # fetch the corresponding picks for this event event_picks = picks[picks[\"event_index\"] == EVENT_IDX] event_picks Out[12]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[13]: Copied! <pre># fetch the waveforms\nevent_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.)\nprint(event_waveforms.__str__(extended=True))\n</pre> # fetch the waveforms event_waveforms = fetch_event_waveforms(event_picks, time_before_phase_onset_sec=10., duration_sec=30.) print(event_waveforms.__str__(extended=True)) <pre>42 Trace(s) in Stream:\nCI.CLC..HHZ  | 2019-07-04T17:02:48.478300Z - 2019-07-04T17:03:18.478300Z | 25.0 Hz, 751 samples\nCI.SRT..HHZ  | 2019-07-04T17:02:49.918300Z - 2019-07-04T17:03:19.918300Z | 25.0 Hz, 751 samples\nCI.CCC..HHZ  | 2019-07-04T17:02:50.118300Z - 2019-07-04T17:03:20.118300Z | 25.0 Hz, 751 samples\nCI.SLA..HHZ  | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHZ | 2019-07-04T17:02:50.518300Z - 2019-07-04T17:03:20.518300Z | 25.0 Hz, 751 samples\nCI.LRL..HHZ  | 2019-07-04T17:02:50.638300Z - 2019-07-04T17:03:20.638300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHZ | 2019-07-04T17:02:50.718300Z - 2019-07-04T17:03:20.718300Z | 25.0 Hz, 751 samples\nCI.CLC..HHN  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.CLC..HHE  | 2019-07-04T17:02:50.958300Z - 2019-07-04T17:03:20.958300Z | 25.0 Hz, 751 samples\nCI.MPM..HHZ  | 2019-07-04T17:02:52.038300Z - 2019-07-04T17:03:22.038300Z | 25.0 Hz, 751 samples\nCI.WBM..HHZ  | 2019-07-04T17:02:52.123100Z - 2019-07-04T17:03:22.123100Z | 25.0 Hz, 751 samples\nCI.WVP2..EHZ | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.WNM..EHZ  | 2019-07-04T17:02:52.160000Z - 2019-07-04T17:03:22.160000Z | 25.0 Hz, 751 samples\nCI.JRC2..HHZ | 2019-07-04T17:02:52.558300Z - 2019-07-04T17:03:22.558300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHZ | 2019-07-04T17:02:52.598300Z - 2019-07-04T17:03:22.598300Z | 25.0 Hz, 751 samples\nCI.WRV2..EHZ | 2019-07-04T17:02:53.600000Z - 2019-07-04T17:03:23.600000Z | 25.0 Hz, 751 samples\nCI.SRT..HHN  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.SRT..HHE  | 2019-07-04T17:02:53.838300Z - 2019-07-04T17:03:23.838300Z | 25.0 Hz, 751 samples\nCI.CCC..HHN  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.CCC..HHE  | 2019-07-04T17:02:54.078300Z - 2019-07-04T17:03:24.078300Z | 25.0 Hz, 751 samples\nCI.SLA..HHE  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.SLA..HHN  | 2019-07-04T17:02:54.638300Z - 2019-07-04T17:03:24.638300Z | 25.0 Hz, 751 samples\nCI.LRL..HHN  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.LRL..HHE  | 2019-07-04T17:02:54.718300Z - 2019-07-04T17:03:24.718300Z | 25.0 Hz, 751 samples\nCI.WMF..HHZ  | 2019-07-04T17:02:54.798300Z - 2019-07-04T17:03:24.798300Z | 25.0 Hz, 751 samples\nCI.DTP..HHZ  | 2019-07-04T17:02:54.958300Z - 2019-07-04T17:03:24.958300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.TOW2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHN | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WRC2..HHE | 2019-07-04T17:02:54.998300Z - 2019-07-04T17:03:24.998300Z | 25.0 Hz, 751 samples\nCI.WBM..HHE  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.WBM..HHN  | 2019-07-04T17:02:57.243100Z - 2019-07-04T17:03:27.243100Z | 25.0 Hz, 751 samples\nCI.MPM..HHE  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.MPM..HHN  | 2019-07-04T17:02:57.318300Z - 2019-07-04T17:03:27.318300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHN | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.JRC2..HHE | 2019-07-04T17:02:57.958300Z - 2019-07-04T17:03:27.958300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHE | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WCS2..HHN | 2019-07-04T17:02:58.118300Z - 2019-07-04T17:03:28.118300Z | 25.0 Hz, 751 samples\nCI.WMF..HHN  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.WMF..HHE  | 2019-07-04T17:03:01.838300Z - 2019-07-04T17:03:31.838300Z | 25.0 Hz, 751 samples\nCI.DTP..HHN  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\nCI.DTP..HHE  | 2019-07-04T17:03:02.038300Z - 2019-07-04T17:03:32.038300Z | 25.0 Hz, 751 samples\n</pre> In\u00a0[14]: Copied! <pre># plot them!\nfig = event_waveforms.select(component=\"Z\").plot(equal_scale=False)\n</pre> # plot them! fig = event_waveforms.select(component=\"Z\").plot(equal_scale=False) In\u00a0[16]: Copied! <pre>selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX]\nselected_event_meta\n</pre> selected_event_meta = event_meta.set_index(\"event_index\").loc[EVENT_IDX] selected_event_meta Out[16]: <pre>time                        2019-07-04 17:02:55.057058245\nadloc_score                                      0.770696\nadloc_residual_time                              0.079675\nnum_picks                                              90\nmagnitude                                        4.476724\nadloc_residual_amplitude                         0.143489\nlongitude                                     -117.495094\nlatitude                                        35.711607\ndepth_km                                        13.586411\nName: 1101, dtype: object</pre> In\u00a0[17]: Copied! <pre>selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX]\nselected_event_picks\n</pre> selected_event_picks = picks[picks[\"event_index\"] == EVENT_IDX] selected_event_picks Out[17]: station_id phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude 565 CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 566 CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 567 CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 568 CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 569 CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 650 CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 651 CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 652 CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 653 CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 654 CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 <p>90 rows \u00d7 14 columns</p> In\u00a0[18]: Copied! <pre>def fetch_day_waveforms(dir_waveforms):\n    \"\"\"\n    Fetches the continuous seismograms for a given day.\n\n    Parameters\n    ----------\n    dir_waveforms : str\n        Directory where the waveform data is stored, by default DIR_WAVEFORMS.\n\n    Returns\n    -------\n    obspy.Stream\n        Stream object containing the fetched continuous seismograms.\n    \"\"\"\n    stream = obs.Stream()\n    files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))\n    for _file in files:\n        stream += obs.read(_file)\n    return stream\n</pre> def fetch_day_waveforms(dir_waveforms):     \"\"\"     Fetches the continuous seismograms for a given day.      Parameters     ----------     dir_waveforms : str         Directory where the waveform data is stored, by default DIR_WAVEFORMS.      Returns     -------     obspy.Stream         Stream object containing the fetched continuous seismograms.     \"\"\"     stream = obs.Stream()     files = glob.glob(os.path.join(dir_waveforms, \"*mseed\"))     for _file in files:         stream += obs.read(_file)     return stream In\u00a0[19]: Copied! <pre># first, read the continuous seismograms into an `obspy.Stream`\ncontinuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS)\nprint(continuous_seismograms.__str__(extended=True))\n</pre> # first, read the continuous seismograms into an `obspy.Stream` continuous_seismograms = fetch_day_waveforms(DIR_WAVEFORMS) print(continuous_seismograms.__str__(extended=True)) <pre>57 Trace(s) in Stream:\nCI.WCS2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRV2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B916..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nPB.B917..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B918..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHZ  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.CLC..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B918..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHN | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B916..EH1 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DTP..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHE  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B917..EHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WVP2..EHZ | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nPB.B917..EH2 | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.CCC..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SLA..HHZ  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WNM..EHZ  | 2019-07-04T00:00:00.000000Z - 2019-07-04T23:59:59.960000Z | 25.0 Hz, 2160000 samples\nCI.WRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.JRC2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.SRT..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.LRL..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.MPM..HHN  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nPB.B921..EHZ | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WBM..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\nPB.B921..EH2 | 2019-07-03T23:59:59.998200Z - 2019-07-04T23:59:59.958200Z | 25.0 Hz, 2160000 samples\nCI.WMF..HHE  | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.WCS2..HHZ | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.TOW2..HHE | 2019-07-03T23:59:59.998300Z - 2019-07-04T23:59:59.958300Z | 25.0 Hz, 2160000 samples\nCI.DAW..HHN  | 2019-07-04T00:00:00.003100Z - 2019-07-04T23:59:59.963100Z | 25.0 Hz, 2160000 samples\n</pre> In\u00a0[20]: Copied! <pre># plot the continuous seismograms from a single station\nfig = continuous_seismograms.select(station=\"CLC\").plot()\n</pre> # plot the continuous seismograms from a single station fig = continuous_seismograms.select(station=\"CLC\").plot() In\u00a0[21]: Copied! <pre># then, cast data into `numpy.ndarray`\nstation_codes = list(set([st.stats.station for st in continuous_seismograms]))\ncomponent_codes = [\"N\", \"E\", \"Z\"]\ncomponent_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}\n\nnum_stations = len(station_codes)\nnum_channels = len(component_codes)\nnum_samples = len(continuous_seismograms[0].data)\n\ncontinuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        for cp_alias in component_aliases[cp]:\n            sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)\n            if len(sel_seismogram) &gt; 0:\n                continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data\n                break\n            \ncontinuous_seismograms_arr\n</pre> # then, cast data into `numpy.ndarray` station_codes = list(set([st.stats.station for st in continuous_seismograms])) component_codes = [\"N\", \"E\", \"Z\"] component_aliases={\"E\": [\"E\", \"2\"], \"N\": [\"N\", \"1\"], \"Z\": [\"Z\"]}  num_stations = len(station_codes) num_channels = len(component_codes) num_samples = len(continuous_seismograms[0].data)  continuous_seismograms_arr = np.zeros((num_stations, num_channels, num_samples), dtype=np.float32) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         for cp_alias in component_aliases[cp]:             sel_seismogram = continuous_seismograms.select(station=sta, component=cp_alias)             if len(sel_seismogram) &gt; 0:                 continuous_seismograms_arr[s, c, :] = sel_seismogram[0].data                 break              continuous_seismograms_arr  Out[21]: <pre>array([[[ 4.09647620e-12,  1.35595988e-11,  1.61010198e-11, ...,\n          1.87989971e-10,  5.81934279e-10,  1.59266211e-11],\n        [ 8.85055397e-12,  1.41080064e-11,  2.29350306e-12, ...,\n         -1.22290775e-10,  4.84999090e-11,  1.02444719e-10],\n        [-1.44921930e-11, -8.83197541e-13,  5.94794233e-13, ...,\n          3.61067509e-10,  1.61199512e-10, -2.93843921e-10]],\n\n       [[-2.02754636e-11,  7.27350949e-12,  2.90683051e-11, ...,\n         -1.27593561e-10,  4.12396610e-11,  2.02138389e-10],\n        [-8.89091231e-12,  1.46336155e-11,  4.67565708e-11, ...,\n         -1.25822366e-10,  5.78710552e-11,  4.38750390e-11],\n        [ 5.92738358e-11,  3.67502244e-11, -1.45799206e-10, ...,\n         -4.12412986e-11,  2.63505495e-10, -1.57162269e-11]],\n\n       [[ 5.47254013e-11,  7.05789246e-11, -6.25662326e-12, ...,\n         -3.65891401e-10, -7.81559206e-10, -6.38798348e-10],\n        [-1.39255118e-11,  2.02905037e-11,  1.07095530e-11, ...,\n          1.02497244e-09,  7.77831466e-10, -4.58769023e-10],\n        [ 1.94260233e-10, -3.34868328e-10, -1.75106818e-10, ...,\n          9.01718644e-10, -9.07585604e-11, -1.75413142e-10]],\n\n       ...,\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [-2.38018650e-11,  4.41909790e-12, -4.36377739e-11, ...,\n         -4.77496098e-11, -1.39665293e-10, -1.53408883e-10]],\n\n       [[ 4.53147459e-12,  2.15497811e-11,  4.97441671e-12, ...,\n          1.37320554e-11, -8.23028538e-12,  2.90706681e-10],\n        [-2.32118665e-11, -7.40775150e-12,  4.76967381e-12, ...,\n         -5.22246857e-10, -1.66053525e-11,  4.45412207e-10],\n        [-6.16842992e-12,  4.56698828e-12,  1.07985522e-11, ...,\n          1.94127353e-10,  1.03605340e-10, -3.92332278e-10]],\n\n       [[ 1.00288475e-11, -1.25283534e-11, -8.96174020e-12, ...,\n          8.97525543e-12, -6.86870838e-11, -2.98185018e-11],\n        [ 1.08289614e-12,  8.16112889e-12, -1.36335960e-11, ...,\n          4.46155092e-11,  7.22212498e-11, -3.26238862e-11],\n        [-7.30912639e-12,  1.16460912e-11,  5.44300976e-12, ...,\n          2.24157377e-11,  3.17579747e-11, -6.67901151e-11]]],\n      dtype=float32)</pre> In\u00a0[22]: Copied! <pre># PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform.\n#                Here, we use windows centered around the S wave for horizontal components\n#                and windows starting 1sec before the P wave for the vertical component.\nPHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"}\n# OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase\n#               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window\n#               1 second before the predicted P arrival time\nOFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0}\n# TEMPLATE_DURATION_SEC\nTEMPLATE_DURATION_SEC = 8. \n# SAMPLING_RATE_HZ\nSAMPLING_RATE_HZ = 25.\n# TEMPLATE_DURATION_SAMP\nTEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)\n</pre> # PHASE_ON_COMP: dictionary defining which moveout we use to extract the waveform. #                Here, we use windows centered around the S wave for horizontal components #                and windows starting 1sec before the P wave for the vertical component. PHASE_ON_COMP = {\"N\": \"S\", \"E\": \"S\", \"Z\": \"P\"} # OFFSET_PHASE_SEC: dictionary defining the time offset taken before a given phase #               for example OFFSET_PHASE_SEC[\"P\"] = 1.0 means that we extract the window #               1 second before the predicted P arrival time OFFSET_PHASE_SEC = {\"P\": 1.0, \"S\": 4.0} # TEMPLATE_DURATION_SEC TEMPLATE_DURATION_SEC = 8.  # SAMPLING_RATE_HZ SAMPLING_RATE_HZ = 25. # TEMPLATE_DURATION_SAMP TEMPLATE_DURATION_SAMP = int(TEMPLATE_DURATION_SEC * SAMPLING_RATE_HZ)  In\u00a0[23]: Copied! <pre># add station_code columns to `selected_event_picks`\nselected_event_picks.set_index(\"station_id\", inplace=True)\nfor staid in selected_event_picks.index:\n    station_code = staid.split(\".\")[1]\n    selected_event_picks.loc[staid, \"station_code\"] = station_code\nselected_event_picks\n</pre> # add station_code columns to `selected_event_picks` selected_event_picks.set_index(\"station_id\", inplace=True) for staid in selected_event_picks.index:     station_code = staid.split(\".\")[1]     selected_event_picks.loc[staid, \"station_code\"] = station_code selected_event_picks <pre>/tmp/ipykernel_471469/1881751651.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  selected_event_picks.loc[staid, \"station_code\"] = station_code\n</pre> Out[23]: phase_index phase_time phase_score phase_type dt_s phase_polarity phase_amplitude event_index sp_ratio event_amplitude adloc_mask adloc_residual_time adloc_residual_amplitude station_code station_id CI.CLC..HN 6137848 2019-07-04 17:02:58.488 0.969 P 0.01 -0.879 -0.511873 1101 1.017569 2.474507e-07 1 -0.010048 -0.054426 CLC CI.CLC..BH 6137847 2019-07-04 17:02:58.489 0.633 P 0.01 -0.324 -0.666956 1101 1.014822 2.022085e-07 1 0.048153 -0.127168 CLC CI.CLC..HH 6137849 2019-07-04 17:02:58.498 0.965 P 0.01 -0.883 -0.586030 1101 1.086886 2.141392e-07 1 -0.007135 -0.236796 CLC CI.SRT..HN 6137990 2019-07-04 17:02:59.908 0.879 P 0.01 0.699 -0.643401 1101 1.377537 4.387864e-07 1 -0.069463 0.065039 SRT CI.SRT..HH 6137990 2019-07-04 17:02:59.908 0.891 P 0.01 0.734 -0.601019 1101 0.729745 5.910421e-07 1 -0.078144 0.020251 SRT ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... CI.WMF..HH 6139185 2019-07-04 17:03:11.858 0.672 S 0.01 0.021 -1.368759 1101 1.096820 3.073536e-07 1 0.140179 -0.329117 WMF CI.WMF..HN 6139189 2019-07-04 17:03:11.898 0.633 S 0.01 0.026 -1.371407 1101 0.989939 2.891483e-07 1 0.159858 -0.171350 WMF CI.DTP..BH 6139200 2019-07-04 17:03:12.019 0.734 S 0.01 0.035 -1.322484 1101 1.385395 1.624267e-07 1 0.068940 0.175198 DTP CI.DTP..HH 6139203 2019-07-04 17:03:12.038 0.707 S 0.01 -0.015 -1.306977 1101 1.122809 3.205840e-07 1 0.052824 0.125587 DTP CI.DTP..HN 6139211 2019-07-04 17:03:12.118 0.625 S 0.01 0.043 -1.282829 1101 0.863380 2.702479e-07 1 0.105026 0.211253 DTP <p>90 rows \u00d7 14 columns</p> <p>In the following cell, we build the <code>numpy.ndarray</code> of moveouts $\\tilde{\\tau}_{s,c}$, expressed in units of samples.</p> In\u00a0[24]: Copied! <pre># first, we extract the set of relative delay times of the beginning of each\n# template window on a given station and component\ntau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        phase_type = PHASE_ON_COMP[cp]\n        picks_s_c = selected_event_picks[\n            (\n                (selected_event_picks[\"station_code\"] == sta)\n                &amp; (selected_event_picks[\"phase_type\"] == phase_type)\n            )\n        ]\n        if len(picks_s_c) == 0:\n            # no pick for this station/component: set to -999\n            tau_s_c_sec[s, c] = -999\n        elif len(picks_s_c) == 1:\n            # express pick relative to beginning of day (midnight)\n            _pick = pd.Timestamp(picks_s_c[\"phase_time\"])\n            _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n        else:\n            # there were several picks from different channels: average them\n            _relative_pick_sec = 0.\n            for _pick in picks_s_c[\"phase_time\"].values:\n                _pick = pd.Timestamp(_pick)\n                _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()\n            _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))\n            tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]\n# now, we convert these relative times into samples \n# and express them relative to the earliest time\n# we also store in memory the minimum time offset `tau_min_samp` for the next step\nmoveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64)\ntau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0])\nmoveouts_samp_arr = moveouts_samp_arr - tau_min_samp\nmoveouts_samp_arr\n</pre> # first, we extract the set of relative delay times of the beginning of each # template window on a given station and component tau_s_c_sec = np.zeros((num_stations, num_channels), dtype=np.float64) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         phase_type = PHASE_ON_COMP[cp]         picks_s_c = selected_event_picks[             (                 (selected_event_picks[\"station_code\"] == sta)                 &amp; (selected_event_picks[\"phase_type\"] == phase_type)             )         ]         if len(picks_s_c) == 0:             # no pick for this station/component: set to -999             tau_s_c_sec[s, c] = -999         elif len(picks_s_c) == 1:             # express pick relative to beginning of day (midnight)             _pick = pd.Timestamp(picks_s_c[\"phase_time\"])             _relative_pick_sec = (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type]         else:             # there were several picks from different channels: average them             _relative_pick_sec = 0.             for _pick in picks_s_c[\"phase_time\"].values:                 _pick = pd.Timestamp(_pick)                 _relative_pick_sec += (_pick - pd.Timestamp(_pick.strftime(\"%Y-%m-%d\"))).total_seconds()             _relative_pick_sec /= float(len(picks_s_c[\"phase_time\"]))             tau_s_c_sec[s, c] = _relative_pick_sec - OFFSET_PHASE_SEC[phase_type] # now, we convert these relative times into samples  # and express them relative to the earliest time # we also store in memory the minimum time offset `tau_min_samp` for the next step moveouts_samp_arr = (tau_s_c_sec * SAMPLING_RATE_HZ).astype(np.int64) tau_min_samp = np.min(moveouts_samp_arr[moveouts_samp_arr &gt; 0]) moveouts_samp_arr = moveouts_samp_arr - tau_min_samp moveouts_samp_arr Out[24]: <pre>array([[-1559399, -1559399, -1559399],\n       [-1559399, -1559399, -1559399],\n       [     156,      156,      104],\n       [      78,       78,       53],\n       [-1559399, -1559399, -1559399],\n       [     225,      225,      140],\n       [-1559399, -1559399, -1559399],\n       [     101,      101,       68],\n       [     272,      272,      171],\n       [     277,      277,      174],\n       [      91,       91,       62],\n       [     179,      179,      116],\n       [     160,      160,      105],\n       [       0,        0,       13],\n       [     175,      175,      115],\n       [     102,      102,       64],\n       [      94,       94,       66],\n       [      72,       72,       48],\n       [     165,      165,      105],\n       [     159,      159,      102],\n       [-1559399, -1559399, -1559399]])</pre> <p>Next, we use the moveouts, in samples, to clip out the relevant template waveforms from the continuous seismograms.</p> In\u00a0[25]: Copied! <pre>template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32)\nweights_arr = np.ones((num_stations, num_channels), dtype=np.float32)\n\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        if moveouts_samp_arr[s, c] &lt; 0:\n            # no picks were found on this station\n            weights_arr[s, c] = 0.\n            continue\n        starttime = tau_min_samp + moveouts_samp_arr[s, c]\n        endtime = starttime + TEMPLATE_DURATION_SAMP\n        template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]\n        if template_waveforms_arr[s, c, :].sum() == 0.:\n            # no data was available on this channel\n            weights_arr[s, c] = 0.\n        \ntemplate_waveforms_arr\n</pre> template_waveforms_arr = np.zeros((num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32) weights_arr = np.ones((num_stations, num_channels), dtype=np.float32)  for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         if moveouts_samp_arr[s, c] &lt; 0:             # no picks were found on this station             weights_arr[s, c] = 0.             continue         starttime = tau_min_samp + moveouts_samp_arr[s, c]         endtime = starttime + TEMPLATE_DURATION_SAMP         template_waveforms_arr[s, c, :] = continuous_seismograms_arr[s, c, starttime:endtime]         if template_waveforms_arr[s, c, :].sum() == 0.:             # no data was available on this channel             weights_arr[s, c] = 0.          template_waveforms_arr Out[25]: <pre>array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n\n       [[-1.15102615e-04, -2.08503159e-04, -8.57295672e-05, ...,\n         -3.02342505e-05,  1.90505059e-04,  2.32697013e-04],\n        [-6.95939161e-06, -1.14591476e-05, -4.32817069e-05, ...,\n          1.95618981e-04, -6.08503397e-05, -1.64061450e-04],\n        [-3.15401110e-08,  6.19310697e-07,  1.24642884e-06, ...,\n         -6.75416959e-05,  9.95227219e-06,  9.39235106e-05]],\n\n       ...,\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 6.11326456e-08,  1.84645728e-07,  2.68150814e-07, ...,\n         -1.40672455e-05,  9.12318046e-06,  1.10272435e-06]],\n\n       [[ 9.79182732e-05,  1.20548120e-05, -6.78718134e-05, ...,\n         -4.39741780e-05, -8.56743718e-05, -4.07844163e-05],\n        [-1.06544954e-04, -2.09494847e-05,  3.22320338e-05, ...,\n         -2.05596989e-05, -2.35213724e-06,  1.43820762e-05],\n        [ 7.50264846e-08, -1.39887931e-07, -4.09550466e-07, ...,\n          2.37395070e-05, -6.56054617e-05,  1.63327525e-06]],\n\n       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n      dtype=float32)</pre> In\u00a0[64]: Copied! <pre>fig, axes = plt.subplots(num=\"template_waveforms\", nrows=num_stations, ncols=num_channels, figsize=(16, 20), sharex=True)\nfor s, sta in enumerate(station_codes):\n    for c, cp in enumerate(component_codes):\n        time_sec = (max(0., moveouts_samp_arr[s, c]) + np.arange(TEMPLATE_DURATION_SAMP)) / SAMPLING_RATE_HZ\n        axes[s, c].plot(\n            time_sec,\n            template_waveforms_arr[s, c, :],\n            color=\"k\",\n            lw=0.75\n        )\n        axes[s, c].text(0.02, 0.96, sta + \" \" + cp, ha=\"left\", va=\"top\", transform=axes[s, c].transAxes)\n        # axes[s, c].set_title(sta + \" \" + cp)\n        if s == num_stations - 1:\n            axes[s, c].set_xlabel(\"Time (s)\")\n        if c == 0:\n            axes[s, c].set_ylabel(\"Amp.\")\n        axes[s, c].grid()\n# plt.tight_layout()\n</pre> fig, axes = plt.subplots(num=\"template_waveforms\", nrows=num_stations, ncols=num_channels, figsize=(16, 20), sharex=True) for s, sta in enumerate(station_codes):     for c, cp in enumerate(component_codes):         time_sec = (max(0., moveouts_samp_arr[s, c]) + np.arange(TEMPLATE_DURATION_SAMP)) / SAMPLING_RATE_HZ         axes[s, c].plot(             time_sec,             template_waveforms_arr[s, c, :],             color=\"k\",             lw=0.75         )         axes[s, c].text(0.02, 0.96, sta + \" \" + cp, ha=\"left\", va=\"top\", transform=axes[s, c].transAxes)         # axes[s, c].set_title(sta + \" \" + cp)         if s == num_stations - 1:             axes[s, c].set_xlabel(\"Time (s)\")         if c == 0:             axes[s, c].set_ylabel(\"Amp.\")         axes[s, c].grid() # plt.tight_layout() In\u00a0[\u00a0]: Copied! <pre># normalize template waveforms for numerical reasons\nnorm = np.std(template_waveforms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ntemplate_waveforms_arr /= norm\n\n# normalize weights so that they sum up to one\nweights_arr /= np.sum(weights_arr)\n\n# normalize continuous seismograms for numerical reasons\nnorm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True)\nnorm[norm == 0.] = 1. \ncontinuous_seismograms_arr /= norm\n</pre> # normalize template waveforms for numerical reasons norm = np.std(template_waveforms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  template_waveforms_arr /= norm  # normalize weights so that they sum up to one weights_arr /= np.sum(weights_arr)  # normalize continuous seismograms for numerical reasons norm = np.std(continuous_seismograms_arr, axis=-1, keepdims=True) norm[norm == 0.] = 1.  continuous_seismograms_arr /= norm In\u00a0[27]: Copied! <pre># FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient\nFMF_STEP_SAMP = 1\n# ARCH: it determines whether you want to use GPUs or CPUs \n#       If you do not have an Nvidia GPU, set ARCH = \"cpu\"\nARCH = \"gpu\"\n</pre> # FMF_STEP_SAMP: this is the step between two consecutive calculation of the correlation coefficient FMF_STEP_SAMP = 1 # ARCH: it determines whether you want to use GPUs or CPUs  #       If you do not have an Nvidia GPU, set ARCH = \"cpu\" ARCH = \"gpu\" <p>The following cell computes the time series of correlation coefficients between the template waveforms, $T_{s,c}$, and the continuous seismograms, $u_{s,c}$: $$ CC(t) = \\sum_{s,c} w_{s,c} \\sum_{i=1}^N \\dfrac{T^*_{s,c}(n \\Delta t) u^*_{s,c}(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}{\\sqrt{\\sum_{i=1}^N {T^*_{s,c}}^2(n \\Delta t) \\sum_{i=1}^N {u^*_{s,c}}^2(t + \\tilde{\\tau}_{s,c} + n \\Delta t)}}, $$ with:</p> <ul> <li>$T^*_{s,c} = T_{s,c} - \\frac{1}{N} \\sum_{i=1}^N T_{s,c}(n \\Delta t)$,</li> <li>$u^*_{s,c}(t) = u_{s,c}(t) - \\frac{1}{N} \\sum_{i=1}^N u_{s,c}(t + n \\Delta t)$.</li> </ul> <p>Note that because the seismograms were filtered below periods that are shorter than the template window ($N \\Delta t$) we have $T^*_{s,c} \\approx T_{s,c}$ and $u^*_{s,c} \\approx u_{s,c}$, which spares us the computation of the mean in each sliding window.</p> In\u00a0[28]: Copied! <pre>cc = fmf.matched_filter(\n    template_waveforms_arr.astype(np.float32),\n    moveouts_samp_arr.astype(np.int32),\n    weights_arr.astype(np.float32),\n    continuous_seismograms_arr.astype(np.float32),\n    FMF_STEP_SAMP,\n    arch=ARCH,\n)\n</pre> cc = fmf.matched_filter(     template_waveforms_arr.astype(np.float32),     moveouts_samp_arr.astype(np.int32),     weights_arr.astype(np.float32),     continuous_seismograms_arr.astype(np.float32),     FMF_STEP_SAMP,     arch=ARCH, ) In\u00a0[29]: Copied! <pre># FMF is programmed to handle multiple templates at once. Here, we only used\n# a single template, hence the size of the outermost axis of \"1\"\ncc.shape\n</pre> # FMF is programmed to handle multiple templates at once. Here, we only used # a single template, hence the size of the outermost axis of \"1\" cc.shape Out[29]: <pre>(1, 2159801)</pre> In\u00a0[37]: Copied! <pre># let's print the output of our template matching run, which a time series of network-averaged correlation coefficients\n# of same duration as the continuous seismograms\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ\n\nfig = plt.figure(\"network_averaged_cc\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nfor ax in [ax1, ax2]:\n    ax.grid()\n</pre> # let's print the output of our template matching run, which a time series of network-averaged correlation coefficients # of same duration as the continuous seismograms _cc = cc[0, :] time_cc = np.arange(len(_cc)) / SAMPLING_RATE_HZ  fig = plt.figure(\"network_averaged_cc\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  for ax in [ax1, ax2]:     ax.grid()  In\u00a0[32]: Copied! <pre>def select_cc_indexes(\n    cc_t,\n    threshold,\n    search_win,\n):\n    \"\"\"Select the peaks in the CC time series.\n\n    Parameters\n    ------------\n    cc_t: (n_corr,) numpy.ndarray\n        The CC time series for one template.\n    threshold: (n_corr,) numpy.ndarray or scalar\n        The detection threshold.\n    search_win: scalar int\n        The minimum inter-event time, in units of correlation step.\n\n\n    Returns\n    --------\n    cc_idx: (n_detections,) numpy.ndarray\n        The list of all selected CC indexes. They give the timings of the\n        detected events.\n    \"\"\"\n\n    cc_detections = cc_t &gt; threshold\n    cc_idx = np.where(cc_detections)[0]\n\n    cc_idx = list(cc_idx)\n    n_rm = 0\n    for i in range(1, len(cc_idx)):\n        if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:\n            if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:\n                # keep (i-n_rm)-th detection\n                cc_idx.remove(cc_idx[i - n_rm - 1])\n            else:\n                # keep (i-n_rm-1)-th detection\n                cc_idx.remove(cc_idx[i - n_rm])\n            n_rm += 1\n    cc_idx = np.asarray(cc_idx)\n    return cc_idx\n    \n</pre> def select_cc_indexes(     cc_t,     threshold,     search_win, ):     \"\"\"Select the peaks in the CC time series.      Parameters     ------------     cc_t: (n_corr,) numpy.ndarray         The CC time series for one template.     threshold: (n_corr,) numpy.ndarray or scalar         The detection threshold.     search_win: scalar int         The minimum inter-event time, in units of correlation step.       Returns     --------     cc_idx: (n_detections,) numpy.ndarray         The list of all selected CC indexes. They give the timings of the         detected events.     \"\"\"      cc_detections = cc_t &gt; threshold     cc_idx = np.where(cc_detections)[0]      cc_idx = list(cc_idx)     n_rm = 0     for i in range(1, len(cc_idx)):         if (cc_idx[i - n_rm] - cc_idx[i - n_rm - 1]) &lt; search_win:             if cc_t[cc_idx[i - n_rm]] &gt; cc_t[cc_idx[i - n_rm - 1]]:                 # keep (i-n_rm)-th detection                 cc_idx.remove(cc_idx[i - n_rm - 1])             else:                 # keep (i-n_rm-1)-th detection                 cc_idx.remove(cc_idx[i - n_rm])             n_rm += 1     cc_idx = np.asarray(cc_idx)     return cc_idx      In\u00a0[35]: Copied! <pre># INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because\n#                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates\n#                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we\n#                                 set a minimum time separation between triggers (rule of thumb: about half the template duration)\nINTEREVENT_TIME_RESOLUTION_SEC = 5.\nINTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ)\n_cc = cc[0, :]\ntime_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ\nNUM_RMS = 8.\ndetection_threshold = NUM_RMS * np.std(_cc)\nevent_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)\n\nfig = plt.figure(\"network_averaged_cc_w_threshold\", figsize=(20, 6))\ngs = fig.add_gridspec(ncols=4)\n\nax1 = fig.add_subplot(gs[:3])\nax1.plot(time_cc, _cc, lw=0.75)\nax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2)\nax1.set_xlabel(\"Elapsed time (sec)\")\nax1.set_ylabel(\"Network-averaged cc\")\nax1.set_xlim(time_cc.min(), time_cc.max())\nax1.set_title(\"Time series of template/continuous seismograms similarity\")\n\nax2 = fig.add_subplot(gs[3], sharey=ax1)\n_ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2)\nax2.set_xlabel(\"Normalized count\")\nax2.set_title(\"Histogram\")\n\nlabel = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\"\nfor ax in [ax1, ax2]:\n    ax.grid()\n    ax.axhline(\n        detection_threshold, ls=\"--\", color=\"r\",\n        label=label\n        )\nax1.legend(loc=\"upper left\")\n</pre> # INTEREVENT_TIME_RESOLUTION_SEC: In some cases, a template might trigger multiple, closely spaced detections because #                                 of a phenomenon similar to that of \"cycle skipping\", where the waveform correlates #                                 well with a time-shifted version of itself. Thus, to avoid redundant detections, we #                                 set a minimum time separation between triggers (rule of thumb: about half the template duration) INTEREVENT_TIME_RESOLUTION_SEC = 5. INTEREVENT_TIME_RESOLUTION_SAMP = int(INTEREVENT_TIME_RESOLUTION_SEC * SAMPLING_RATE_HZ) _cc = cc[0, :] time_cc = np.arange(len(_cc)) * FMF_STEP_SAMP / SAMPLING_RATE_HZ NUM_RMS = 8. detection_threshold = NUM_RMS * np.std(_cc) event_cc_indexes = select_cc_indexes(_cc, detection_threshold, INTEREVENT_TIME_RESOLUTION_SAMP)  fig = plt.figure(\"network_averaged_cc_w_threshold\", figsize=(20, 6)) gs = fig.add_gridspec(ncols=4)  ax1 = fig.add_subplot(gs[:3]) ax1.plot(time_cc, _cc, lw=0.75) ax1.scatter(time_cc[event_cc_indexes], _cc[event_cc_indexes], linewidths=0.25, edgecolor=\"k\", color=\"r\", zorder=2) ax1.set_xlabel(\"Elapsed time (sec)\") ax1.set_ylabel(\"Network-averaged cc\") ax1.set_xlim(time_cc.min(), time_cc.max()) ax1.set_title(\"Time series of template/continuous seismograms similarity\")  ax2 = fig.add_subplot(gs[3], sharey=ax1) _ = ax2.hist(_cc, orientation=\"horizontal\", bins=250, density=True, zorder=2) ax2.set_xlabel(\"Normalized count\") ax2.set_title(\"Histogram\")  label = f\"Detection threshold: {NUM_RMS:.0f}\"r\"$\\times \\mathrm{RMS}(\\mathrm{CC}(t))$\"f\"\\n{len(event_cc_indexes):d} detected events\" for ax in [ax1, ax2]:     ax.grid()     ax.axhline(         detection_threshold, ls=\"--\", color=\"r\",         label=label         ) ax1.legend(loc=\"upper left\") Out[35]: <pre>&lt;matplotlib.legend.Legend at 0x7fadd03341c0&gt;</pre> <p>Use the trigger times to build the earthquake catalog and extract event waveforms on a given station/component.</p> In\u00a0[39]: Copied! <pre>STATION_NAME = \"CLC\"\nCOMPONENT_NAME = \"Z\"\ndate = pd.Timestamp(\n    (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,\n    unit=\"s\"\n).strftime(\"%Y-%m-%d\")\n\nsta_idx = station_codes.index(STATION_NAME)\ncp_idx = component_codes.index(COMPONENT_NAME)\n\ncatalog = {\n    \"detection_time\": [],\n    \"peak_amplitude\": [],\n    \"cc\": [],\n    \"normalized_cc\": []\n}\n\ndetected_event_waveforms = []\nfor i in range(len(event_cc_indexes)):\n    idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]\n    idx_end = idx_start + TEMPLATE_DURATION_SAMP\n    detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])\n    # --------------------------------------\n    detection_time = pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\")\n    cc = _cc[event_cc_indexes[i]]\n    normalized_cc = cc / detection_threshold\n    peak_amplitude = np.abs(detected_event_waveforms[-1]).max()\n    catalog[\"detection_time\"].append(detection_time)\n    catalog[\"peak_amplitude\"].append(peak_amplitude)\n    catalog[\"cc\"].append(cc)\n    catalog[\"normalized_cc\"].append(normalized_cc)\ndetected_event_waveforms = np.asarray(detected_event_waveforms)\ncatalog = pd.DataFrame(catalog)\ncatalog\n</pre> STATION_NAME = \"CLC\" COMPONENT_NAME = \"Z\" date = pd.Timestamp(     (continuous_seismograms[0].stats.starttime.timestamp + continuous_seismograms[0].stats.endtime.timestamp) / 2.,     unit=\"s\" ).strftime(\"%Y-%m-%d\")  sta_idx = station_codes.index(STATION_NAME) cp_idx = component_codes.index(COMPONENT_NAME)  catalog = {     \"detection_time\": [],     \"peak_amplitude\": [],     \"cc\": [],     \"normalized_cc\": [] }  detected_event_waveforms = [] for i in range(len(event_cc_indexes)):     idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]     idx_end = idx_start + TEMPLATE_DURATION_SAMP     detected_event_waveforms.append(continuous_seismograms_arr[sta_idx, cp_idx, idx_start:idx_end])     # --------------------------------------     detection_time = pd.Timestamp(date) + pd.Timedelta(event_cc_indexes[i] * FMF_STEP_SAMP / SAMPLING_RATE_HZ, \"s\")     cc = _cc[event_cc_indexes[i]]     normalized_cc = cc / detection_threshold     peak_amplitude = np.abs(detected_event_waveforms[-1]).max()     catalog[\"detection_time\"].append(detection_time)     catalog[\"peak_amplitude\"].append(peak_amplitude)     catalog[\"cc\"].append(cc)     catalog[\"normalized_cc\"].append(normalized_cc) detected_event_waveforms = np.asarray(detected_event_waveforms) catalog = pd.DataFrame(catalog) catalog Out[39]: detection_time peak_amplitude cc normalized_cc 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378858 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506305 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888470 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482784 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524972 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 In\u00a0[40]: Copied! <pre>fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15))\ngs = fig.add_gridspec(nrows=4)\n\nax1 = fig.add_subplot(gs[:3])\n\n_time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ\n\nstack = np.zeros(detected_event_waveforms.shape[1])\nfor i in range(detected_event_waveforms.shape[0]):\n    norm = np.abs(detected_event_waveforms[i, :]).max()\n    if catalog[\"cc\"].iloc[i] &gt; 0.999:\n        color = \"r\"\n        template_wav = detected_event_waveforms[i, :] / norm\n    else:\n        color = \"k\"\n    time_of_day = catalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")\n    ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)\n    ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")\n    stack += detected_event_waveforms[i, :] / norm\nstack /= np.abs(stack).max()\nax1.set_xlabel(\"Time (s)\")\nax1.set_xlim(_time_wav.min(), _time_wav.max())\nax1.set_ylabel(\"Normalized offset amplitude\")\nax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")\n\nax2 = fig.add_subplot(gs[3], sharex=ax1)\nax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\")\nax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\")\nax2.legend(loc=\"upper left\")\nax2.set_xlabel(\"Time (s)\")\nax2.set_ylabel(\"Normalized amplitude\")\n</pre> fig = plt.figure(\"detected_event_waveforms\", figsize=(10, 15)) gs = fig.add_gridspec(nrows=4)  ax1 = fig.add_subplot(gs[:3])  _time_wav = np.arange(detected_event_waveforms.shape[1]) / SAMPLING_RATE_HZ  stack = np.zeros(detected_event_waveforms.shape[1]) for i in range(detected_event_waveforms.shape[0]):     norm = np.abs(detected_event_waveforms[i, :]).max()     if catalog[\"cc\"].iloc[i] &gt; 0.999:         color = \"r\"         template_wav = detected_event_waveforms[i, :] / norm     else:         color = \"k\"     time_of_day = catalog[\"detection_time\"].iloc[i].strftime(\"%H:%M:%S\")     ax1.plot(_time_wav, detected_event_waveforms[i, :] / norm + i * 1.5, color=color)     ax1.text(0.98 * _time_wav.max(), i * 1.5 + 0.1, time_of_day, ha=\"right\", va=\"bottom\")     stack += detected_event_waveforms[i, :] / norm stack /= np.abs(stack).max() ax1.set_xlabel(\"Time (s)\") ax1.set_xlim(_time_wav.min(), _time_wav.max()) ax1.set_ylabel(\"Normalized offset amplitude\") ax1.set_title(f\"Events detected on {date} and recorded by {STATION_NAME}.{COMPONENT_NAME}\")  ax2 = fig.add_subplot(gs[3], sharex=ax1) ax2.plot(_time_wav, stack, color=\"blue\", label=\"Stacked waveforms\") ax2.plot(_time_wav, template_wav, color=\"red\", ls=\"--\", label=\"Template waveform\") ax2.legend(loc=\"upper left\") ax2.set_xlabel(\"Time (s)\") ax2.set_ylabel(\"Normalized amplitude\") Out[40]: <pre>Text(0, 0.5, 'Normalized amplitude')</pre> In\u00a0[42]: Copied! <pre>catalog[\"return_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds()\ncatalog\n</pre> catalog[\"return_time_s\"] = catalog[\"detection_time\"].diff().dt.total_seconds() catalog Out[42]: detection_time peak_amplitude cc normalized_cc return_time_s 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378858 1472.04 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506305 2952.00 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888470 384.72 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482784 317.48 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524972 2715.32 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 In\u00a0[43]: Copied! <pre>fig, ax = plt.subplots(num=\"return_time_vs_detection_time\", figsize=(16, 7))\n\nax.scatter(catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50)\nax.set_xlabel(\"Detection time\")\nax.set_ylabel(\"Return time (s)\")\nax.set_title(\"Return time vs detection time\")\nax.grid()\ncbar = plt.colorbar(ax.collections[0], ax=ax)\ncbar.set_label(\"CC\")\nax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"return_time_vs_detection_time\", figsize=(16, 7))  ax.scatter(catalog[\"detection_time\"], catalog[\"return_time_s\"], c=catalog[\"cc\"], linewidths=0.25, edgecolor=\"k\", cmap=\"magma\", s=50) ax.set_xlabel(\"Detection time\") ax.set_ylabel(\"Return time (s)\") ax.set_title(\"Return time vs detection time\") ax.grid() cbar = plt.colorbar(ax.collections[0], ax=ax) cbar.set_label(\"CC\") ax.set_yscale(\"log\") In\u00a0[44]: Copied! <pre>M_REF = selected_event_meta.magnitude\n</pre> M_REF = selected_event_meta.magnitude <p>We can use the peak amplitudes that we read before.</p> In\u00a0[45]: Copied! <pre>template_index = np.where(catalog[\"cc\"] &gt; 0.999)[0][0]\n\nm_rel = M_REF + np.log10(catalog[\"peak_amplitude\"] / catalog[\"peak_amplitude\"].iloc[template_index])\n\ncatalog[\"m_rel_one_channel\"] = m_rel\ncatalog\n</pre> template_index = np.where(catalog[\"cc\"] &gt; 0.999)[0][0]  m_rel = M_REF + np.log10(catalog[\"peak_amplitude\"] / catalog[\"peak_amplitude\"].iloc[template_index])  catalog[\"m_rel_one_channel\"] = m_rel catalog Out[45]: detection_time peak_amplitude cc normalized_cc return_time_s m_rel_one_channel 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 0.750315 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378858 1472.04 0.604785 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 1.814178 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506305 2952.00 4.476724 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888470 384.72 2.459368 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 1.091826 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 2.331995 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 1.754599 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 0.545083 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 0.840592 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 1.551985 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 0.588959 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 0.116397 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 0.393615 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 -0.096164 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 0.651551 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482784 317.48 0.747959 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 4.242001 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 3.802184 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 2.464358 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 2.709977 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 2.690371 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 2.331373 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524972 2715.32 1.637578 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 1.428355 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 1.696379 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 1.898305 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 1.359715 <p>However, it is much better to average the log ratios over multiple channels.</p> In\u00a0[46]: Copied! <pre># first, extract clips from the continuous seismograms\ndetected_event_waveforms_all_channels = np.zeros(\n    (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32\n)\nfor i in range(len(catalog)):\n    for s in range(num_stations):\n        for c in range(num_channels):\n            idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]\n            idx_end = idx_start + TEMPLATE_DURATION_SAMP\n            detected_event_waveforms_all_channels[i, s, c, :] = (\n                continuous_seismograms_arr[s, c, idx_start:idx_end]\n                )\n</pre> # first, extract clips from the continuous seismograms detected_event_waveforms_all_channels = np.zeros(     (len(catalog), num_stations, num_channels, TEMPLATE_DURATION_SAMP), dtype=np.float32 ) for i in range(len(catalog)):     for s in range(num_stations):         for c in range(num_channels):             idx_start = event_cc_indexes[i] * FMF_STEP_SAMP + moveouts_samp_arr[sta_idx, cp_idx]             idx_end = idx_start + TEMPLATE_DURATION_SAMP             detected_event_waveforms_all_channels[i, s, c, :] = (                 continuous_seismograms_arr[s, c, idx_start:idx_end]                 )  In\u00a0[47]: Copied! <pre># then, measure the peak amplitudes\namplitude_ratios = (\n    np.max(detected_event_waveforms_all_channels, axis=-1)\n    / np.max(detected_event_waveforms_all_channels[template_index, ...], axis=-1)[None, ...]\n)\n\n# if some data were missing, we may have divided by 0 and nans or infs\n# mask them and ignore them in the following calculation\ninvalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios))\namplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)\n\n# apply the above formula to get the relative magnitudes\nm_rel_all_channels = M_REF + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))\n\ncatalog[\"m_rel_all_channels\"] = m_rel_all_channels\ncatalog\n</pre> # then, measure the peak amplitudes amplitude_ratios = (     np.max(detected_event_waveforms_all_channels, axis=-1)     / np.max(detected_event_waveforms_all_channels[template_index, ...], axis=-1)[None, ...] )  # if some data were missing, we may have divided by 0 and nans or infs # mask them and ignore them in the following calculation invalid = (np.isnan(amplitude_ratios) | np.isinf(amplitude_ratios)) amplitude_ratios = np.ma.masked_where(invalid, amplitude_ratios)  # apply the above formula to get the relative magnitudes m_rel_all_channels = M_REF + np.ma.mean(np.log10(amplitude_ratios), axis=(1, 2))  catalog[\"m_rel_all_channels\"] = m_rel_all_channels catalog <pre>/tmp/ipykernel_471469/457280950.py:3: RuntimeWarning: invalid value encountered in true_divide\n  np.max(detected_event_waveforms_all_channels, axis=-1)\n</pre> Out[47]: detection_time peak_amplitude cc normalized_cc return_time_s m_rel_one_channel m_rel_all_channels 0 2019-07-04 15:42:49.840 0.003044 0.167660 1.426171 NaN 0.750315 0.948512 1 2019-07-04 16:07:21.880 0.002178 0.162098 1.378858 1472.04 0.604785 0.914793 2 2019-07-04 16:13:44.960 0.035267 0.252016 2.143727 383.08 1.814178 1.781098 3 2019-07-04 17:02:56.960 16.214684 1.000000 8.506305 2952.00 4.476724 4.476724 4 2019-07-04 17:09:21.680 0.155795 0.339568 2.888470 384.72 2.459368 2.556656 5 2019-07-04 17:11:41.320 0.006684 0.120572 1.025621 139.64 1.091826 1.440504 6 2019-07-04 17:12:16.520 0.116193 0.181086 1.540373 35.20 2.331995 2.389888 7 2019-07-04 17:12:39.680 0.030746 0.150769 1.282485 23.16 1.754599 2.043036 8 2019-07-04 17:13:28.560 0.001898 0.156879 1.334461 48.88 0.545083 1.010687 9 2019-07-04 17:13:53.480 0.003748 0.125633 1.068674 24.92 0.840592 1.131839 10 2019-07-04 17:15:48.640 0.019283 0.254803 2.167428 115.16 1.551985 1.764089 11 2019-07-04 17:16:20.080 0.002100 0.200094 1.702063 31.44 0.588959 1.078424 12 2019-07-04 17:17:43.000 0.000707 0.123891 1.053855 82.92 0.116397 0.674097 13 2019-07-04 17:18:09.040 0.001339 0.118229 1.005688 26.04 0.393615 0.755659 14 2019-07-04 17:20:56.360 0.000434 0.154932 1.317895 167.32 -0.096164 0.674497 15 2019-07-04 17:27:36.680 0.002425 0.143366 1.219511 400.32 0.651551 1.081399 16 2019-07-04 17:32:54.160 0.003028 0.174316 1.482784 317.48 0.747959 1.004731 17 2019-07-04 17:37:28.920 9.444638 0.129424 1.100916 274.76 4.242001 4.492626 18 2019-07-04 17:39:37.680 3.430590 0.118089 1.004503 128.76 3.802184 4.082750 19 2019-07-04 18:10:10.960 0.157595 0.127386 1.083580 1833.28 2.464358 2.506171 20 2019-07-04 18:19:09.040 0.277436 0.121896 1.036887 538.08 2.709977 3.136539 21 2019-07-04 19:03:40.560 0.265189 0.148633 1.264321 2671.52 2.690371 2.455167 22 2019-07-04 20:09:45.160 0.116027 0.146313 1.244580 3964.60 2.331373 2.180167 23 2019-07-04 20:55:00.480 0.023483 0.179276 1.524972 2715.32 1.637578 1.975095 24 2019-07-04 21:20:53.760 0.014506 0.133311 1.133981 1553.28 1.428355 1.859795 25 2019-07-04 21:46:58.000 0.026888 0.132058 1.123324 1564.24 1.696379 2.244630 26 2019-07-04 21:50:55.440 0.042805 0.216717 1.843457 237.44 1.898305 2.152389 27 2019-07-04 23:38:45.680 0.012385 0.130068 1.106398 6470.24 1.359715 1.358247 <p>Next, we plot the distribution of earthquake magnitudes. The cumulative distribution usually follows the so-called Gutenberg-Richter law: $$ \\log N(m \\geq M) = a - b M$$ In this example, the number of events is too low to estimate a meaningful b-value.</p> In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8))\naxb = ax.twinx()\nax.set_title(\"Distribution of earthquake magnitudes\")\n\n_ = ax.hist(catalog[\"m_rel_all_channels\"], bins=15, color=\"k\", alpha=0.5)\naxb.plot(np.sort(catalog[\"m_rel_all_channels\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2)\n\nax.set_xlabel(\"Magnitude\")\nax.set_ylabel(\"Event Count\")\naxb.set_ylabel(\"Cumulative Event Count\")\n\nfor ax in [ax, axb]:\n    ax.set_yscale(\"log\")\n</pre> fig, ax = plt.subplots(num=\"magnitude_distribution\", figsize=(8, 8)) axb = ax.twinx() ax.set_title(\"Distribution of earthquake magnitudes\")  _ = ax.hist(catalog[\"m_rel_all_channels\"], bins=15, color=\"k\", alpha=0.5) axb.plot(np.sort(catalog[\"m_rel_all_channels\"]), np.arange(len(catalog))[::-1], color=\"k\", lw=2)  ax.set_xlabel(\"Magnitude\") ax.set_ylabel(\"Event Count\") axb.set_ylabel(\"Cumulative Event Count\")  for ax in [ax, axb]:     ax.set_yscale(\"log\")"},{"location":"notebooks/tm_one_template/#template-matching-with-a-single-template","title":"Template matching with a single template\u00b6","text":"<p>Templates are selected from Weiqiang Zhu's PhaseNet catalog.</p>"},{"location":"notebooks/tm_one_template/#data-requirements","title":"Data requirements\u00b6","text":"<p>Download the seismic data at: https://doi.org/10.5281/zenodo.15097180</p> <p></p> <p>Download the PhaseNet earthquake catalog with the following three commands:</p> <ul> <li>curl -o adloc_events.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_events.csv</li> <li>curl -o adloc_picks.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_picks.csv</li> <li>curl -o adloc_stations.csv https://storage.googleapis.com/quakeflow_share/Ridgecrest/adloc/adloc_stations.csv</li> </ul>"},{"location":"notebooks/tm_one_template/#installing-fast_matched_filter","title":"Installing <code>fast_matched_filter</code>\u00b6","text":"<p>This example uses <code>fast_matched_filter</code>, a Python wrapper for C and CUDA-C routines. The C and CUDA-C libraries have to be compiled. Read the instructions at https://ebeauce.github.io/FMF_documentation/introduction.html.</p>"},{"location":"notebooks/tm_one_template/#load-phasenet-catalog","title":"Load PhaseNet catalog\u00b6","text":"<p>Here, we read the catalog of the 2019 Ridgecrest sequence made with PhaseNet. Information is divided into three files:</p> <ul> <li>a station metadata file,</li> <li>an event metadata file (the catalog per se),</li> <li>a pick database, which contains all the P- and S-wave picks found by PhaseNet.</li> </ul>"},{"location":"notebooks/tm_one_template/#pick-one-event","title":"Pick one event\u00b6","text":"<p>Let's use the PhaseNet catalog to read the waveforms of an event.</p>"},{"location":"notebooks/tm_one_template/#run-template-matching","title":"Run template matching\u00b6","text":"<p>We will now use one of the events from the PhaseNet catalog as a template event to detect events with template matching.</p>"},{"location":"notebooks/tm_one_template/#read-data-from-same-day","title":"Read data from same day\u00b6","text":""},{"location":"notebooks/tm_one_template/#build-template","title":"Build template\u00b6","text":""},{"location":"notebooks/tm_one_template/#-background-","title":"------------------ Background ------------------\u00b6","text":"<p>A template is a collection of waveforms at different channels, $T_{s,c}(t)$, which are clips taken from the continuous seismograms, $u_{s,c}$. These clips are taken at times defined by: $$ u_{s,c}(t)\\ |\\ t \\in \\lbrace \\tau_{s,c}; \\tau_{s,c} + D \\rbrace, $$ where $\\tau_{s,c}$ is the start time of the template window and $D$ is the template duration.</p> <p>$\\tau_{s,c}$ is given by some prior information on the event: picks or modeled arrival times. The moveouts, $\\tilde{\\tau}_{s,c}$, are the collection of delay times relative to the earliest $\\tau_{s,c}$: $$ \\tilde{\\tau}_{s,c} = \\tau_{s,c} - \\underset{s,c}{\\min} \\lbrace \\tau_{s,c} \\rbrace .$$</p>"},{"location":"notebooks/tm_one_template/#-on-the-necessity-to-clip-template-waveforms-out-of-numpyndarray-instead-of-obspystream-","title":"------------------ On the necessity to clip template waveforms out of <code>numpy.ndarray</code> instead of <code>obspy.Stream</code> ------------------\u00b6","text":"<p>Looking carefully at the output of <code>print(continuous_seismograms.__str__(extended=True))</code>, a few cells before, we see that start times are generally not exactly at midnight. This is a consequence of the discrete nature of the continuous seismograms (here, sampled at 25 samples per second). Thus, in general, the $\\tau_{s,c}$ computed from picks or modeled arrival times fall in between two samples of the seismograms.</p> <p>When running a matched-filter search, we need to make sure the moveouts, $\\tilde{\\tau}_{s,c}$, ultimately expressed in samples, match exactly the times that were used when clipping the template waveforms out of $u_{s,c}$. One way to ensure this is to first cast the $\\tau_{s,c}$ to times in samples and then operate exclusively on the <code>numpy.ndarray</code>: <code>continuous_seismograms_arr</code>:</p> <p>$$ T_{s,c}[t_n] = u_{s,c}[\\tau_{s,c} + n \\Delta t],$$ where $\\Delta t$ is the sampling time.</p>"},{"location":"notebooks/tm_one_template/#clip-out-waveforms-and-moveout-and-station-weight-arrays","title":"Clip out waveforms and moveout and station-weight arrays\u00b6","text":""},{"location":"notebooks/tm_one_template/#run-fmf","title":"Run FMF\u00b6","text":"<p>After all this data formatting, we can now run template matching (also called matched-filtering) to detect new events that are similar to our template event.</p> <p>For that, use the software Fast Matched Filter (FMF): https://github.com/beridel/fast_matched_filter</p> <p>FMF offers C and CUDA-C routines to efficiently run template matching on CPUs, or even on GPUs if available to you.</p>"},{"location":"notebooks/tm_one_template/#set-detection-threshold-and-find-events","title":"Set detection threshold and find events\u00b6","text":"<p>We will use the time series of correlation coefficients to build an earthquake catalog. For that, we need to set a detection threshold and define all times above that threshold as triggers caused by near-repeats of the template event.</p>"},{"location":"notebooks/tm_one_template/#plot-some-waveforms","title":"Plot some waveforms\u00b6","text":"<p>When building a catalog, it is always necessary to visualize some of the detected event waveforms to get a sense of the ratio of true-to-false detection rate.</p> <p>In the following, we plot the waveforms of each detected event and we also compare the stack of all the waveforms to the original template waveform. Since all events share similar waveforms, the stack is similar to the template waveform. Moreover, since noise across all these waveforms sums up incoherently, stacking acts as a denoiser which may help you produce a cleaner version of the template waveform, for example on remote stations.</p>"},{"location":"notebooks/tm_one_template/#plot-template-return-time-vs-detection-time","title":"Plot template return time vs detection time\u00b6","text":""},{"location":"notebooks/tm_one_template/#relative-magnitude","title":"Relative magnitude\u00b6","text":"<p>Template matching lends itself well to quickly estimate relative event magnitudes, relative to the magnitude of the template event.</p> <p>$$ M_r = M_{\\mathrm{ref}} + \\dfrac{1}{N} \\sum_{ch=1}^{N} \\log \\dfrac{A_{ch}}{A_{\\mathrm{ref},ch}} $$</p> <p>where the sum is taken over $N$ channels and $A_{ch}$ is the peak amplitude measured on channel $ch$ while $A_{\\mathrm{ref},ch}$ is the peak amplitude of the reference event (the template) measured on the same channel.</p>"},{"location":"seismic_network/tutorials/","title":"Tutorials","text":""},{"location":"seismic_network/tutorials/#scedc-web-services","title":"SCEDC Web Services","text":"<ul> <li>Retrieve event, station and waveform data for the 2019 Ridgecrest 7.1</li> </ul>"},{"location":"seismic_network/tutorials/#scedc-pystp","title":"SCEDC PySTP","text":"<ul> <li>Retrieve and plot event triggered data and phase picks using PySTP</li> </ul>"},{"location":"seismic_network/tutorials/#scedc-public-data-set-pds","title":"SCEDC Public Data Set (PDS)","text":"<ul> <li> <p>Retrieve continuous waveform data from SCEDC PDS </p> <p>This tutorial uses the Clickable Station Map to query and retrieve a list of stations. It then, leverages the SCEDC FDSN availability web service to get time spans for waveform data for these stations. Finally, it retrieves data from PDS for a time span.</p> </li> <li> <p>Create an API to decimate waveform data from SCEDC PDS</p> <p>This tutorial shows how to create an API that runs a Lambda function to decimate a waveform from the SCEDC PDS using ObsPy and store the decimated waveform in another S3 bucket. AWS services used: Lambda, ECR, S3, Cloud9.</p> </li> <li> <p>Create a Lambda function to remove response from waveform data from SCEDC PDS</p> <p>This tutorial shows how to make a Lambda function from a Docker image that uses ObsPy to remove response from waveform data from the PDS. Video Tutorial</p> </li> </ul>"},{"location":"seismic_network/tutorials/#scedc-aws-ridgecrest-das","title":"SCEDC AWS Ridgecrest DAS","text":"<ul> <li> <p>Retrieve DAS waveform data from AWS and basic processing </p> <p>This tutorial shows how to download distributed acoustic sensing (DAS) data from the Ridgecrest array and how to perform simple processing operations on the data (e.g., plotting earthquake strain, computing cross-correlations).</p> <p>If GitHub cannot render this notebook, one can visualize it at this link: https://nbviewer.org/github/SCEDC/tutorials/blob/main/jupyter-notebooks/DAS_aws_Ridgecrest/access_aws_data.ipynb</p> </li> </ul>"},{"location":"seismic_network/tutorials/#sample-aws-boto3-script","title":"Sample AWS Boto3 script","text":"<ul> <li>Sample boto3 script to access SCEDC PDS</li> </ul> <p>This script uses boto3 to retrieve a waveform file from the SCEDC PDS and print waveform information using Obspy functions.</p>"},{"location":"seismic_network/tutorials/#accessing-scedcscsn-data-via-matlab","title":"Accessing SCEDC/SCSN data via MATLAB","text":"<ul> <li>Retrieve event, station, and waveform data from SCEDC using MATLAB: Regular .m Script or Live Script</li> <li> <p>Note that both versions contain the same information and code.</p> <p>This tutorial demonstrates how to retrieve event, station, and waveform data using irisFetch in MATLAB and make basic plots with the data. The tutorial also briefly covers waveform access and plotting with the GISMO toolbox.</p> </li> </ul>"}]}